{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score,  confusion_matrix, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.special import softmax\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID Diagnosis  radius1  texture1  perimeter1   area1  smoothness1  \\\n",
       "0      842302         M    17.99     10.38      122.80  1001.0      0.11840   \n",
       "1      842517         M    20.57     17.77      132.90  1326.0      0.08474   \n",
       "2    84300903         M    19.69     21.25      130.00  1203.0      0.10960   \n",
       "3    84348301         M    11.42     20.38       77.58   386.1      0.14250   \n",
       "4    84358402         M    20.29     14.34      135.10  1297.0      0.10030   \n",
       "..        ...       ...      ...       ...         ...     ...          ...   \n",
       "564    926424         M    21.56     22.39      142.00  1479.0      0.11100   \n",
       "565    926682         M    20.13     28.25      131.20  1261.0      0.09780   \n",
       "566    926954         M    16.60     28.08      108.30   858.1      0.08455   \n",
       "567    927241         M    20.60     29.33      140.10  1265.0      0.11780   \n",
       "568     92751         B     7.76     24.54       47.92   181.0      0.05263   \n",
       "\n",
       "     compactness1  concavity1  concave_points1  ...  radius3  texture3  \\\n",
       "0         0.27760     0.30010          0.14710  ...   25.380     17.33   \n",
       "1         0.07864     0.08690          0.07017  ...   24.990     23.41   \n",
       "2         0.15990     0.19740          0.12790  ...   23.570     25.53   \n",
       "3         0.28390     0.24140          0.10520  ...   14.910     26.50   \n",
       "4         0.13280     0.19800          0.10430  ...   22.540     16.67   \n",
       "..            ...         ...              ...  ...      ...       ...   \n",
       "564       0.11590     0.24390          0.13890  ...   25.450     26.40   \n",
       "565       0.10340     0.14400          0.09791  ...   23.690     38.25   \n",
       "566       0.10230     0.09251          0.05302  ...   18.980     34.12   \n",
       "567       0.27700     0.35140          0.15200  ...   25.740     39.42   \n",
       "568       0.04362     0.00000          0.00000  ...    9.456     30.37   \n",
       "\n",
       "     perimeter3   area3  smoothness3  compactness3  concavity3  \\\n",
       "0        184.60  2019.0      0.16220       0.66560      0.7119   \n",
       "1        158.80  1956.0      0.12380       0.18660      0.2416   \n",
       "2        152.50  1709.0      0.14440       0.42450      0.4504   \n",
       "3         98.87   567.7      0.20980       0.86630      0.6869   \n",
       "4        152.20  1575.0      0.13740       0.20500      0.4000   \n",
       "..          ...     ...          ...           ...         ...   \n",
       "564      166.10  2027.0      0.14100       0.21130      0.4107   \n",
       "565      155.00  1731.0      0.11660       0.19220      0.3215   \n",
       "566      126.70  1124.0      0.11390       0.30940      0.3403   \n",
       "567      184.60  1821.0      0.16500       0.86810      0.9387   \n",
       "568       59.16   268.6      0.08996       0.06444      0.0000   \n",
       "\n",
       "     concave_points3  symmetry3  fractal_dimension3  \n",
       "0             0.2654     0.4601             0.11890  \n",
       "1             0.1860     0.2750             0.08902  \n",
       "2             0.2430     0.3613             0.08758  \n",
       "3             0.2575     0.6638             0.17300  \n",
       "4             0.1625     0.2364             0.07678  \n",
       "..               ...        ...                 ...  \n",
       "564           0.2216     0.2060             0.07115  \n",
       "565           0.1628     0.2572             0.06637  \n",
       "566           0.1418     0.2218             0.07820  \n",
       "567           0.2650     0.4087             0.12400  \n",
       "568           0.0000     0.2871             0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['ID','Diagnosis','radius1','texture1','perimeter1','area1','smoothness1','compactness1','concavity1','concave_points1','symmetry1','fractal_dimension1','radius2','texture2',\n",
    "           'perimeter2','area2','smoothness2','compactness2','concavity2','concave_points2','symmetry2','fractal_dimension2','radius3','texture3','perimeter3','area3','smoothness3',\n",
    "            'compactness3','concavity3','concave_points3','symmetry3','fractal_dimension3']\n",
    "wdbc = pd.read_csv('../hw8-data/wdbc.csv', names = columns)\n",
    "wdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=wdbc.iloc[:,2:]\n",
    "y=wdbc.iloc[:,1]\n",
    "label_encoder = LabelEncoder() \n",
    "y= label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normed = (X - X.mean())/X.std()\n",
    "X_normed = X_normed.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised, Semi-Supervised, and Unsupervised Learning\n",
    "\n",
    "**Monte-Carlo Simulation:**\n",
    "\n",
    "Repeat the following procedures for supervised, unsupervised, and semi-supervised learning M = 30 times, and use randomly selected train and test data (make sure you use 20% of both the positve and negative classes as the test set). Then compare the average scores (accuracy, precision,\n",
    "recall, F1-score, and AUC) that you obtain from each algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_base_l1 = {'penalty':[\"l1\"],\n",
    "                   'loss': ['squared_hinge'],\n",
    "                   'dual':['auto'],\n",
    "                   'max_iter':[1000],\n",
    "                   'C':[0.1*n for n in range(1,11)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  param_C  mean_test_score\n",
      "0     0.1         0.973653\n",
      "1     0.2         0.973669\n",
      "2     0.3         0.966636\n",
      "3     0.4         0.968390\n",
      "4     0.5         0.973669\n",
      "5     0.6         0.973669\n",
      "6     0.7         0.973669\n",
      "7     0.8         0.971914\n",
      "8     0.9         0.971930\n",
      "9     1.0         0.970175\n"
     ]
    }
   ],
   "source": [
    "def find_best_param_range_l1(X_train, y_train):\n",
    "    clf = LinearSVC(penalty='l1', dual=\"auto\", random_state=0, max_iter=10000)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=clf, \n",
    "                                param_grid=parameters_base_l1, \n",
    "                                scoring = 'accuracy', \n",
    "                                cv=5)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)[['param_C','mean_test_score']]\n",
    "    results_range_df = results_df[results_df['mean_test_score']>0.8]\n",
    "    print(results_range_df)\n",
    "\n",
    "find_best_param_range_l1(X_normed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monteCarlo(X,y,clf):\n",
    "    scores = {'accuracy':[],'precision':[],'recall':[],'f1':[],'auc':[]}\n",
    "\n",
    "    for i in range(30):\n",
    "        print(f'----------------------Iteration {i+1} ----------------------')\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "        \n",
    "        X_train_normed = (X_train - X_train.mean())/X_train.std()\n",
    "        X_train_normed = X_train_normed.reset_index(drop=True)\n",
    "        X_test_normed = (X_test - X_test.mean())/X_test.std()\n",
    "        X_test_normed = X_test_normed.reset_index(drop=True)\n",
    "\n",
    "        clf.fit(X_train_normed, y_train)\n",
    "        y_pred = clf.predict(X_test_normed)\n",
    "\n",
    "        scores['accuracy'].append(accuracy_score(y_pred, y_test))\n",
    "        scores['precision'].append(precision_score(y_pred, y_test))\n",
    "        scores['recall'].append(recall_score(y_pred, y_test))\n",
    "        scores['f1'].append(f1_score(y_pred, y_test))\n",
    "        scores['auc'].append( roc_auc_score(y_pred, y_test))\n",
    "\n",
    "    return pd.DataFrame(scores) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Iteration 1 ----------------------\n",
      "----------------------Iteration 2 ----------------------\n",
      "----------------------Iteration 3 ----------------------\n",
      "----------------------Iteration 4 ----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Iteration 5 ----------------------\n",
      "----------------------Iteration 6 ----------------------\n",
      "----------------------Iteration 7 ----------------------\n",
      "----------------------Iteration 8 ----------------------\n",
      "----------------------Iteration 9 ----------------------\n",
      "----------------------Iteration 10 ----------------------\n",
      "----------------------Iteration 11 ----------------------\n",
      "----------------------Iteration 12 ----------------------\n",
      "----------------------Iteration 13 ----------------------\n",
      "----------------------Iteration 14 ----------------------\n",
      "----------------------Iteration 15 ----------------------\n",
      "----------------------Iteration 16 ----------------------\n",
      "----------------------Iteration 17 ----------------------\n",
      "----------------------Iteration 18 ----------------------\n",
      "----------------------Iteration 19 ----------------------\n",
      "----------------------Iteration 20 ----------------------\n",
      "----------------------Iteration 21 ----------------------\n",
      "----------------------Iteration 22 ----------------------\n",
      "----------------------Iteration 23 ----------------------\n",
      "----------------------Iteration 24 ----------------------\n",
      "----------------------Iteration 25 ----------------------\n",
      "----------------------Iteration 26 ----------------------\n",
      "----------------------Iteration 27 ----------------------\n",
      "----------------------Iteration 28 ----------------------\n",
      "----------------------Iteration 29 ----------------------\n",
      "----------------------Iteration 30 ----------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>0.960513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.969702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.947973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.955062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.981151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.977273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.951032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.981151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.969702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.969702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.940260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.947973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.981151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.974106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.981151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.974106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.967230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.967230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>0.941026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.993151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.986486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.981151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  precision    recall        f1       auc\n",
       "0   0.956140   0.904762  0.974359  0.938272  0.960513\n",
       "1   0.973684   0.976190  0.953488  0.964706  0.969702\n",
       "2   0.947368   0.904762  0.950000  0.926829  0.947973\n",
       "3   0.973684   0.928571  1.000000  0.962963  0.980000\n",
       "4   0.956140   0.928571  0.951220  0.939759  0.955062\n",
       "5   0.982456   0.976190  0.976190  0.976190  0.981151\n",
       "6   0.982456   1.000000  0.954545  0.976744  0.977273\n",
       "7   0.964912   0.904762  1.000000  0.950000  0.973684\n",
       "8   0.991228   0.976190  1.000000  0.987952  0.993151\n",
       "9   0.956140   0.952381  0.930233  0.941176  0.951032\n",
       "10  0.982456   0.976190  0.976190  0.976190  0.981151\n",
       "11  0.991228   0.976190  1.000000  0.987952  0.993151\n",
       "12  0.973684   0.976190  0.953488  0.964706  0.969702\n",
       "13  0.973684   0.976190  0.953488  0.964706  0.969702\n",
       "14  0.947368   0.952381  0.909091  0.930233  0.940260\n",
       "15  0.947368   0.904762  0.950000  0.926829  0.947973\n",
       "16  0.982456   0.976190  0.976190  0.976190  0.981151\n",
       "17  0.973684   0.952381  0.975610  0.963855  0.974106\n",
       "18  0.991228   0.976190  1.000000  0.987952  0.993151\n",
       "19  0.973684   0.928571  1.000000  0.962963  0.980000\n",
       "20  0.982456   0.976190  0.976190  0.976190  0.981151\n",
       "21  0.973684   0.928571  1.000000  0.962963  0.980000\n",
       "22  0.973684   0.952381  0.975610  0.963855  0.974106\n",
       "23  0.964912   0.928571  0.975000  0.951220  0.967230\n",
       "24  0.964912   0.928571  0.975000  0.951220  0.967230\n",
       "25  0.938596   0.880952  0.948718  0.913580  0.941026\n",
       "26  0.991228   0.976190  1.000000  0.987952  0.993151\n",
       "27  0.982456   0.952381  1.000000  0.975610  0.986486\n",
       "28  0.973684   0.928571  1.000000  0.962963  0.980000\n",
       "29  0.982456   0.976190  0.976190  0.976190  0.981151"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = monteCarlo(X,y,LinearSVC(penalty='l1', dual=\"auto\", random_state=0, max_iter=100000, C=0.2, loss = 'squared_hinge'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy     0.971637\n",
       "precision    0.949206\n",
       "recall       0.973693\n",
       "f1           0.960930\n",
       "auc          0.972381\n",
       "dtype: float64"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Plot----------------------\n"
     ]
    }
   ],
   "source": [
    "print('----------------------Plot----------------------')\n",
    "clf = LinearSVC(penalty='l1', dual=\"auto\", random_state=0, max_iter=100000, C=0.2, loss = 'squared_hinge')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "\n",
    "X_train_normed = (X_train - X_train.mean())/X_train.std()\n",
    "X_train_normed = X_train_normed.reset_index(drop=True)\n",
    "X_test_normed = (X_test - X_test.mean())/X_test.std()\n",
    "X_test_normed = X_test_normed.reset_index(drop=True)\n",
    "\n",
    "clf.fit(X_train_normed, y_train)\n",
    "y_pred = clf.predict(X_test_normed)\n",
    "y_score = clf.decision_function(X_test_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc_curve(y_true, y_pred_prob):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot ROC curve\n",
    "plot_roc_curve(y_test, y_score)\n",
    "\n",
    "# Confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for testing data\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Semi-Supervised Learning/ Self-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_label, X_unlabel, y_label, y_unlabel = train_test_split(X, y, stratify=y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>fractal_dimension1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.474254</td>\n",
       "      <td>-0.551144</td>\n",
       "      <td>-0.535951</td>\n",
       "      <td>-0.501159</td>\n",
       "      <td>-0.791243</td>\n",
       "      <td>-1.214039</td>\n",
       "      <td>-1.039204</td>\n",
       "      <td>-1.085898</td>\n",
       "      <td>-2.144176</td>\n",
       "      <td>-0.828044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613106</td>\n",
       "      <td>-0.918121</td>\n",
       "      <td>-0.681119</td>\n",
       "      <td>-0.588469</td>\n",
       "      <td>-0.994727</td>\n",
       "      <td>-1.278422</td>\n",
       "      <td>-1.170935</td>\n",
       "      <td>-1.298627</td>\n",
       "      <td>-1.563298</td>\n",
       "      <td>-1.156072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.226194</td>\n",
       "      <td>-0.655893</td>\n",
       "      <td>-0.254718</td>\n",
       "      <td>-0.299085</td>\n",
       "      <td>-1.733300</td>\n",
       "      <td>-0.592520</td>\n",
       "      <td>-0.135487</td>\n",
       "      <td>-0.551206</td>\n",
       "      <td>-1.418451</td>\n",
       "      <td>-0.625531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.429837</td>\n",
       "      <td>-0.437010</td>\n",
       "      <td>-0.466882</td>\n",
       "      <td>-0.443949</td>\n",
       "      <td>-1.341704</td>\n",
       "      <td>-0.041119</td>\n",
       "      <td>0.206823</td>\n",
       "      <td>-0.400826</td>\n",
       "      <td>-1.363178</td>\n",
       "      <td>-0.397864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.818156</td>\n",
       "      <td>-1.436957</td>\n",
       "      <td>-0.781368</td>\n",
       "      <td>-0.768998</td>\n",
       "      <td>1.812910</td>\n",
       "      <td>0.081342</td>\n",
       "      <td>-0.158185</td>\n",
       "      <td>0.130262</td>\n",
       "      <td>0.671074</td>\n",
       "      <td>0.847411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.712708</td>\n",
       "      <td>-0.788348</td>\n",
       "      <td>-0.668282</td>\n",
       "      <td>-0.700436</td>\n",
       "      <td>1.227679</td>\n",
       "      <td>-0.103114</td>\n",
       "      <td>-0.262576</td>\n",
       "      <td>0.208241</td>\n",
       "      <td>0.615102</td>\n",
       "      <td>0.429699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.341768</td>\n",
       "      <td>-0.223233</td>\n",
       "      <td>-0.332861</td>\n",
       "      <td>-0.378621</td>\n",
       "      <td>-1.574041</td>\n",
       "      <td>-0.490328</td>\n",
       "      <td>-0.612484</td>\n",
       "      <td>-0.778444</td>\n",
       "      <td>0.166222</td>\n",
       "      <td>-0.522925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.455733</td>\n",
       "      <td>-0.142646</td>\n",
       "      <td>-0.343646</td>\n",
       "      <td>-0.460719</td>\n",
       "      <td>-1.746217</td>\n",
       "      <td>-0.170581</td>\n",
       "      <td>-0.547046</td>\n",
       "      <td>-0.744350</td>\n",
       "      <td>0.407345</td>\n",
       "      <td>-0.596542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.255832</td>\n",
       "      <td>1.473247</td>\n",
       "      <td>0.229605</td>\n",
       "      <td>0.159306</td>\n",
       "      <td>0.375531</td>\n",
       "      <td>-0.157403</td>\n",
       "      <td>0.367871</td>\n",
       "      <td>0.378812</td>\n",
       "      <td>-0.629620</td>\n",
       "      <td>0.239873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416787</td>\n",
       "      <td>1.219975</td>\n",
       "      <td>0.366387</td>\n",
       "      <td>0.243141</td>\n",
       "      <td>1.447284</td>\n",
       "      <td>-0.147484</td>\n",
       "      <td>0.525801</td>\n",
       "      <td>0.509730</td>\n",
       "      <td>-0.437435</td>\n",
       "      <td>0.545985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>-1.472133</td>\n",
       "      <td>-0.799354</td>\n",
       "      <td>-1.368253</td>\n",
       "      <td>-1.165559</td>\n",
       "      <td>-0.149483</td>\n",
       "      <td>0.316348</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>-0.673127</td>\n",
       "      <td>-1.828643</td>\n",
       "      <td>1.183582</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.395187</td>\n",
       "      <td>-1.335928</td>\n",
       "      <td>-1.284176</td>\n",
       "      <td>-1.039295</td>\n",
       "      <td>-0.665319</td>\n",
       "      <td>-0.437406</td>\n",
       "      <td>-0.573591</td>\n",
       "      <td>-1.144693</td>\n",
       "      <td>-1.968120</td>\n",
       "      <td>-0.348846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>-0.460160</td>\n",
       "      <td>-0.273331</td>\n",
       "      <td>-0.526590</td>\n",
       "      <td>-0.472211</td>\n",
       "      <td>-1.516681</td>\n",
       "      <td>-1.424833</td>\n",
       "      <td>-1.077967</td>\n",
       "      <td>-1.110034</td>\n",
       "      <td>-1.046824</td>\n",
       "      <td>-1.486885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537408</td>\n",
       "      <td>-0.717131</td>\n",
       "      <td>-0.614366</td>\n",
       "      <td>-0.520237</td>\n",
       "      <td>-1.762028</td>\n",
       "      <td>-1.316470</td>\n",
       "      <td>-1.240832</td>\n",
       "      <td>-1.465171</td>\n",
       "      <td>-1.080567</td>\n",
       "      <td>-1.496588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>-0.482711</td>\n",
       "      <td>0.141111</td>\n",
       "      <td>-0.485483</td>\n",
       "      <td>-0.530107</td>\n",
       "      <td>-0.831733</td>\n",
       "      <td>-0.091352</td>\n",
       "      <td>-0.284674</td>\n",
       "      <td>-0.477112</td>\n",
       "      <td>-0.159828</td>\n",
       "      <td>-0.050395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589201</td>\n",
       "      <td>-0.386367</td>\n",
       "      <td>-0.576996</td>\n",
       "      <td>-0.576796</td>\n",
       "      <td>-1.231901</td>\n",
       "      <td>-0.267829</td>\n",
       "      <td>-0.414323</td>\n",
       "      <td>-0.596939</td>\n",
       "      <td>-0.390078</td>\n",
       "      <td>-0.326945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.554632</td>\n",
       "      <td>-1.004298</td>\n",
       "      <td>0.486011</td>\n",
       "      <td>0.393701</td>\n",
       "      <td>-0.127214</td>\n",
       "      <td>-0.383151</td>\n",
       "      <td>-0.441900</td>\n",
       "      <td>-0.122996</td>\n",
       "      <td>-0.321100</td>\n",
       "      <td>-0.555327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257423</td>\n",
       "      <td>-0.938695</td>\n",
       "      <td>0.215195</td>\n",
       "      <td>0.075274</td>\n",
       "      <td>-0.546732</td>\n",
       "      <td>-0.532831</td>\n",
       "      <td>-0.234704</td>\n",
       "      <td>-0.065709</td>\n",
       "      <td>-0.248009</td>\n",
       "      <td>-0.719086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>-0.361500</td>\n",
       "      <td>-0.813017</td>\n",
       "      <td>-0.336931</td>\n",
       "      <td>-0.402792</td>\n",
       "      <td>-1.258897</td>\n",
       "      <td>-0.191051</td>\n",
       "      <td>0.226013</td>\n",
       "      <td>-0.405000</td>\n",
       "      <td>-1.383392</td>\n",
       "      <td>-0.474322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.501551</td>\n",
       "      <td>-0.921286</td>\n",
       "      <td>-0.463173</td>\n",
       "      <td>-0.503960</td>\n",
       "      <td>-1.484008</td>\n",
       "      <td>-0.171188</td>\n",
       "      <td>0.222749</td>\n",
       "      <td>-0.228049</td>\n",
       "      <td>-1.438032</td>\n",
       "      <td>-0.659117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      radius1  texture1  perimeter1     area1  smoothness1  compactness1  \\\n",
       "0   -0.474254 -0.551144   -0.535951 -0.501159    -0.791243     -1.214039   \n",
       "1   -0.226194 -0.655893   -0.254718 -0.299085    -1.733300     -0.592520   \n",
       "2   -0.818156 -1.436957   -0.781368 -0.768998     1.812910      0.081342   \n",
       "3   -0.341768 -0.223233   -0.332861 -0.378621    -1.574041     -0.490328   \n",
       "4    0.255832  1.473247    0.229605  0.159306     0.375531     -0.157403   \n",
       "..        ...       ...         ...       ...          ...           ...   \n",
       "279 -1.472133 -0.799354   -1.368253 -1.165559    -0.149483      0.316348   \n",
       "280 -0.460160 -0.273331   -0.526590 -0.472211    -1.516681     -1.424833   \n",
       "281 -0.482711  0.141111   -0.485483 -0.530107    -0.831733     -0.091352   \n",
       "282  0.554632 -1.004298    0.486011  0.393701    -0.127214     -0.383151   \n",
       "283 -0.361500 -0.813017   -0.336931 -0.402792    -1.258897     -0.191051   \n",
       "\n",
       "     concavity1  concave_points1  symmetry1  fractal_dimension1  ...  \\\n",
       "0     -1.039204        -1.085898  -2.144176           -0.828044  ...   \n",
       "1     -0.135487        -0.551206  -1.418451           -0.625531  ...   \n",
       "2     -0.158185         0.130262   0.671074            0.847411  ...   \n",
       "3     -0.612484        -0.778444   0.166222           -0.522925  ...   \n",
       "4      0.367871         0.378812  -0.629620            0.239873  ...   \n",
       "..          ...              ...        ...                 ...  ...   \n",
       "279    0.002942        -0.673127  -1.828643            1.183582  ...   \n",
       "280   -1.077967        -1.110034  -1.046824           -1.486885  ...   \n",
       "281   -0.284674        -0.477112  -0.159828           -0.050395  ...   \n",
       "282   -0.441900        -0.122996  -0.321100           -0.555327  ...   \n",
       "283    0.226013        -0.405000  -1.383392           -0.474322  ...   \n",
       "\n",
       "      radius3  texture3  perimeter3     area3  smoothness3  compactness3  \\\n",
       "0   -0.613106 -0.918121   -0.681119 -0.588469    -0.994727     -1.278422   \n",
       "1   -0.429837 -0.437010   -0.466882 -0.443949    -1.341704     -0.041119   \n",
       "2   -0.712708 -0.788348   -0.668282 -0.700436     1.227679     -0.103114   \n",
       "3   -0.455733 -0.142646   -0.343646 -0.460719    -1.746217     -0.170581   \n",
       "4    0.416787  1.219975    0.366387  0.243141     1.447284     -0.147484   \n",
       "..        ...       ...         ...       ...          ...           ...   \n",
       "279 -1.395187 -1.335928   -1.284176 -1.039295    -0.665319     -0.437406   \n",
       "280 -0.537408 -0.717131   -0.614366 -0.520237    -1.762028     -1.316470   \n",
       "281 -0.589201 -0.386367   -0.576996 -0.576796    -1.231901     -0.267829   \n",
       "282  0.257423 -0.938695    0.215195  0.075274    -0.546732     -0.532831   \n",
       "283 -0.501551 -0.921286   -0.463173 -0.503960    -1.484008     -0.171188   \n",
       "\n",
       "     concavity3  concave_points3  symmetry3  fractal_dimension3  \n",
       "0     -1.170935        -1.298627  -1.563298           -1.156072  \n",
       "1      0.206823        -0.400826  -1.363178           -0.397864  \n",
       "2     -0.262576         0.208241   0.615102            0.429699  \n",
       "3     -0.547046        -0.744350   0.407345           -0.596542  \n",
       "4      0.525801         0.509730  -0.437435            0.545985  \n",
       "..          ...              ...        ...                 ...  \n",
       "279   -0.573591        -1.144693  -1.968120           -0.348846  \n",
       "280   -1.240832        -1.465171  -1.080567           -1.496588  \n",
       "281   -0.414323        -0.596939  -0.390078           -0.326945  \n",
       "282   -0.234704        -0.065709  -0.248009           -0.719086  \n",
       "283    0.222749        -0.228049  -1.438032           -0.659117  \n",
       "\n",
       "[284 rows x 30 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_label_normed = (X_label - X_label.mean())/X_label.std()\n",
    "X_label_normed = pd.DataFrame(X_label_normed.reset_index(drop=True), columns = X.columns)\n",
    "X_label_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Train an L1-penalized SVM to classify the labeled data Use normalized\n",
    "data. Choose the penalty parameter using 5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_base_l1 = {'penalty':[\"l1\"],\n",
    "                   'loss': ['squared_hinge'],\n",
    "                   'dual':['auto'],\n",
    "                   'max_iter':[1000],\n",
    "                   'C':[0.1*n for n in range(1,10)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  param_C  mean_test_score\n",
      "0     0.1         0.971805\n",
      "1     0.2         0.982331\n",
      "2     0.3         0.978822\n",
      "3     0.4         0.982393\n",
      "4     0.5         0.978885\n",
      "5     0.6         0.978885\n",
      "6     0.7         0.978885\n",
      "7     0.8         0.978885\n",
      "8     0.9         0.978885\n"
     ]
    }
   ],
   "source": [
    "find_best_param_range_l1(X_label_normed, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(penalty='l1', dual=\"auto\", random_state=0, max_iter=100000, C=0.4, loss = 'squared_hinge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Find the unlabeled data point that is the farthest to the decision boundary\n",
    "of the SVM. Let the SVM label it (ignore its true label), and add it to\n",
    "the labeled data, and retrain the SVM. Continue this process until all\n",
    "unlabeled data are used. Test the final SVM on the test data andthe\n",
    "average accuracy, precision, recall, F1-score, and AUC, for both training\n",
    "and test sets over your M runs. Plot the ROC and report the confusion\n",
    "matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_label_normed, y_label)\n",
    "\n",
    "def find_farthest_point(X_unlabel, clf):\n",
    "    decision_values = clf.decision_function(X_unlabel)\n",
    "    distance_to_boundary = np.abs(decision_values)\n",
    "    farthest_index = np.argmax(distance_to_boundary)\n",
    "    return farthest_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearSVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_unlabel_normed = (X_unlabel - X_unlabel.mean())/X_unlabel.std()\n",
    "X_unlabel_normed = pd.DataFrame(X_unlabel_normed.reset_index(drop=True), columns = X.columns)\n",
    "\n",
    "while len(X_unlabel_normed) > 0:\n",
    "    farthest_index = find_farthest_point(X_unlabel_normed, clf)\n",
    "    farthest_point = X_unlabel_normed.iloc[farthest_index, :]\n",
    "    \n",
    "    X_unlabel_normed = X_unlabel_normed.drop(index=farthest_index).reset_index(drop=True)\n",
    "    X_unlabel_normed = pd.DataFrame(X_unlabel_normed, columns = X.columns)\n",
    "\n",
    "    X_label_normed = np.vstack([X_label_normed, farthest_point])\n",
    "\n",
    "    y_label = np.append(y_label, clf.predict([farthest_point]))\n",
    "    X_label_normed=pd.DataFrame(X_label_normed,columns = X.columns)\n",
    "\n",
    "    clf.fit(X_label_normed, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>fractal_dimension1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.474254</td>\n",
       "      <td>-0.551144</td>\n",
       "      <td>-0.535951</td>\n",
       "      <td>-0.501159</td>\n",
       "      <td>-0.791243</td>\n",
       "      <td>-1.214039</td>\n",
       "      <td>-1.039204</td>\n",
       "      <td>-1.085898</td>\n",
       "      <td>-2.144176</td>\n",
       "      <td>-0.828044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613106</td>\n",
       "      <td>-0.918121</td>\n",
       "      <td>-0.681119</td>\n",
       "      <td>-0.588469</td>\n",
       "      <td>-0.994727</td>\n",
       "      <td>-1.278422</td>\n",
       "      <td>-1.170935</td>\n",
       "      <td>-1.298627</td>\n",
       "      <td>-1.563298</td>\n",
       "      <td>-1.156072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.226194</td>\n",
       "      <td>-0.655893</td>\n",
       "      <td>-0.254718</td>\n",
       "      <td>-0.299085</td>\n",
       "      <td>-1.733300</td>\n",
       "      <td>-0.592520</td>\n",
       "      <td>-0.135487</td>\n",
       "      <td>-0.551206</td>\n",
       "      <td>-1.418451</td>\n",
       "      <td>-0.625531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.429837</td>\n",
       "      <td>-0.437010</td>\n",
       "      <td>-0.466882</td>\n",
       "      <td>-0.443949</td>\n",
       "      <td>-1.341704</td>\n",
       "      <td>-0.041119</td>\n",
       "      <td>0.206823</td>\n",
       "      <td>-0.400826</td>\n",
       "      <td>-1.363178</td>\n",
       "      <td>-0.397864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.818156</td>\n",
       "      <td>-1.436957</td>\n",
       "      <td>-0.781368</td>\n",
       "      <td>-0.768998</td>\n",
       "      <td>1.812910</td>\n",
       "      <td>0.081342</td>\n",
       "      <td>-0.158185</td>\n",
       "      <td>0.130262</td>\n",
       "      <td>0.671074</td>\n",
       "      <td>0.847411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.712708</td>\n",
       "      <td>-0.788348</td>\n",
       "      <td>-0.668282</td>\n",
       "      <td>-0.700436</td>\n",
       "      <td>1.227679</td>\n",
       "      <td>-0.103114</td>\n",
       "      <td>-0.262576</td>\n",
       "      <td>0.208241</td>\n",
       "      <td>0.615102</td>\n",
       "      <td>0.429699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.341768</td>\n",
       "      <td>-0.223233</td>\n",
       "      <td>-0.332861</td>\n",
       "      <td>-0.378621</td>\n",
       "      <td>-1.574041</td>\n",
       "      <td>-0.490328</td>\n",
       "      <td>-0.612484</td>\n",
       "      <td>-0.778444</td>\n",
       "      <td>0.166222</td>\n",
       "      <td>-0.522925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.455733</td>\n",
       "      <td>-0.142646</td>\n",
       "      <td>-0.343646</td>\n",
       "      <td>-0.460719</td>\n",
       "      <td>-1.746217</td>\n",
       "      <td>-0.170581</td>\n",
       "      <td>-0.547046</td>\n",
       "      <td>-0.744350</td>\n",
       "      <td>0.407345</td>\n",
       "      <td>-0.596542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.255832</td>\n",
       "      <td>1.473247</td>\n",
       "      <td>0.229605</td>\n",
       "      <td>0.159306</td>\n",
       "      <td>0.375531</td>\n",
       "      <td>-0.157403</td>\n",
       "      <td>0.367871</td>\n",
       "      <td>0.378812</td>\n",
       "      <td>-0.629620</td>\n",
       "      <td>0.239873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416787</td>\n",
       "      <td>1.219975</td>\n",
       "      <td>0.366387</td>\n",
       "      <td>0.243141</td>\n",
       "      <td>1.447284</td>\n",
       "      <td>-0.147484</td>\n",
       "      <td>0.525801</td>\n",
       "      <td>0.509730</td>\n",
       "      <td>-0.437435</td>\n",
       "      <td>0.545985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.275997</td>\n",
       "      <td>-0.056659</td>\n",
       "      <td>0.236015</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>-0.281429</td>\n",
       "      <td>-0.328741</td>\n",
       "      <td>-0.139145</td>\n",
       "      <td>-0.114183</td>\n",
       "      <td>-0.898668</td>\n",
       "      <td>-0.533886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314147</td>\n",
       "      <td>0.371063</td>\n",
       "      <td>0.237589</td>\n",
       "      <td>0.204310</td>\n",
       "      <td>-0.309453</td>\n",
       "      <td>-0.257385</td>\n",
       "      <td>0.138535</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>-1.000309</td>\n",
       "      <td>-0.854016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0.435716</td>\n",
       "      <td>0.016859</td>\n",
       "      <td>0.349886</td>\n",
       "      <td>0.311932</td>\n",
       "      <td>-1.316903</td>\n",
       "      <td>-0.934094</td>\n",
       "      <td>-0.578850</td>\n",
       "      <td>-0.514000</td>\n",
       "      <td>-0.952047</td>\n",
       "      <td>-1.237352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385209</td>\n",
       "      <td>0.975146</td>\n",
       "      <td>0.302980</td>\n",
       "      <td>0.245219</td>\n",
       "      <td>-1.016374</td>\n",
       "      <td>-0.453502</td>\n",
       "      <td>-0.181497</td>\n",
       "      <td>-0.419018</td>\n",
       "      <td>-0.307882</td>\n",
       "      <td>-0.927935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>-0.034885</td>\n",
       "      <td>-0.535712</td>\n",
       "      <td>-0.006272</td>\n",
       "      <td>-0.136220</td>\n",
       "      <td>1.032681</td>\n",
       "      <td>0.522733</td>\n",
       "      <td>0.169870</td>\n",
       "      <td>0.130598</td>\n",
       "      <td>0.424351</td>\n",
       "      <td>-0.213451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057894</td>\n",
       "      <td>-0.631280</td>\n",
       "      <td>0.060101</td>\n",
       "      <td>-0.062736</td>\n",
       "      <td>0.851294</td>\n",
       "      <td>0.517079</td>\n",
       "      <td>0.319675</td>\n",
       "      <td>0.411024</td>\n",
       "      <td>0.356916</td>\n",
       "      <td>-0.260890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0.689555</td>\n",
       "      <td>-0.241640</td>\n",
       "      <td>0.624172</td>\n",
       "      <td>0.540025</td>\n",
       "      <td>0.073267</td>\n",
       "      <td>-0.354937</td>\n",
       "      <td>-0.357052</td>\n",
       "      <td>0.020165</td>\n",
       "      <td>-1.150309</td>\n",
       "      <td>-1.013793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432584</td>\n",
       "      <td>-0.065684</td>\n",
       "      <td>0.343459</td>\n",
       "      <td>0.283855</td>\n",
       "      <td>0.092008</td>\n",
       "      <td>-0.538887</td>\n",
       "      <td>-0.496775</td>\n",
       "      <td>-0.329947</td>\n",
       "      <td>-0.806913</td>\n",
       "      <td>-1.140821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0.295961</td>\n",
       "      <td>-0.623459</td>\n",
       "      <td>0.299184</td>\n",
       "      <td>0.191430</td>\n",
       "      <td>-0.536721</td>\n",
       "      <td>-0.127498</td>\n",
       "      <td>-0.130557</td>\n",
       "      <td>-0.182969</td>\n",
       "      <td>-0.772848</td>\n",
       "      <td>-0.428068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355062</td>\n",
       "      <td>-0.937505</td>\n",
       "      <td>0.359029</td>\n",
       "      <td>0.246924</td>\n",
       "      <td>0.759656</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.381992</td>\n",
       "      <td>0.215229</td>\n",
       "      <td>0.956098</td>\n",
       "      <td>0.793490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      radius1  texture1  perimeter1     area1  smoothness1  compactness1  \\\n",
       "0   -0.474254 -0.551144   -0.535951 -0.501159    -0.791243     -1.214039   \n",
       "1   -0.226194 -0.655893   -0.254718 -0.299085    -1.733300     -0.592520   \n",
       "2   -0.818156 -1.436957   -0.781368 -0.768998     1.812910      0.081342   \n",
       "3   -0.341768 -0.223233   -0.332861 -0.378621    -1.574041     -0.490328   \n",
       "4    0.255832  1.473247    0.229605  0.159306     0.375531     -0.157403   \n",
       "..        ...       ...         ...       ...          ...           ...   \n",
       "564  0.275997 -0.056659    0.236015  0.149254    -0.281429     -0.328741   \n",
       "565  0.435716  0.016859    0.349886  0.311932    -1.316903     -0.934094   \n",
       "566 -0.034885 -0.535712   -0.006272 -0.136220     1.032681      0.522733   \n",
       "567  0.689555 -0.241640    0.624172  0.540025     0.073267     -0.354937   \n",
       "568  0.295961 -0.623459    0.299184  0.191430    -0.536721     -0.127498   \n",
       "\n",
       "     concavity1  concave_points1  symmetry1  fractal_dimension1  ...  \\\n",
       "0     -1.039204        -1.085898  -2.144176           -0.828044  ...   \n",
       "1     -0.135487        -0.551206  -1.418451           -0.625531  ...   \n",
       "2     -0.158185         0.130262   0.671074            0.847411  ...   \n",
       "3     -0.612484        -0.778444   0.166222           -0.522925  ...   \n",
       "4      0.367871         0.378812  -0.629620            0.239873  ...   \n",
       "..          ...              ...        ...                 ...  ...   \n",
       "564   -0.139145        -0.114183  -0.898668           -0.533886  ...   \n",
       "565   -0.578850        -0.514000  -0.952047           -1.237352  ...   \n",
       "566    0.169870         0.130598   0.424351           -0.213451  ...   \n",
       "567   -0.357052         0.020165  -1.150309           -1.013793  ...   \n",
       "568   -0.130557        -0.182969  -0.772848           -0.428068  ...   \n",
       "\n",
       "      radius3  texture3  perimeter3     area3  smoothness3  compactness3  \\\n",
       "0   -0.613106 -0.918121   -0.681119 -0.588469    -0.994727     -1.278422   \n",
       "1   -0.429837 -0.437010   -0.466882 -0.443949    -1.341704     -0.041119   \n",
       "2   -0.712708 -0.788348   -0.668282 -0.700436     1.227679     -0.103114   \n",
       "3   -0.455733 -0.142646   -0.343646 -0.460719    -1.746217     -0.170581   \n",
       "4    0.416787  1.219975    0.366387  0.243141     1.447284     -0.147484   \n",
       "..        ...       ...         ...       ...          ...           ...   \n",
       "564  0.314147  0.371063    0.237589  0.204310    -0.309453     -0.257385   \n",
       "565  0.385209  0.975146    0.302980  0.245219    -1.016374     -0.453502   \n",
       "566  0.057894 -0.631280    0.060101 -0.062736     0.851294      0.517079   \n",
       "567  0.432584 -0.065684    0.343459  0.283855     0.092008     -0.538887   \n",
       "568  0.355062 -0.937505    0.359029  0.246924     0.759656      0.563107   \n",
       "\n",
       "     concavity3  concave_points3  symmetry3  fractal_dimension3  \n",
       "0     -1.170935        -1.298627  -1.563298           -1.156072  \n",
       "1      0.206823        -0.400826  -1.363178           -0.397864  \n",
       "2     -0.262576         0.208241   0.615102            0.429699  \n",
       "3     -0.547046        -0.744350   0.407345           -0.596542  \n",
       "4      0.525801         0.509730  -0.437435            0.545985  \n",
       "..          ...              ...        ...                 ...  \n",
       "564    0.138535         0.003385  -1.000309           -0.854016  \n",
       "565   -0.181497        -0.419018  -0.307882           -0.927935  \n",
       "566    0.319675         0.411024   0.356916           -0.260890  \n",
       "567   -0.496775        -0.329947  -0.806913           -1.140821  \n",
       "568    0.381992         0.215229   0.956098            0.793490  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_label_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Iteration 1 ----------------------\n",
      "----------------------Iteration 2 ----------------------\n",
      "----------------------Iteration 3 ----------------------\n",
      "----------------------Iteration 4 ----------------------\n",
      "----------------------Iteration 5 ----------------------\n",
      "----------------------Iteration 6 ----------------------\n",
      "----------------------Iteration 7 ----------------------\n",
      "----------------------Iteration 8 ----------------------\n",
      "----------------------Iteration 9 ----------------------\n",
      "----------------------Iteration 10 ----------------------\n",
      "----------------------Iteration 11 ----------------------\n",
      "----------------------Iteration 12 ----------------------\n",
      "----------------------Iteration 13 ----------------------\n",
      "----------------------Iteration 14 ----------------------\n",
      "----------------------Iteration 15 ----------------------\n",
      "----------------------Iteration 16 ----------------------\n",
      "----------------------Iteration 17 ----------------------\n",
      "----------------------Iteration 18 ----------------------\n",
      "----------------------Iteration 19 ----------------------\n",
      "----------------------Iteration 20 ----------------------\n",
      "----------------------Iteration 21 ----------------------\n",
      "----------------------Iteration 22 ----------------------\n",
      "----------------------Iteration 23 ----------------------\n",
      "----------------------Iteration 24 ----------------------\n",
      "----------------------Iteration 25 ----------------------\n",
      "----------------------Iteration 26 ----------------------\n",
      "----------------------Iteration 27 ----------------------\n",
      "----------------------Iteration 28 ----------------------\n",
      "----------------------Iteration 29 ----------------------\n",
      "----------------------Iteration 30 ----------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy     0.985965\n",
       "precision    0.978295\n",
       "recall       0.984720\n",
       "f1           0.981327\n",
       "auc          0.985873\n",
       "dtype: float64"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = monteCarlo(X_label_normed, y_label ,LinearSVC(penalty='l1', dual=\"auto\", random_state=0, max_iter=100000, C=0.2, loss = 'squared_hinge'))\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAIjCAYAAAAQgZNYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAACE8ElEQVR4nOzdd1hT5/sG8DuMhL2UIYiiuPeqft2LirV1K7hxb2u17r1H3dY9cQviog7c1j2qYt0bNwoO9kze3x/+TI0MCQKHwP25Lq42T845uZOT4MOb95wjE0IIEBERERHpID2pAxARERERpRebWSIiIiLSWWxmiYiIiEhnsZklIiIiIp3FZpaIiIiIdBabWSIiIiLSWWxmiYiIiEhnsZklIiIiIp3FZpaIiIiIdBabWaIs4uLigq5du0odI9epV68e6tWrJ3WMb5o0aRJkMhlCQ0OljpLtyGQyTJo0KUO2FRQUBJlMBm9v7wzZHgBcunQJcrkcT58+zbBtZrR27drBw8ND6hhEmYLNLOUI3t7ekMlk6h8DAwM4OTmha9euePnypdTxsrWoqChMnToV5cqVg4mJCSwtLVG7dm1s3LgRunK169u3b2PSpEkICgqSOkoSSqUS69evR7169WBjYwOFQgEXFxd069YN//zzj9TxMsTWrVuxcOFCqWNoyMpMY8eORfv27VGwYEF1rV69ehq/k4yNjVGuXDksXLgQKpUq2e28e/cOw4cPR/HixWFkZAQbGxu4u7tj3759KT52eHg4Jk+ejPLly8PMzAzGxsYoU6YMRo4ciVevXqmXGzlyJHbu3Inr16+n+Xnlhvcu5QwyoSv/WhGlwtvbG926dcOUKVNQqFAhxMbG4sKFC/D29oaLiwtu3rwJIyMjSTPGxcVBT08PhoaGkub40ps3b9CwYUPcuXMH7dq1Q926dREbG4udO3fi1KlT8PT0xJYtW6Cvry911FT5+fmhbdu2OHHiRJJR2Pj4eACAXC7P8lwxMTFo1aoVAgICUKdOHTRt2hQ2NjYICgqCr68v7t+/j2fPniF//vyYNGkSJk+ejJCQEOTNmzfLs36PX375BTdv3sy0PyZiY2NhYGAAAwOD784khEBcXBwMDQ0z5H0dGBiIihUr4ty5c6hevbq6Xq9ePTx69AgzZ84EAISGhmLr1q24fPkyxowZg+nTp2ts5969e2jYsCFCQkLQrVs3VKlSBR8/fsSWLVsQGBiIYcOGYc6cORrrPH78GG5ubnj27Bnatm2LWrVqQS6X499//8W2bdtgY2OD+/fvq5evVq0aihcvjo0bN37zeWnz3iWSnCDKAdavXy8AiMuXL2vUR44cKQAIHx8fiZJJKyYmRiiVyhTvd3d3F3p6emLv3r1J7hs2bJgAIGbNmpWZEZMVGRmp1fI7duwQAMSJEycyJ1A6DRgwQAAQCxYsSHJfYmKimDNnjnj+/LkQQoiJEycKACIkJCTT8qhUKhEdHZ3h2/35559FwYIFM3SbSqVSxMTEpHv9zMiUnF9//VUUKFBAqFQqjXrdunVF6dKlNWoxMTGiYMGCwtzcXCQmJqrr8fHxokyZMsLExERcuHBBY53ExETh6ekpAIjt27er6wkJCaJ8+fLCxMREnD59OkmusLAwMWbMGI3a3LlzhampqYiIiPjm89Lmvfs9vnc/EwkhBJtZyhFSamb37dsnAIgZM2Zo1O/cuSNat24trK2thUKhEJUrV062ofvw4YP47bffRMGCBYVcLhdOTk6ic+fOGg1HbGysmDBhgnB1dRVyuVzkz59fDB8+XMTGxmpsq2DBgsLLy0sIIcTly5cFAOHt7Z3kMQMCAgQA8ddff6lrL168EN26dRN2dnZCLpeLUqVKibVr12qsd+LECQFAbNu2TYwdO1Y4OjoKmUwmPnz4kOxrdv78eQFAdO/ePdn7ExISRNGiRYW1tbW6AXry5IkAIObMmSPmz58vChQoIIyMjESdOnXEjRs3kmwjLa/z53138uRJ0a9fP2FrayusrKyEEEIEBQWJfv36iWLFigkjIyNhY2Mj2rRpI548eZJk/a9/Pje2devWFXXr1k3yOvn4+Ihp06YJJycnoVAoRIMGDcSDBw+SPIclS5aIQoUKCSMjI/HDDz+IU6dOJdlmcp4/fy4MDAzEjz/+mOpyn31uZh88eCC8vLyEpaWlsLCwEF27dhVRUVEay65bt07Ur19f2NraCrlcLkqWLCmWLVuWZJsFCxYUP//8swgICBCVK1cWCoVC3ZykdRtCCHHgwAFRp04dYWZmJszNzUWVKlXEli1bhBCfXt+vX/svm8i0fj4AiAEDBojNmzeLUqVKCQMDA7F79271fRMnTlQvGx4eLgYPHqz+XNra2go3Nzdx5cqVb2b6/B5ev369xuPfuXNHtG3bVuTNm1cYGRmJYsWKJWkGk1OgQAHRtWvXJPXkmlkhhGjTpo0AIF69eqWubdu2TQAQU6ZMSfYxPn78KKysrESJEiXUte3btwsAYvr06d/M+Nn169cFALFr165Ul9P2vevl5ZXsHw6f39NfSm4/+/r6Cmtr62Rfx7CwMKFQKMTvv/+urqX1PUW5R9q/syHSQZ+/YrS2tlbXbt26hZo1a8LJyQmjRo2CqakpfH190aJFC+zcuRMtW7YEAERGRqJ27dq4c+cOunfvjkqVKiE0NBT+/v548eIF8ubNC5VKhWbNmuHMmTPo3bs3SpYsiRs3bmDBggW4f/8+9uzZk2yuKlWqoHDhwvD19YWXl5fGfT4+PrC2toa7uzuAT1MB/ve//0Emk2HgwIGwtbXFwYMH0aNHD4SHh+O3337TWH/q1KmQy+UYNmwY4uLiUvx6/a+//gIAdOnSJdn7DQwM0KFDB0yePBlnz56Fm5ub+r6NGzciIiICAwYMQGxsLBYtWoQGDRrgxo0bsLe31+p1/qx///6wtbXFhAkTEBUVBQC4fPkyzp07h3bt2iF//vwICgrC8uXLUa9ePdy+fRsmJiaoU6cOfv31VyxevBhjxoxByZIlAUD935TMmjULenp6GDZsGMLCwvDHH3+gY8eOuHjxonqZ5cuXY+DAgahduzaGDBmCoKAgtGjRAtbW1t/8evXgwYNITExE586dU13uax4eHihUqBBmzpyJq1evYs2aNbCzs8Ps2bM1cpUuXRrNmjWDgYEB/vrrL/Tv3x8qlQoDBgzQ2N69e/fQvn179OnTB7169ULx4sW12oa3tze6d++O0qVLY/To0bCyssK1a9cQEBCADh06YOzYsQgLC8OLFy+wYMECAICZmRkAaP35OH78OHx9fTFw4EDkzZsXLi4uyb5Gffv2hZ+fHwYOHIhSpUrh3bt3OHPmDO7cuYNKlSqlmik5//77L2rXrg1DQ0P07t0bLi4uePToEf76668k0wG+9PLlSzx79gyVKlVKcZmvfT4AzcrKSl371mfR0tISzZs3x4YNG/Dw4UMUKVIE/v7+AKDV+6tUqVIwNjbG2bNnk3z+vpTe925afb2fixYtipYtW2LXrl1YuXKlxu+sPXv2IC4uDu3atQOg/XuKcgmpu2mijPB5dO7o0aMiJCREPH/+XPj5+QlbW1uhUCg0vg5r2LChKFu2rMZf8SqVStSoUUMULVpUXZswYUKKoxifv1LctGmT0NPTS/I134oVKwQAcfbsWXXty5FZIYQYPXq0MDQ0FO/fv1fX4uLihJWVlcZoaY8ePUS+fPlEaGioxmO0a9dOWFpaqkdNP484Fi5cOE1fJbdo0UIASHHkVgghdu3aJQCIxYsXCyH+G9UyNjYWL168UC938eJFAUAMGTJEXUvr6/x539WqVUvjq1chRLLP4/OI8saNG9W11KYZpDQyW7JkSREXF6euL1q0SABQjzDHxcWJPHnyiB9++EEkJCSol/P29hYAvjkyO2TIEAFAXLt2LdXlPvs8ivX1SHnLli1Fnjx5NGrJvS7u7u6icOHCGrWCBQsKACIgICDJ8mnZxsePH4W5ubmoVq1akq+Cv/xaPaWv9LX5fAAQenp64tatW0m2g69GZi0tLcWAAQOSLPellDIlNzJbp04dYW5uLp4+fZric0zO0aNHk3yL8lndunVFiRIlREhIiAgJCRF3794Vw4cPFwDEzz//rLFshQoVhKWlZaqPNX/+fAFA+Pv7CyGEqFix4jfXSU6xYsXETz/9lOoy2r53tR2ZTW4/Hzp0KNnXskmTJhrvSW3eU5R78GwGlKO4ubnB1tYWzs7OaNOmDUxNTeHv768eRXv//j2OHz8ODw8PREREIDQ0FKGhoXj37h3c3d3x4MED9dkPdu7cifLlyyc7giGTyQAAO3bsQMmSJVGiRAn1tkJDQ9GgQQMAwIkTJ1LM6unpiYSEBOzatUtdO3z4MD5+/AhPT08Anw5W2blzJ5o2bQohhMZjuLu7IywsDFevXtXYrpeXF4yNjb/5WkVERAAAzM3NU1zm833h4eEa9RYtWsDJyUl9u2rVqqhWrRoOHDgAQLvX+bNevXolOSDny+eRkJCAd+/eoUiRIrCyskryvLXVrVs3jRGg2rVrA/h0UA0A/PPPP3j37h169eqlceBRx44dNUb6U/L5NUvt9U1O3759NW7Xrl0b796909gHX74uYWFhCA0NRd26dfH48WOEhYVprF+oUCH1KP+X0rKNI0eOICIiAqNGjUpyAOXnz0BqtP181K1bF6VKlfrmdq2srHDx4kWNo/XTKyQkBKdOnUL37t1RoEABjfu+9RzfvXsHACm+H+7evQtbW1vY2tqiRIkSmDNnDpo1a5bktGARERHffJ98/VkMDw/X+r31Oeu3Tv+W3vduWiW3nxs0aIC8efPCx8dHXfvw4QOOHDmi/n0IfN/vXMq5OM2AcpSlS5eiWLFiCAsLw7p163Dq1CkoFAr1/Q8fPoQQAuPHj8f48eOT3cbbt2/h5OSER48eoXXr1qk+3oMHD3Dnzh3Y2tqmuK2UlC9fHiVKlICPjw969OgB4NMUg7x586p/MYeEhODjx49YtWoVVq1alabHKFSoUKqZP/v8D1VERITGV55fSqnhLVq0aJJlixUrBl9fXwDavc6p5Y6JicHMmTOxfv16vHz5UuNUYV83bdr6unH53JB8+PABANTnDC1SpIjGcgYGBil+/f0lCwsLAP+9hhmR6/M2z549i4kTJ+L8+fOIjo7WWD4sLAyWlpbq2ym9H9KyjUePHgEAypQpo9Vz+Ezbz0da37t//PEHvLy84OzsjMqVK6NJkybo0qULChcurHXGz3+8pPc5AkjxFHYuLi5YvXo1VCoVHj16hOnTpyMkJCTJHwbm5ubfbDC//ixaWFios2ub9VtNenrfu2mV3H42MDBA69atsXXrVsTFxUGhUGDXrl1ISEjQaGa/53cu5VxsZilHqVq1KqpUqQLg0+hhrVq10KFDB9y7dw9mZmbq8zsOGzYs2dEqIGnzkhqVSoWyZcti/vz5yd7v7Oyc6vqenp6YPn06QkNDYW5uDn9/f7Rv3149Evg5b6dOnZLMrf2sXLlyGrfTMioLfJpTumfPHvz777+oU6dOssv8+++/AJCm0bIvped1Ti73oEGDsH79evz222+oXr06LC0tIZPJ0K5duxTP1ZlWKZ2WKaXGRFslSpQAANy4cQMVKlRI83rfyvXo0SM0bNgQJUqUwPz58+Hs7Ay5XI4DBw5gwYIFSV6X5F5XbbeRXtp+PtL63vXw8EDt2rWxe/duHD58GHPmzMHs2bOxa9cu/PTTT9+dO63y5MkD4L8/gL5mamqqMde8Zs2aqFSpEsaMGYPFixer6yVLlkRgYCCePXuW5I+Zz77+LJYoUQLXrl3D8+fPv/l75ksfPnxI9o/RL2n73k2pOVYqlcnWU9rP7dq1w8qVK3Hw4EG0aNECvr6+KFGiBMqXL69e5nt/51LOxGaWcix9fX3MnDkT9evXx5IlSzBq1Cj1yI2hoaHGPzLJcXV1xc2bN7+5zPXr19GwYcM0fe36NU9PT0yePBk7d+6Evb09wsPD1Qc6AICtrS3Mzc2hVCq/mVdbv/zyC2bOnImNGzcm28wqlUps3boV1tbWqFmzpsZ9Dx48SLL8/fv31SOW2rzOqfHz84OXlxfmzZunrsXGxuLjx48ay6Xntf+WzyfAf/jwIerXr6+uJyYmIigoKMkfEV/76aefoK+vj82bN2fogTR//fUX4uLi4O/vr9H4aPP1alq34erqCgC4efNmqn/kpfT6f+/nIzX58uVD//790b9/f7x9+xaVKlXC9OnT1c1sWh/v83v1W5/15Hxu+p48eZKm5cuVK4dOnTph5cqVGDZsmPq1/+WXX7Bt2zZs3LgR48aNS7JeeHg49u7dixIlSqj3Q9OmTbFt2zZs3rwZo0ePTtPjJyYm4vnz52jWrFmqy2n73rW2tk7ymQSg9RXR6tSpg3z58sHHxwe1atXC8ePHMXbsWI1lMvM9RbqLc2YpR6tXrx6qVq2KhQsXIjY2FnZ2dqhXrx5WrlyJ169fJ1k+JCRE/f+tW7fG9evXsXv37iTLfR4l8/DwwMuXL7F69eoky8TExKiPyk9JyZIlUbZsWfj4+MDHxwf58uXTaCz19fXRunVr7Ny5M9l/bL/Mq60aNWrAzc0N69evT/YKQ2PHjsX9+/cxYsSIJCMpe/bs0ZjzeunSJVy8eFHdSGjzOqdGX18/yUjpn3/+mWTEx9TUFACS/Qc1vapUqYI8efJg9erVSExMVNe3bNmS4kjcl5ydndGrVy8cPnwYf/75Z5L7VSoV5s2bhxcvXmiV6/PI7ddTLtavX5/h22jUqBHMzc0xc+ZMxMbGatz35bqmpqbJTvv43s9HcpRKZZLHsrOzg6OjI+Li4r6Z6Wu2traoU6cO1q1bh2fPnmnc961ReicnJzg7O2t1NawRI0YgISFBY2SxTZs2KFWqFGbNmpVkWyqVCv369cOHDx8wceJEjXXKli2L6dOn4/z580keJyIiIkkjePv2bcTGxqJGjRqpZtT2vevq6oqwsDD16DEAvH79OtnfnanR09NDmzZt8Ndff2HTpk1ITEzUmGIAZM57inQfR2Ypxxs+fDjatm0Lb29v9O3bF0uXLkWtWrVQtmxZ9OrVC4ULF8abN29w/vx5vHjxQn25x+HDh6uvLNW9e3dUrlwZ79+/h7+/P1asWIHy5cujc+fO8PX1Rd++fXHixAnUrFkTSqUSd+/eha+vLw4dOqSe9pAST09PTJgwAUZGRujRowf09DT/xpw1axZOnDiBatWqoVevXihVqhTev3+Pq1ev4ujRo3j//n26X5uNGzeiYcOGaN68OTp06IDatWsjLi4Ou3btwsmTJ+Hp6Ynhw4cnWa9IkSKoVasW+vXrh7i4OCxcuBB58uTBiBEj1Muk9XVOzS+//IJNmzbB0tISpUqVwvnz53H06FH117ufVahQAfr6+pg9ezbCwsKgUCjQoEED2NnZpfu1kcvlmDRpEgYNGoQGDRrAw8MDQUFB8Pb2hqura5pGhebNm4dHjx7h119/xa5du/DLL7/A2toaz549w44dO3D37l2Nkfi0aNSoEeRyOZo2bYo+ffogMjISq1evhp2dXbJ/OHzPNiwsLLBgwQL07NkTP/zwAzp06ABra2tcv34d0dHR2LBhAwCgcuXK8PHxwdChQ/HDDz/AzMwMTZs2zZDPx9ciIiKQP39+tGnTRn0J16NHj+Ly5csaI/gpZUrO4sWLUatWLVSqVAm9e/dGoUKFEBQUhP379yMwMDDVPM2bN8fu3bvTNBcV+DRNoEmTJlizZg3Gjx+PPHnyQC6Xw8/PDw0bNkStWrU0rgC2detWXL16Fb///rvGe8XQ0BC7du2Cm5sb6tSpAw8PD9SsWROGhoa4deuW+luVL08tduTIEZiYmODHH3/8Zk5t3rvt2rXDyJEj0bJlS/z666+Ijo7G8uXLUaxYMa0P1PT09MSff/6JiRMnomzZsklOsZcZ7ynKAbL+BApEGS+liyYI8ekKM66ursLV1VV96qdHjx6JLl26CAcHB2FoaCicnJzEL7/8Ivz8/DTWfffunRg4cKBwcnJSn5zby8tL4zRZ8fHxYvbs2aJ06dJCoVAIa2trUblyZTF58mQRFhamXu7rU3N99uDBA/WJ3c+cOZPs83vz5o0YMGCAcHZ2FoaGhsLBwUE0bNhQrFq1Sr3M51NO7dixQ6vXLiIiQkyaNEmULl1aGBsbC3Nzc1GzZk3h7e2d5NREX140Yd68ecLZ2VkoFApRu3Ztcf369STbTsvrnNq++/Dhg+jWrZvImzevMDMzE+7u7uLu3bvJvparV68WhQsXFvr6+mm6aMLXr1NKJ9NfvHixKFiwoFAoFKJq1ari7NmzonLlyqJx48ZpeHU/XS1pzZo1onbt2sLS0lIYGhqKggULim7dummc+iilK4B9fn2+vFCEv7+/KFeunDAyMhIuLi5i9uzZYt26dUmW+3zRhOSkdRufl61Ro4YwNjYWFhYWomrVqmLbtm3q+yMjI0WHDh2ElZVVkosmpPXzgf8/mX5y8MWpueLi4sTw4cNF+fLlhbm5uTA1NRXly5dPcsGHlDKltJ9v3rwpWrZsKaysrISRkZEoXry4GD9+fLJ5vnT16lUBIMmpolK6aIIQQpw8eTLJ6caEEOLt27di6NChokiRIkKhUAgrKyvh5uamPh1Xcj58+CAmTJggypYtK0xMTISRkZEoU6aMGD16tHj9+rXGstWqVROdOnX65nP6LK3vXSGEOHz4sChTpoyQy+WiePHiYvPmzaleNCElKpVKODs7CwBi2rRpyS6T1vcU5R4yITLoaAciyvGCgoJQqFAhzJkzB8OGDZM6jiRUKhVsbW3RqlWrZL/qpNynYcOGcHR0xKZNm6SOkqLAwEBUqlQJV69e1eqARCJdwDmzREQpiI2NTTJvcuPGjXj//j3q1asnTSjKdmbMmAEfHx+tD3jKSrNmzUKbNm3YyFKOxDmzREQpuHDhAoYMGYK2bdsiT548uHr1KtauXYsyZcqgbdu2UsejbKJatWqIj4+XOkaqtm/fLnUEokzDZpaIKAUuLi5wdnbG4sWL8f79e9jY2KBLly6YNWuWxtXDiIhIOpwzS0REREQ6i3NmiYiIiEhnsZklIiIiIp2V6+bMqlQqvHr1Cubm5rwUHhEREVE2JIRAREQEHB0dk1xM6Gu5rpl99eoVnJ2dpY5BRERERN/w/Plz5M+fP9Vlcl0za25uDuDTi2NhYSFxGiIiIiL6Wnh4OJydndV9W2pyXTP7eWqBhYUFm1kiIiKibCwtU0J5ABgRERER6Sw2s0RERESks9jMEhEREZHOYjNLRERERDqLzSwRERER6Sw2s0RERESks9jMEhEREZHOYjNLRERERDqLzSwRERER6Sw2s0RERESks9jMEhEREZHOYjNLRERERDqLzSwRERER6Sw2s0RERESksyRtZk+dOoWmTZvC0dERMpkMe/bs+eY6J0+eRKVKlaBQKFCkSBF4e3tnek4iIiIiyp4kbWajoqJQvnx5LF26NE3LP3nyBD///DPq16+PwMBA/Pbbb+jZsycOHTqUyUmJiIiIKDsykPLBf/rpJ/z0009pXn7FihUoVKgQ5s2bBwAoWbIkzpw5gwULFsDd3T2zYhIRERFRNiVpM6ut8+fPw83NTaPm7u6O3377LcV14uLiEBcXp74dHh6eWfGISBfd2wGcmwDER0idhIgo23n41gJ9ttTC6k6nUdg2AjB1ADr9I3UsDTrVzAYHB8Pe3l6jZm9vj/DwcMTExMDY2DjJOjNnzsTkyZOzKiIR6ZpzE4D3d6VOQUSU7fgGlkbPHc0QEadAu1W1cWbAOsilDpUMnWpm02P06NEYOnSo+nZ4eDicnZ0lTERE2crnEVmZHmCaT9osRETZQEy8PobsqI6Vp0uqax/jzPBaWQQFTc0kTJY8nWpmHRwc8ObNG43amzdvYGFhkeyoLAAoFAooFIqsiEdEusw0H9DnhdQpiIgkde9eKDw8/PDvv//1Wx06lMWKFT/D3Hy+hMlSplPNbPXq1XHgwAGN2pEjR1C9enWJEhERERHlDFu2/Is+ffYhKioBAGBkZIAlS35C9+4VIZPJJE6XMkmb2cjISDx8+FB9+8mTJwgMDISNjQ0KFCiA0aNH4+XLl9i4cSMAoG/fvliyZAlGjBiB7t274/jx4/D19cX+/fulegpEREREOi06OgG//noQa9deU9dKlMiLHTvaokwZOwmTpY2k55n9559/ULFiRVSsWBEAMHToUFSsWBETJkwAALx+/RrPnj1TL1+oUCHs378fR44cQfny5TFv3jysWbOGp+UiIiIiSqeLF19oNLJeXuXxzz+9dKKRBQCZEEJIHSIrhYeHw9LSEmFhYbCwsJA6DhFJbWV+IPIlYObEObNElGuNGnUUf/55CcuWNYGXVwWp42jVr+nUnFkiIiIi+j4xMQkwMjLQmAc7dWp99OhREUWL5pEwWfpIOs2AiIiIiLLOjRtvUKnSKixfrnnhA0NDfZ1sZAGOzGY+Xl2IKHuLei11AiKiTCeEwJo1V/HrrwGIjU3EkCGHUL16flSsqPvn12Yzm9l4dSEi3SA3lzoBEVGmiIiIQ58++7Bt2011rWTJvDAzy47X89Iem9nMxqsLEWV/cnOg5lSpUxARZbhr117Dw8MPDx++V9f696+CefPcYWSUM9rAnPEsdAGvLkRERERZRAiB5cv/wdChhxAXpwQAWFgosGZNU7RtW1ridBmLzSwRERFRDhIWFouePf+Cn99tda1y5Xzw8WkDV1cbCZNlDp7NgIiIiCgHEQL4559X6tu//loVZ892z5GNLMBmloiIiChHsbIygo9PG9jZmWL3bk8sWvQTFIqc+2V8zn1mRERERLnAhw8xiItTwsHBTF2rWtUJT54MhomJoYTJsgZHZomIiIh01IULL1Cx4kq0a+eHxESVxn25oZEF2MwSERER6RyVSmDu3HOoXXs9nj4Nw99/P8Xs2WekjiUJTjMgIiIi0iGhodHo2nUP9u9/oK7VrOmMLl3KS5hKOmxmiYiIiHTEmTPP0L79Trx4Ea6ujRpVE1Om1Iehob6EyaTDZpaIiIgom1OpBGbPPoPx409AqRQAgLx5TbBpU0s0blxE4nTSYjNLRERElI3FxyvRrNk2HDr0SF2rW7cgtm5tDUdHcwmTZQ88AIyIiIgoG5PL9VGokBUAQCYDxo+vg6NHu7CR/X8cmSUiIiLK5hYsaIwnTz5i2LAacHMrLHWcbIXNLBEREVE2EhwciX//fYNGjVzVNSMjAwQEdJIwVfbFaQZERERE2cTRo49RocIKtGrlg7t3Q6WOoxPYzBIRERFJLDFRhfHjj6NRo0148yYKUVEJ+O23AKlj6QROMyAiIiKS0MuX4ejQYRdOnXqqrjVuXAQbN7aQLpQOYTNLREREJJGAgIfo3Hk3QkOjAQD6+jJMn94Aw4fXhJ6eTOJ0uoHNLBEREVEWS0hQYvz4E5g9+6y6lj+/BbZvb42aNQtImEz3sJklIiIiymIdOuyCn99t9e1ffikGb+/myJPHRMJUuokHgBERERFlsf79q0BPTwYDAz3Mnfsj/P3bsZFNJ47MEhEREWWx+vULYdGixqhSxRH/+19+qePoNI7MEhEREWWioKCPGDXqKFQqoVEfOLAqG9kMwJFZIiIiokyye/cddO/uj48fY5EnjzGGD68pdaQchyOzRERERBksLi4Rv/56EK1a+eLjx1gAwNq11xAXlyhxspyHI7NEREREGejRo/fw9PTDlSuv1bW2bUth9eqmUCjYemU0vqJEREREGWTHjlvo2fMvhIfHAQAUCn0sWOCOvn2rQCbjRRAyA5tZIiIiou8UG5uIoUMPYfnyf9S1okVt4OvbFhUqOEiYLOdjM0tERET0naZPP6XRyHboUBYrVvwMc3OFhKlyBx4ARkRERPSdRoyoiWLF8sDIyACrVzfF5s0t2chmEY7MEhEREX0nc3MF/PzaAgDKlrWXOE3uwpFZIiIiIi3cuROCOnXWIyjoo0a9bFl7NrISYDNLRERElEYbNgSiSpXVOH36GTw9/RAfr5Q6Uq7HZpaIiIjoG6Ki4tG16x507boX0dEJAIDo6ASEhERJnIw4Z5aIiIgoFTduvIGHhx/u3g1V13r2rIhFi36CiYmhhMkIYDNLRERElCwhBNauvYZBgw4iNvbTZWjNzORYufIXdOhQVuJ09BmbWSIiIqKvRETEoW/f/di69Ya6Vr68PXx926JYsTwSJqOvcc4sERER0VfOn3+h0cj27VsZFy70ZCObDbGZJSIiIvpKo0au+P336jA3l8PHpw2WL/8FRkb8Qjs74l4hIiKiXC8qKh4mJoaQyWTq2owZDTFgwA8oVMhawmT0LRyZJSIiolztn39eoVy5FVi16opGXS7XZyOrA9jMEhERUa4khMDixRdRo8ZaPH78AYMHB+D69WCpY5GWOM2AiIiIcp0PH2LQo4c/du++q66VL+8AS0sjCVNRerCZJSIiolzl4sUX8PT0w9OnYera779Xx4wZDSGX60uYjNKDzSwRERHlCkIIzJ9/HqNGHUNiogoAYGNjDG/v5mjatLjE6Si92MwSERFRjvf+fQy8vPZg37776lrNms7Ytq01nJ0tJUxG34sHgBEREVGu8O+/b9T/P2pUTZw44cVGNgdgM0tEREQ5no2NMXx82iBfPjMcPNgRM2e6wdCQ82NzAk4zICIiohwnJCQKKpWAvb2Zuva//+XH48eDeSWvHIYjs0RERJSjnDr1FBUqrET79juhVKo07mMjm/OwmSUiIqIcQalUYdq0U6hffwNevYrAiRNBmDv3nNSxKJPxzxMiIiLSecHBkejUaReOHXuirjVoUAheXhWkC0VZgs0sERER6bRjxx6jY8ddePMmCgCgpyfDpEl1MWZMbejr80vonI7NLBEREekkpVKFKVP+xtSppyDEp1q+fGbYurU16tVzkTQbZR02s0RERKRzYmMT0bjxZvz991N1rVEjV2za1BJ2dqYSJqOsxrF3IiIi0jlGRgYoViwPAEBfX4aZMxvi4MGObGRzIY7MEhERkU5atKgxXr6MwOjRtVCrVgGp45BE2MwSERFRtvf8eRju3AlFo0au6pqxsSH27+8gYSrKDjjNgIiIiLK1/fvvo0KFlWjd2hf377+TOg5lM2xmiYiIKFtKSFBi2LDD+OWXbXj/PgaRkfEYPvyI1LEom+E0AyIiIsp2goI+ol07P1y8+FJda9GiBNatayZhKsqO2MwSERFRtrJnz11067YXHz/GAgAMDfUwd24jDBpUFTKZTOJ0lN2wmSUiIqJsIS4uESNHHsWiRRfVtcKFreHj0wZVqjhKmIyyMzazRERElC20abMD+/bd/+J2KaxZ0xSWlkYSpqLsjgeAERERUbbw22/VIJMBCoU+li1rAl/fNmxk6Zs4MktERETZQsOGhfHnnz+hZs0CqFDBQeo4pCM4MktERERZ7sGDdxgx4giEEBr1AQOqspElrXBkloiIiLLUtm030Lv3PkRGxiNfPjMMGVJd6kikwyQfmV26dClcXFxgZGSEatWq4dKlS6kuv3DhQhQvXhzGxsZwdnbGkCFDEBsbm0VpiYiIKL1iYhLQq5c/OnTYhcjIeACAt/d1JCQoJU5GukzSZtbHxwdDhw7FxIkTcfXqVZQvXx7u7u54+/Ztsstv3boVo0aNwsSJE3Hnzh2sXbsWPj4+GDNmTBYnJyIiIm3cuROCqlXXYM2aa+paly7lcfZsdxga6kuYjHSdpM3s/Pnz0atXL3Tr1g2lSpXCihUrYGJignXr1iW7/Llz51CzZk106NABLi4uaNSoEdq3b//N0VwiIiKSzsaN11GlymrcvPlpsMrExBDr1zfHhg0tYGYmlzgd6TrJmtn4+HhcuXIFbm5u/4XR04ObmxvOnz+f7Do1atTAlStX1M3r48ePceDAATRp0iTFx4mLi0N4eLjGDxEREWW+qKh4dOu2F15eexAdnQAAKF3aFpcv90LXrhWkDUc5hmQHgIWGhkKpVMLe3l6jbm9vj7t37ya7TocOHRAaGopatWpBCIHExET07ds31WkGM2fOxOTJkzM0OxEREX3blCl/w9s7UH27R4+KWLz4J5iYGEoXinIcyQ8A08bJkycxY8YMLFu2DFevXsWuXbuwf/9+TJ06NcV1Ro8ejbCwMPXP8+fPszAxERFR7jV2bB0UKWIDU1NDbN7cEmvWNGMjSxlOspHZvHnzQl9fH2/evNGov3nzBg4OyZ9fbvz48ejcuTN69uwJAChbtiyioqLQu3dvjB07Fnp6SXtzhUIBhUKR8U+AiIiINAghIJPJ1LctLBTYtcsDcrk+ihfPK2EyyskkG5mVy+WoXLkyjh07pq6pVCocO3YM1asnf7656OjoJA2rvv6nIyC/PukyERERZZ3r14NRo8Y6PHsWplEvW9aejSxlKkmnGQwdOhSrV6/Ghg0bcOfOHfTr1w9RUVHo1q0bAKBLly4YPXq0evmmTZti+fLl2L59O548eYIjR45g/PjxaNq0qbqpJSIioqwjhMCKFf+gWrU1uHDhBdq338nzxlKWkvQKYJ6enggJCcGECRMQHByMChUqICAgQH1Q2LNnzzRGYseNGweZTIZx48bh5cuXsLW1RdOmTTF9+nSpngIREVGuFRYWi96998HX95a6FhubiPfvY2BvbyZhMspNZCKXfT8fHh4OS0tLhIWFwcLCIvMfcGV+IPIlYOYE9HmR+Y9HRESUBa5ceQVPTz88evRBXRs0qCrmzPkRCoWkY2WUA2jTr/HdRkRERGkmhMCSJZcwbNgRxMd/mk5gZWWEdeuaoWXLkhKno9yIzSwRERGlyYcPMejRwx+7d/93PviqVZ3g49MGLi5W0gWjXE2nzjNLRERE0jl37rlGI/v779Vx+nQ3NrIkKTazRERElCY//1wMgwdXg42NMfz922Hu3EaQy3k2IZIWpxkQERFRsiIi4mBmJte4EMIff/yIYcNqIH/+LDiImigNODJLRERESZw79xylSy/DunXXNOpyuT4bWcpW2MwSERGRmkolMHv2GdSpsx7Pn4dj0KCDuHnzrdSxiFLEaQZEREQEAAgJiUKXLnsQEPBQXatSxRHW1kYSpiJKHZtZIiIiwqlTT9G+/U68ehUBAJDJgLFja2PixHowMOAXuZR9sZklIiLKxZRKFWbOPIOJE09Cpfp0UVA7O1Ns2dIKbm6FJU5H9G1sZomIiHKpt2+j0LHjLhw9+lhda9CgEDZvbol8+cwlTEaUdvzegIiIKJfS15fh7t1QAICengyTJ9fD4cOd2MiSTmEzS0RElEvlyWOCbdtaw9nZAseOdcGECXWhr8/WgHQLpxkQERHlEq9eRcDAQA92dqbqWq1aBfDgwSAoFGwJSDfxzy8iIqJc4PDhR6hQYQU6ddqlPtDrMzaypMvYzBIREeVgiYkqjBlzDO7umxESEo0jRx5j4cILUsciyjD8U4yIiCiHevEiHO3b78SZM8/UtSZNiqJLl/ISpiLKWGxmiYiIcqD9++/Dy2sP3r2LAQAYGOhh5syGGDq0OvT0ZBKnI8o4bGaJiIhykIQEJcaMOYa5c8+rawUKWGL79taoXt1ZwmREmYPNLBERUQ4RHZ2Ahg034sKFF+pa8+bFsW5dc9jYGEuYjCjz8AAwIiKiHMLExBAlS+YFABga6mHhQnfs3u3JRpZyNI7MEhER5SBLljRBSEg0Jkyogx9+cJI6DlGmYzNLRESkox4//oAHD97B3b2IumZiYoi//movYSqirMVpBkRERDrIz+82KlZcibZtd+Dhw/dSxyGSDJtZIiIiHRIbm4gBA/ajbdsdCA+PQ0REPEaPPiZ1LCLJcJoBERGRjnjw4B08Pf1w7VqwutauXRmsXPmLhKmIpMVmloiISAds334TvXr9hcjIeACAkZEBFi9ujJ49K0Em40UQKPdiM0tERJSNxcQk4LffArBq1VV1rXjxPPD1bYty5ewlTEaUPbCZJSIiysaaNduOo0cfq2937lwOy5b9DDMzuYSpiLIPHgBGRESUjQ0bVh0AYGxsgPXrm2PjxpZsZIm+wJFZIiKibMzdvQiWLPkJ9esXQqlStlLHIcp2ODJLRESUTdy69RbDhh2GEEKjPmBAVTayRCngyCwREZHEhBBYvz4QAwceQExMIgoUsMSvv1aTOhaRTuDILBERkYQiI+PRpcse9Ojhj5iYRADApk3/QqlUSZyMSDdwZJaIiEgi168Hw8PDD/fvv1PX+vSpjAUL3KGvz/EmorRgM0tERJTFhBBYteoKBg8OQFycEgBgbi7HqlVN0a5dGYnTEekWNrNERERZKDw8Dr17/wUfn1vqWqVK+eDj0wZFithImIxIN/E7DCIioiw0YcIJjUZ24MAfcO5cdzayROnEZpaIiCgLTZ5cD4ULW8PSUgE/v7b4888mUCj4RSlRevHTQ0RElImEEJDJZOrblpZG2L3bE+bmchQqZC1hMqKcgSOzREREmeTSpZeoWnUNXrwI16iXK2fPRpYog7CZJSIiymBCCCxYcB61aq3DP/+8Qvv2O5GYyPPGEmUGTjMgIiLKQO/fx6Bbt73w97+nrimVKnz8GIu8eU0kTEaUM7GZJSIiyiDnzz+Hp6cfnj//b1rBiBE1MG1aAxga6kuYjCjnYjNLRET0nVQqgblzz2HMmGNQKgUAIE8eY2zc2BJNmhSVOB1RzsZmloiI6DuEhETBy2sPDh58qK7VqlUA27a1Rv78FhImI8odeAAYERHRdzh37rm6kZXJgLFja+PECS82skRZhM0sERHRd2jevAQGDvwBdnamOHSoE6ZNawADA/7zSpRV+GkjIiLSQlhYbJLa3LmNcP16X/z4o6sEiYhyNzazREREaXTixBOUKLEU3t6BGnWFwgAODmbShCLK5djMEhERfYNSqcLkySfh5rYJwcGRGDDgAG7fDpE6FhGBZzMgIiJK1evXEejYcRdOnAhS12rWdOYFEIiyCTazREREKThy5BE6ddqNt2+jAAB6ejJMnVofo0bVgp6eTOJ0RASwmSUiIkoiMVGFSZNOYsaM0xCfroEAJydzbNvWGrVrF5Q2HBFpYDNLRET0hdevI+Dp6YfTp5+paz/9VAQbN7bk1AKibIgHgBEREX3BwEAPjx59AADo68vwxx9u2LevAxtZomyKzSwREdEXbG1NsW1baxQqZIXTp7th+PCanB9LlI1xmgEREeVqz56FwdjYALa2pupanToFce/eQBga6kuYjIjS4rtGZmNjk14FhYiISFf4+99DhQor0KXLHqhUQuM+NrJEukHrZlalUmHq1KlwcnKCmZkZHj9+DAAYP3481q5dm+EBiYiIMlp8vBJDhgSgefPt+PAhFgEBD7Fs2WWpYxFROmjdzE6bNg3e3t74448/IJfL1fUyZcpgzZo1GRqOiIgooz158gG1aq3DwoUX1bXWrUuiU6dyEqYiovTSupnduHEjVq1ahY4dO0Jf/7+vYMqXL4+7d+9maDgiIqKMtGvXHVSsuBKXL78CAMjl+liy5Cfs2NEWVlZGEqcjovTQ+gCwly9fokiRIknqKpUKCQkJGRKKiIgoI8XGJmL48MNYsuS/qQSurtbw9W2LSpXySZiMiL6X1s1sqVKlcPr0aRQsqHkFFD8/P1SsWDHDghEREWWEiIg41K3rjWvXgtU1T8/SWLWqKSwsFBImI6KMoHUzO2HCBHh5eeHly5dQqVTYtWsX7t27h40bN2Lfvn2ZkZGIiCjdzM0VKFvWHteuBUOh0MfixT+hV69KkMl47liinEDrObPNmzfHX3/9haNHj8LU1BQTJkzAnTt38Ndff+HHH3/MjIxERETfZdmyJmjevDguXeqF3r0rs5ElykHSddGE2rVr48iRIxmdhYiI6LvduxeKp0/D0KiRq7pmairHnj3tJExFRJlF65HZwoUL4927d0nqHz9+ROHChTMkFBERUXps3vwvKldeBQ+PHXj8+IPUcYgoC2jdzAYFBUGpVCapx8XF4eXLlxkSioiISBvR0Qno3n0vOnfejaioBISFxWHixJNSxyKiLJDmaQb+/v7q/z906BAsLS3Vt5VKJY4dOwYXF5cMDUdERPQtt269hYeHH27fDlHXunWrgD///EnCVESUVdLczLZo0QIAIJPJ4OXlpXGfoaEhXFxcMG/evAwNR0RElBIhBLy9AzFgwAHExCQCAExNDbF8+c/o3Lm8xOmIKKukuZlVqVQAgEKFCuHy5cvImzdvpoUiIiJKTWRkPPr3349Nm/5V18qWtYOvb1uUKMF/n4hyE63PZvDkyZPMyEFERJQmQgg0abIFp08/U9f69KmMBQvcYWxsKGEyIpKC1geAAUBUVBQOHDiAFStWYPHixRo/2lq6dClcXFxgZGSEatWq4dKlS6ku//HjRwwYMAD58uWDQqFAsWLFcODAgfQ8DSIi0kEymQyjRtUCAJiby7FtW2usWPELG1miXErrkdlr166hSZMmiI6ORlRUFGxsbBAaGgoTExPY2dnh119/TfO2fHx8MHToUKxYsQLVqlXDwoUL4e7ujnv37sHOzi7J8vHx8fjxxx9hZ2cHPz8/ODk54enTp7CystL2aRARkQ5r0qQoliz5Ce7uRVCkiI3UcYhIQlqPzA4ZMgRNmzbFhw8fYGxsjAsXLuDp06eoXLky5s6dq9W25s+fj169eqFbt24oVaoUVqxYARMTE6xbty7Z5detW4f3799jz549qFmzJlxcXFC3bl2UL8+J/kREOdW1a68xbNhhCCE06gMGVGUjS0TaN7OBgYH4/fffoaenB319fcTFxcHZ2Rl//PEHxowZk+btxMfH48qVK3Bzc/svjJ4e3NzccP78+WTX8ff3R/Xq1TFgwADY29ujTJkymDFjRrLnvf0sLi4O4eHhGj9ERJT9CSGwdOkl/O9/azFv3nksX/6P1JGIKBvSupk1NDSEnt6n1ezs7PDs2acJ+JaWlnj+/HmatxMaGgqlUgl7e3uNur29PYKDg5Nd5/Hjx/Dz84NSqcSBAwcwfvx4zJs3D9OmTUvxcWbOnAlLS0v1j7Ozc5ozEhGRND5+jEXbtjswcOBBxMd/GrDYtu0mVCrxjTWJKLfRes5sxYoVcfnyZRQtWhR169bFhAkTEBoaik2bNqFMmTKZkVFNpVLBzs4Oq1atgr6+PipXroyXL19izpw5mDhxYrLrjB49GkOHDlXfDg8PZ0NLRJSNXb78Ep6efnjy5KO69ttv1TB79o/Q05NJF4yIsiWtm9kZM2YgIiICADB9+nR06dIF/fr1Q9GiRbF27do0bydv3rzQ19fHmzdvNOpv3ryBg4NDsuvky5cPhoaG0NfXV9dKliyJ4OBgxMfHQy6XJ1lHoVBAoVCkORcREUlDCIFFiy5ixIgjSEj4dG5zKysjeHs3R/PmJSROR0TZldbNbJUqVdT/b2dnh4CAgHQ9sFwuR+XKlXHs2DH11cVUKhWOHTuGgQMHJrtOzZo1sXXrVqhUKvVUh/v37yNfvnzJNrJERKQb3r+PQbdue+Hvf09d+9//8mP79tYoWNBKumBElO2l6zyzybl69Sp++eUXrdYZOnQoVq9ejQ0bNuDOnTvo168foqKi0K1bNwBAly5dMHr0aPXy/fr1w/v37zF48GDcv38f+/fvx4wZMzBgwICMehpERCSBsWOPaTSyI0bUwKlTXdnIEtE3aTUye+jQIRw5cgRyuRw9e/ZE4cKFcffuXYwaNQp//fUX3N3dtXpwT09PhISEYMKECQgODkaFChUQEBCgPijs2bNn6hFYAHB2dsahQ4cwZMgQlCtXDk5OThg8eDBGjhyp1eMSEVH2MmNGQwQEPEJERBw2bmyJJk2KSh2JiHSETHx94r4UrF27Fr169YKNjQ0+fPiAPHnyYP78+Rg0aBA8PT0xePBglCxZMrPzfrfw8HBYWloiLCwMFhYWmf+AK/MDkS8BMyegz4vMfzwiIh0ghIBMpnkw1/XrwciTxwT582fB72Yiyta06dfSPM1g0aJFmD17NkJDQ+Hr64vQ0FAsW7YMN27cwIoVK3SikSUiIumdPv0UlSuvwqtXERr18uUd2MgSkdbS3Mw+evQIbdu2BQC0atUKBgYGmDNnDvLnz59p4YiIKOdQqQRmzDiN+vU34Nq1YHTosBNKpUrqWESk49I8ZzYmJgYmJiYAAJlMBoVCgXz58mVaMCIiyjnevo1C5867cfjwI3VNJpMhPDwO1tbGEiYjIl2n1QFga9asgZmZGQAgMTER3t7eyJs3r8Yyv/76a8alIyIinXfixBN06LALwcGRAACZDJgwoS7Gj68Dff0MO6kOEeVSaT4AzMXFJclk/SQbk8nw+PHjDAmWWXgAGBFR1lAqVZg27RSmTDmlvgytg4MZtmxphQYNCkmcjoiyM236tTSPzAYFBX1vLiIiyiVev45Ap067cfz4E3XNza0wNm9uCXt7MwmTEVFOw+93iIgow50791zdyOrpyTBtWn0cOtSJjSwRZTg2s0RElOFaty6Fvn0rw9HRHCdOeGHs2DrQ00t9qhoRUXqwmSUiou/24UNMktqCBY0RGNgHdeoUlCAREeUWbGaJiOi7HDz4AMWKLcHmzf9q1I2MDGBraypRKiLKLdjMEhFRuiQkKDFy5BE0abIVoaHR6Nt3H+7eDZU6FhHlMulqZh89eoRx48ahffv2ePv2LQDg4MGDuHXrVoaGIyKi7OnZszDUq7cBf/xxTl1r0KAQbG1NJExFRLmR1s3s33//jbJly+LixYvYtWsXIiM/nQT7+vXrmDhxYoYHJCKi7MXf/x4qVFiBc+eeAwAMDPQwf34j7N3bDnnysJkloqyldTM7atQoTJs2DUeOHIFcLlfXGzRogAsXLmRoOCIiyj7i45UYOvQQmjffjg8fYgEALi5WOHu2O4YMqf7NC+sQEWUGrS5nCwA3btzA1q1bk9Tt7OwQGsq5UkREOdGzZ2Fo23YHLl16qa61alUSa9c2g5WVkYTJiCi303pk1srKCq9fv05Sv3btGpycnDIkFBERZS8KhT6ePQsDAMjl+vjzz5/g59eWjSwRSU7rZrZdu3YYOXIkgoODIZPJoFKpcPbsWQwbNgxdunTJjIxERCQxe3szbN3aCsWK5cG5c90xcGBVTisgomxB62Z2xowZKFGiBJydnREZGYlSpUqhTp06qFGjBsaNG5cZGYmIKIs9evQeoaHRGrX69Qvh1q3+qFzZUaJURERJaT1nVi6XY/Xq1Rg/fjxu3ryJyMhIVKxYEUWLFs2MfERElMV8fW+hZ09/1KlTEP7+7TUuQ2tgwNOTE1H2onUze+bMGdSqVQsFChRAgQIFMiMTERFJICYmAUOHHsKKFVcAAPv3P8Dq1VfQp08ViZMREaVM6z+xGzRogEKFCmHMmDG4fft2ZmQiIqIsdu9eKP73v7XqRhYAOnYsiw4dykqYiojo27RuZl+9eoXff/8df//9N8qUKYMKFSpgzpw5ePHiRWbkIyKiTLZly7+oXHkV/v33DQDA2NgAa9c2w6ZNLWFurpA4HRFR6rRuZvPmzYuBAwfi7NmzePToEdq2bYsNGzbAxcUFDRo0yIyMRESUCaKjE9Czpz86ddqNqKgEAEDJknlx6VIvdO9ekWcrICKdoPWc2S8VKlQIo0aNQvny5TF+/Hj8/fffGZWLiIgy0cePsahVax1u3QpR17p2rYAlS36Cqak8lTWJiLKXdB+WevbsWfTv3x/58uVDhw4dUKZMGezfvz8jsxERUSaxtFSgfHkHAICJiSE2bGiB9eubs5ElIp2j9cjs6NGjsX37drx69Qo//vgjFi1ahObNm8PExCQz8hERUSaQyWRYseJnxMYmYvr0BihRIq/UkYiI0kXrZvbUqVMYPnw4PDw8kDcvf/kREemCGzfe4PXrSDRq5KqumZsrsHOnh4SpiIi+n9bN7NmzZzMjBxERZQIhBNasuYpffw2AkZEBrl3rAxcXK6ljERFlmDQ1s/7+/vjpp59gaGgIf3//VJdt1qxZhgQjIqLvExERhz599mHbtpsAgNjYREyd+jfWrm0ucTIiooyTpma2RYsWCA4Ohp2dHVq0aJHicjKZDEqlMqOyERFROl279hoeHn54+PC9uta/fxXMm+cuYSoiooyXpmZWpVIl+/9ERJS9CCGwfPk/GDr0EOLiPg0uWFgosGZNU7RtW1ridEREGU/rU3Nt3LgRcXFxSerx8fHYuHFjhoQiIiLthYXFwsPDDwMGHFA3slWqOOLatT5sZIkox9K6me3WrRvCwsKS1CMiItCtW7cMCUVERNoRQuDHHzfBz++2ujZ4cDWcOdMNhQtbS5iMiChzad3MCiGSvcThixcvYGlpmSGhiIhIOzKZDOPH1wEAWFkZYfduTyxc2BgKxXdd6JGIKNtL82+5ihU/XadbJpOhYcOGMDD4b1WlUoknT56gcePGmRKSiIi+rWnT4li6tAmaNCnK028RUa6R5mb281kMAgMD4e7uDjMzM/V9crkcLi4uaN26dYYHJCKipC5ceAFf31uYN6+Rxrdl/fv/IGEqIqKsl+ZmduLEiQAAFxcXeHp6wsjIKNNCERFR8lQqgXnzzmHMmONITFShePE86NOnitSxiIgko/WcWS8vLzayREQSCA2NRrNm2zBixFEkJn46TaKf3x0IISRORkQknTSNzNrY2OD+/fvImzcvrK2tkz0A7LP379+neB8REaXPmTPP0L79Trx4Ea6ujR5dC1Om1E/1dzIRUU6XpmZ2wYIFMDc3V/8/f3ESEWUNlUpg9uwzGD/+BJTKTyOwtrYm2LSpJdzdi0icjohIemlqZr28vNT/37Vr18zKQkREX3j7NgqdO+/G4cOP1LW6dQti69bWcHQ0lzAZEVH2ofWc2atXr+LGjRvq23v37kWLFi0wZswYxMfHZ2g4IqLcbMyYY+pGViYDJkyog6NHu7CRJSL6gtbNbJ8+fXD//n0AwOPHj+Hp6QkTExPs2LEDI0aMyPCARES51R9//IgCBSxhb2+KI0c6Y/Lk+jAw0PrXNhFRjqb1pWHu37+PChUqAAB27NiBunXrYuvWrTh79izatWuHhQsXZnBEIqLcQaUS0NP775gEGxtj+Pu3g729GRwczFJZk4go90rX5WxVqk+nhDl69CiaNGkCAHB2dkZoaGjGpiMiyiWOHn2MihVXIjg4UqNevrwDG1kiolRo3cxWqVIF06ZNw6ZNm/D333/j559/BgA8efIE9vb2GR6QiCgnS0xUYfz442jUaBP+/fcNOnbcBaVSJXUsIiKdofU0g4ULF6Jjx47Ys2cPxo4diyJFPp0axs/PDzVq1MjwgEREOdXLl+Ho0GEXTp16qq7J5fqIikqAhYVCwmRERLpD62a2XLlyGmcz+GzOnDnQ19fPkFBERDldQMBDdO68G6Gh0QAAfX0Zpk9vgOHDa2rMmyUiotRp3cx+duXKFdy5cwcAUKpUKVSqVCnDQhER5VQJCUqMH38Cs2efVdfy57fA9u2tUbNmAQmTERHpJq2b2bdv38LT0xN///03rKysAAAfP35E/fr1sX37dtja2mZ0RiKiHOH58zC0a7cT5849V9d++aUYvL2bI08eEwmTERHpLq0PABs0aBAiIyNx69YtvH//Hu/fv8fNmzcRHh6OX3/9NTMyEhHlCOfOPVc3sgYGepg3rxH8/duxkSUi+g5aj8wGBATg6NGjKFmypLpWqlQpLF26FI0aNcrQcEREOYmnZxkcO/YEhw8/go9PG1Srll/qSEREOk/rZlalUsHQ0DBJ3dDQUH3+WSIiAt69i04y6rpoUWPExibC2tpYolRERDmL1tMMGjRogMGDB+PVq1fq2suXLzFkyBA0bNgwQ8MREemqXbvuwNV1MbZt0zz7i7GxIRtZIqIMpHUzu2TJEoSHh8PFxQWurq5wdXVFoUKFEB4ejj///DMzMhIR6Yy4uEQMGnQArVv7IiwsDr1778ODB++kjkVElGNpPc3A2dkZV69exbFjx9Sn5ipZsiTc3NwyPBwRkS559Og9PD39cOXKa3WtSZOisLMzlTAVEVHOplUz6+PjA39/f8THx6Nhw4YYNGhQZuUiItIpvr630LOnPyIi4gEACoU+Fi5sjD59KkMm40UQiIgyS5qb2eXLl2PAgAEoWrQojI2NsWvXLjx69Ahz5szJzHxERNlabGwihgwJwIoVV9S1okVt4OvbFhUqOEiYjIgod0jznNklS5Zg4sSJuHfvHgIDA7FhwwYsW7YsM7MREWVrjx9/wP/+t0ajke3QoSyuXOnNRpaIKIukuZl9/PgxvLy81Lc7dOiAxMREvH79OpW1iIhyLhMTQ7x+HQkAMDIywJo1TbF5c0uYmyskTkZElHukuZmNi4uDqel/BzHo6elBLpcjJiYmU4IREWV3Dg5m2LKlFUqXtsXly73Qo0clzo8lIspiWh0ANn78eJiY/HcC8Pj4eEyfPh2Wlpbq2vz58zMuHRFRNnLnTgjs7c1gY/PfeWLd3AojMLAvDAy0PtMhERFlgDQ3s3Xq1MG9e/c0ajVq1MDjx4/VtzkiQUQ5lbd3IAYMOAA3t8LYs8dT4/cdG1kiIumkuZk9efJkJsYgIsqeIiPjMWDAAWzceB0A4O9/D97egejWraLEyYiICEjHRROIiHKLGzfewMPDD3fvhqprPXtWhKdnGQlTERHRl9jMEhF9RQiBtWuvYdCgg4iNTQQAmJnJsXLlL+jQoazE6YiI6EtsZomIvhAREYe+ffdj69Yb6lr58vbw9W2LYsXySJiMiIiSw2aWiOj/vXsXjerV1+LBg/fqWv/+VTBvnjuMjPjrkogoO+IhuERE/8/GxhiVKuUDAFhYKODr2wZLl/7MRpaIKBtLVzN7+vRpdOrUCdWrV8fLly8BAJs2bcKZM2cyNBwRUVaSyWRYtaopPDxK4+rV3mjbtrTUkYiI6Bu0bmZ37twJd3d3GBsb49q1a4iLiwMAhIWFYcaMGRkekIgos/zzzyscPvxIo2ZhoYCPTxu4utpIlIqIiLShdTM7bdo0rFixAqtXr4ahoaG6XrNmTVy9ejVDwxERZQYhBBYtuoAaNdaiXTs/PHsWJnUkIiJKJ62b2Xv37qFOnTpJ6paWlvj48WNGZCIiyjTv38egZUsf/PbbISQkqPDhQyxmz+YUKSIiXaV1M+vg4ICHDx8mqZ85cwaFCxdOV4ilS5fCxcUFRkZGqFatGi5dupSm9bZv3w6ZTIYWLVqk63GJKHe5cOEFKlZcib17/7s09++/V8eCBY0lTEVERN9D62a2V69eGDx4MC5evAiZTIZXr15hy5YtGDZsGPr166d1AB8fHwwdOhQTJ07E1atXUb58ebi7u+Pt27eprhcUFIRhw4ahdu3aWj8mEeUuKpXA3LnnULv2evWUAhsbY/z1V3vMndsIcrm+xAmJiCi9tD7fzKhRo6BSqdCwYUNER0ejTp06UCgUGDZsGAYNGqR1gPnz56NXr17o1q0bAGDFihXYv38/1q1bh1GjRiW7jlKpRMeOHTF58mScPn2a0xuIKEWhodHo2nUP9u9/oK7VrOmMbdtaw9nZUsJkRESUEbQemZXJZBg7dizev3+Pmzdv4sKFCwgJCcHUqVO1fvD4+HhcuXIFbm5u/wXS04ObmxvOnz+f4npTpkyBnZ0devTo8c3HiIuLQ3h4uMYPEeUOKpVAgwYbNBrZ0aNr4cQJLzayREQ5RLrPBC6Xy1GqVKnvevDQ0FAolUrY29tr1O3t7XH37t1k1zlz5gzWrl2LwMDAND3GzJkzMXny5O/KSUS6SU9PhilT6qNlSx/kzWuCzZtbwt29iNSxiIgoA2ndzNavXx8ymSzF+48fP/5dgVITERGBzp07Y/Xq1cibN2+a1hk9ejSGDh2qvh0eHg5nZ+fMikhE2UyLFiWwbFkTNG9eAo6O5lLHISKiDKZ1M1uhQgWN2wkJCQgMDMTNmzfh5eWl1bby5s0LfX19vHnzRqP+5s0bODg4JFn+0aNHCAoKQtOmTdU1lUoFADAwMMC9e/fg6uqqsY5CoYBCodAqFxHppr//DsLevfcwb14jjT+6+/X7QcJURESUmbRuZhcsWJBsfdKkSYiMjNRqW3K5HJUrV8axY8fUp9dSqVQ4duwYBg4cmGT5EiVK4MaNGxq1cePGISIiAosWLeKIK1EupVSqMH36aUye/DdUKoHSpW3Ro0clqWMREVEWSPec2a916tQJVatWxdy5c7Vab+jQofDy8kKVKlVQtWpVLFy4EFFRUeqzG3Tp0gVOTk6YOXMmjIyMUKZMGY31raysACBJnYhyh+DgSHTsuAvHjz9R1/bsuYfu3SumOiWKiIhyhgxrZs+fPw8jIyOt1/P09ERISAgmTJiA4OBgVKhQAQEBAeqDwp49ewY9Pa1PukBEucDRo4/RqdMuvHkTBeDTAV+TJtXFmDG12cgSEeUSMiGE0GaFVq1aadwWQuD169f4559/MH78eEycODFDA2a08PBwWFpaIiwsDBYWFpn/gCvzA5EvATMnoM+LzH88olwgMVGFyZNPYvr00/j8GyxfPjNs29Yadeu6SJqNiIi+nzb9mtYjs5aWmudm1NPTQ/HixTFlyhQ0atRI280REWnl5ctwdOiwC6dOPVXX3N1dsXFjS9jZmUqYjIiIpKBVM6tUKtGtWzeULVsW1tbWmZWJiChFo0cfUzey+voyTJvWACNG1ISeHqcVEBHlRlpNRtXX10ejRo14+Vgiksz8+e5wcjJH/vwWOHmyK0aNqsVGlogoF9N6mkGZMmXw+PFjFCpUKDPyEBFpUKmERrOaN68J9u/vgPz5LZAnj4mEyYiIKDvQ+jQB06ZNw7Bhw7Bv3z68fv0a4eHhGj9ERBll3777KF9+Bd680TyHdfnyDmxkiYgIgBbN7JQpUxAVFYUmTZrg+vXraNasGfLnzw9ra2tYW1vDysqK82iJKEPExyvx+++H0LTpNty8+RadO++GSqXViVeIiCiXSPM0g8mTJ6Nv3744ceJEZuYholwuKOgjPD39cOnSS3XN1FSOmJgEmJrKJUxGRETZUZqb2c+no61bt26mhSGi3G337jvo3t0fHz/GAgAMDfUwd24jDBpUlRdBICKiZGl1ABj/MSGizBAXl4jhw4/gzz8vqWuFC1vDx6cNqlRxlDAZERFld1o1s8WKFftmQ/v+/fvvCkREucujR+/h6emHK1deq2tt25bC6tVNYWmp/SWyiYgod9GqmZ08eXKSK4AREX2PCxdeqBtZhUIfCxa4o2/fKvwmiIiI0kSrZrZdu3aws7PLrCxElAt17FgOx449wZkzz+Dr2xYVKjhIHYmIiHRImptZjpIQUUZ4+zYKdnamGrUlS5pAqVTB3FwhUSoiItJVaT7P7OezGRARpdfWrTfg6roYvr63NOomJoZsZImIKF3S3MyqVCpOMSCidImOTkCvXv7o2HEXIiPj0bOnPx494sGiRET0/bSaM0tEpK07d0Lg4eGHmzffqmutWpWEg4OZhKmIiCinYDNLRJlmw4ZA9O9/ANHRCQA+TSdYtqwJvLwqSBuMiIhyDDazRJThoqLi0b//AWzceF1dK13aFr6+bVGqlK2EyYiIKKdhM0tEGerevVC0aOGDu3dD1bWePSti0aKfYGJiKGEyIiLKidjMElGGMjdX4N27aACAmZkcK1f+gg4dykqcioiIcqo0n82AiCgtHB3NsWlTS1Ss6IArV3qzkSUiokzFkVki+i7XrwejQAFLWFsbq2vu7kXg5lYY+vr8e5mIiDIX/6UhonQRQmD58suoVm0Nunf3T3JhFTayRESUFfivDRFpLSwsFp6efujf/wDi4pTYs+cutmy5IXUsIiLKhTjNgIi08s8/r+Dp6YfHjz+oa4MGVUXbtqUkTEVERLkVm1kiShMhBP788xKGDTuMhAQVAMDKygjr1jVDy5YlJU5HRES5FZtZIvqmDx9i0KOHP3bvvquuVa3qBB+fNnBxsZIuGBER5XpsZokoVW/eRKJatTV4+jRMXfv99+qYMaMh5HJ9CZMRERGxmSWib7CzM8UPPzjh6dMw2NgYw9u7OZo2LS51LCIiIgBsZonoG2QyGdasaQpDQz3MmuWGAgUspY5ERESkxmaWiDScPfsM0dEJ+PFHV3XN0tIIW7e2ljAVERFR8nieWSICAKhUArNmnUHdut5o334nXrwIlzoSERHRN7GZJSKEhETh55+3YvToY1AqBd69i8H8+eeljkVERPRNnGZAlMv9/XcQOnTYhVevIgAAMhkwdmxtTJxYT9pgREREacBmliiXUipVmDHjNCZN+hsqlQAA2NubYvPmVnBzKyxxOiIiorRhM0uUCwUHR6JTp104duyJutagQSFs2dIKDg5mEiYjIiLSDptZolxGqVShfv0NuHs3FACgpyfDxIl1MXZsbejrcxo9ERHpFv7LRZTL6OvrYdq0+gCAfPnMcOxYF0yYUJeNLBER6SSOzBLlQq1bl8KKFT+jZcuSsLMzlToOERFRunEohiiHO3ToIYYOPZSk3qdPFTayRESk8zgyS5RDJSaqMH78ccyadRYAUL68Pby8KkgbioiIKINxZJYoB3r+PAz16nmrG1kAOHDgoYSJiIiIMgdHZolymP3776NLlz14/z4GAGBgoIdZsxpi6NDqEicjIiLKeGxmiXKIhAQlRo8+hnnz/rsMbcGClti+vQ3+97/8EiYjIiLKPGxmiXKAoKCPaNfODxcvvlTXWrQogXXrmsHa2ljCZERERJmLzSxRDjB69DF1I2toqIe5cxth0KCqkMlkEicjIiLKXGxmiXKAxYsb49SppzAyMoCPTxtUqeIodSQiIqIswWaWSAcplSqNK3bZ2pri4MGOKFjQEpaWRhImIyIiylo8NReRjtmx4xbKlVuBkJAojXq5cvZsZImIKNdhM0ukI2JjE9G//354ePjh9u0QdOmyByqVkDoWERGRpDjNgEgHPHjwDh4efggMDFbXrK2NEBeXCGNjQwmTERERSYvNLFE2t23bDfTuvQ+RkfEAACMjA/z550/o0aMiz1ZARES5HptZomwqJiYBgwcHYPXqq+paiRJ54evbBmXL2kuYjIiIKPtgM0uUDd29G4q2bXfg5s236pqXV3ksXdoEpqZyCZMRERFlL2xmibKhixdfqBtZExNDLFvWBF5eFaQNRURElA2xmSXKhry8KuD48SBcvfoaPj5tUKqUrdSRiIiIsiU2s0TZQHBwJBwczDRqy5Y1gUwmg4kJz1ZARESUEp5nlkhCQgisXXsVhQsvws6dtzXuMzWVs5ElIiL6BjazRBKJiIhD58670bPnX4iJSUSPHv4ICvoodSwiIiKdwmkGRBK4fj0YHh5+uH//nbrWvn2ZJFMNiIiIKHVsZomykBACK1dewW+/BSAuTgkAMDeXY82aZvDwKC1xOiIiIt3DZpYoi4SFxaJ3733w9b2lrlWqlA++vm3g6mojYTIiIiLdxWaWKAvcvPkWzZtvx+PHH9S1QYOqYs6cH6FQ8GNIRESUXvxXlCgLWFkZISwsVv3/69Y1Q8uWJSVORUREpPt4NgOiLJA/vwU2bmyJatWccO1aHzayREREGYQjs0SZ4J9/XqFoURtYWhqpa02aFEXjxkWgpyeTMBkREVHOwpFZogwkhMD8+edRvfpa9Oz5F4QQGvezkSUiIspYbGaJMsi7d9Fo1mw7fv/9MBITVfDzu40dO25/e0UiIiJKN04zIMoA5849R7t2fnj+PFxdGzmyJlq2LCFhKiIiopyPzSzRd1CpBObMOYuxY49Dqfw0pSBvXhNs2tQSjRsXkTgdERFRzsdmliidQkKi0KXLHgQEPFTX6tQpiK1bW8HJyULCZERERLkHm1midHjxIhzVqq3Bq1cRAACZDBg7tjYmTqwHAwNORSciIsoq/FeXKB2cnMxRrZoTAMDe3hSHD3fG1KkN2MgSERFlsWzxL+/SpUvh4uICIyMjVKtWDZcuXUpx2dWrV6N27dqwtraGtbU13NzcUl2eKDPIZDKsXdsMXbqUR2BgX7i5FZY6EhERUa4keTPr4+ODoUOHYuLEibh69SrKly8Pd3d3vH37NtnlT548ifbt2+PEiRM4f/48nJ2d0ahRI7x8+TKLk1Nucvz4Exw79lijZm1tjA0bWsDBwUyiVERERCQTX5/VPYtVq1YNP/zwA5YsWQIAUKlUcHZ2xqBBgzBq1Khvrq9UKmFtbY0lS5agS5cu31w+PDwclpaWCAsLg4VFFhykszI/EPkSMHMC+rzI/MejDKVUqjBlyt+YOvUU8uY1QWBgXzg6mksdi4iIKEfTpl+TdGQ2Pj4eV65cgZubm7qmp6cHNzc3nD9/Pk3biI6ORkJCAmxsbJK9Py4uDuHh4Ro/RGnx6lUE3Nw2YcqUUxACCAmJxpIlnNJCRESUnUjazIaGhkKpVMLe3l6jbm9vj+Dg4DRtY+TIkXB0dNRoiL80c+ZMWFpaqn+cnZ2/OzflfIcPP0KFCitw8mQQAEBfX4YZMxpg2rQG0gYjIiIiDZLPmf0es2bNwvbt27F7924YGRklu8zo0aMRFham/nn+/HkWpyRdkpiowpgxx+DuvhkhIdEAPp254OTJrhg9ujb09GQSJyQiIqIvSXqe2bx580JfXx9v3rzRqL958wYODg6prjt37lzMmjULR48eRbly5VJcTqFQQKFQZEheytlevAhH+/Y7cebMM3WtSZOi2LChBfLmNZEwGREREaVE0pFZuVyOypUr49ixY+qaSqXCsWPHUL169RTX++OPPzB16lQEBASgSpUqWRGVcriEBCXq1vVWN7IGBnqYM+dH/PVXezayRERE2Zjk0wyGDh2K1atXY8OGDbhz5w769euHqKgodOvWDQDQpUsXjB49Wr387NmzMX78eKxbtw4uLi4IDg5GcHAwIiMjpXoKlAMYGupj5syGAIACBSxx+nQ3DBtWg9MKiIiIsjnJL2fr6emJkJAQTJgwAcHBwahQoQICAgLUB4U9e/YMenr/9dzLly9HfHw82rRpo7GdiRMnYtKkSVkZnXIYD4/SCAuLRevWpWBjYyx1HCIiIkoDyc8zm9V4nlkCgL177+Lvv59i/nx3qaMQERHRV7Tp1yQfmSXKSvHxSowYcQSLFl0EAFSqlA+dOqV8ACERERFlb5LPmSXKKo8ff0DNmuvUjSwAHD36OJU1iIiIKLvjyCzlCn5+t9Gjhz/Cw+MAAHK5PhYscEe/fjwbBhERkS5jM0s5WmxsIn7//RCWLftHXStSxAa+vm1QsWI+CZMRERFRRmAzSznWgwfv4Onph2vX/rs0crt2ZbBy5S+wsOCFNIiIiHICNrOUY40adUzdyBoZGWDx4sbo2bMSZDKeO5aIiCinYDNLOdayZU1w7txzWFoq4OvbFuXK2UsdiYiIiDIYm1nKMRITVTAw+O8EHfb2Zjh0qBMKF7aGmZlcwmRERESUWXhqLsoRNm26jrJll+Pdu2iNerly9mxkiYiIcjA2s6TToqLi0b37XnTpsgd374bCy2sPVKpcdVE7IiKiXI3TDEhn3br1Fh4efrh9O0Rds7c3RUKCEgoF39pERES5Af/FJ50jhMD69YEYOPAAYmISAQCmpoZYseIXXpqWiIgol2EzSzolMjIeffvuw5YtN9S1cuXs4ePTBiVK5JUwGREREUmBzSzpjOvXg+Hh4Yf799+pa336VMaCBe4wNjaUMBkRERFJhc0s6Yx//nmlbmTNzeVYvbopPD3LSJyKiIiIpMRmlnRG9+4Vcfx4EO7eDYWPTxsUKWIjdSQiIiKSGJtZyrZevgyHk5OF+rZMJsOqVb/AwECPZysgIiIiADzPLGVDQggsWXIJrq6LsWfPXY37TE3lbGSJiIhIjc0sZSsfP8aibdsdGDToIOLilOjWbS+ePQuTOhYRERFlUxziomzj0qWX8PT0Q1DQR3WtW7cKcHAwky4UERERZWtsZklyQggsXHgBI0ceRUKCCgBgbW0Eb+8WaNasuMTpiIiIKDtjM0uSev8+Bt267YW//z11rXr1/Ni2rTUKFrSSLhgRERHpBDazJJlr116jefPteP48XF0bMaIGpk1rAENDfQmTERERka5gM0uSyZPHBJGR8f///8bYuLElmjQpKnEqIiIi0iU8mwFJpkABS2zY0AJ16hREYGBfNrJERESkNTazlGXOnXuO8PA4jVrTpsVx8qQX8ue3SGEtIiIiopSxmaVMp1IJTJ9+CrVrr0fv3n9BCKFxv0wmkygZERER6To2s5Sp3ryJROPGmzFu3AmoVAI+Prewd++9b69IRERElAY8AIwyzfHjT9Cx4y4EB0cCAGQyYOLEumjatJjEyYiIiCinYDNLGU6pVGHq1FOYMuVvfJ5R4OBghq1bW6F+/ULShiMiIqIchc0sZajXryPQseMunDgRpK79+GNhbN7cCnZ2ptIFIyIiohyJzSxlmKCgj6hWbQ3evo0CAOjpyTB1an2MGlULeno8yIuIiIgyHg8AowxTsKAl/ve//AAAJydznDzphTFjarORJSIiokzDZpYyjEwmw/r1zdGjR0UEBvZF7doFpY5EREREORynGVC6HTjwAEZGBmjQ4L+DumxsjLFmTTMJUxEREVFuwpFZ0lpCghIjRhzBzz9vRYcOO9Wn3iIiIiLKamxmSSvPnoWhbl1vzJlzDgDw5k0UVq26InEqIiIiyq04zYDSzN//Hrp23YMPH2IBAIaGevjjjx8xeHA1iZMRERFRbsVmlr4pPl6JkSOPYOHCi+qai4sVfH3b4IcfnCRMRkRERLkdm1lK1ZMnH+Dp6YfLl1+pa61alcTatc1gZWUkYTIiIiIiNrOUivh4JerU8caLF+EAALlcH/PnN0L//j9AJuO5Y4mIiEh6PACMUiSX6+OPP9wAAK6u1jh/vgcGDKjKRpaIiIiyDY7MUqraty+L6OgEtG1bGhYWCqnjEBEREWngyCyp+fjcxO+/H0pS79GjEhtZIiIiypY4MkuIiUnAb78FYNWqqwCAH35wQrt2ZSRORURERPRtHJnN5e7dC8X//rdW3cgCwKlTTyVMRERERJR2HJnNxTZv/hd9++5DVFQCAMDY2ABLlzZB164VpA1GRERElEZsZnOh6OgEDBp0AOvWBaprpUrZwte3DUqXtpMuGBEREZGW2MzmMrdvh6Bt2x24fTtEXevevQL+/LMJTEwMJUxGREREpD02s7nMqFFH1Y2sqakhli//GZ07l5c4FREREVH68ACwXGbVqqawszNF2bJ2+Oef3mxkiYiISKdxZDaHS0hQwtBQX33bwcEMR492RpEiNjA25rQCIiIi0m0cmc2hhBBYteoKypZdjvfvYzTuK1vWno0sERER5QhsZnOg8PA4dOiwC3367MO9e+/QrdteCCGkjkVERESU4TjNIIe5du01PDz88PDhe3XN2dkCiYkqjekGRERERDkBm9kcQgiBZcsuY+jQw4iPVwIALC0VWLu2GVq3LiVxOiIiIqLMwWY2B/j4MRY9e/pj58476toPPzhi+/Y2KFzYWsJkRERERJmLzayOu3z5JTw9/fDkyUd17bffqmH27B8hl3NaAREREeVsbGZ13NWrr9WNrLW1Eby9W6BZs+LShiIiIiLKImxmdVzv3pVx/HgQnj0Lw/btrVGwoJXUkYiIiIiyDJtZHfP8eRicnS3Vt2UyGdatawa5XJ9nKyAiIqJch+eZ1REqlcCcOWfh6roY+/bd17jP1FTORpaIiIhyJTazOiA0NBpNm27DiBFHkZCggpfXHrx8GS51LCIiIiLJcZpBNnf69FO0b78TL19GAABkMqBv38qwtzeTOBkRERGR9NjMZlMqlcCsWWcwYcIJKJWfLkVra2uCzZtboVEjV4nTEREREWUPbGazobdvo9Cp0y4cOfJYXatXzwVbt7ZCvnzmEiYjIsr5hBBITEyEUqmUOgpRjmZoaAh9/e8/5ofNbDZz8eILtGjhg+DgSACfphVMmFAX48fXgb4+pzgTEWWm+Ph4vH79GtHR0VJHIcrxZDIZ8ufPDzOz75s6yWY2m7G3N0NsbCIAwMHBDFu2tEKDBoUkTkVElPOpVCo8efIE+vr6cHR0hFwuh0wmkzoWUY4khEBISAhevHiBokWLftcILZvZbMbFxQrr1zfHsmWXsWlTSx7oRUSUReLj46FSqeDs7AwTExOp4xDleLa2tggKCkJCQsJ3NbP83lpiJ08GISIiTqPWokUJHDrUiY0sEZEE9PT4TyNRVsiobz74iZVIYqIK48YdR4MGG9Cv334IITTu51dbRERERN/GZlYCL1+Go0GDDZg+/TSEALZsuYGDBx9KHYuIiIhI57CZzWIHDz5AhQorcfr0MwCAvr4Ms2e7oXHjIhInIyIiyn3u3bsHBwcHRERESB0lx/nf//6HnTt3ZvrjZItmdunSpXBxcYGRkRGqVauGS5cupbr8jh07UKJECRgZGaFs2bI4cOBAFiVNvwSlDCNHHkGTJlsRGvrplC/OzhY4daobRoyoCT09TisgIqL06dq1K2QyGWQyGQwNDVGoUCGMGDECsbGxSZbdt28f6tatC3Nzc5iYmOCHH36At7d3stvduXMn6tWrB0tLS5iZmaFcuXKYMmUK3r9/n8nPKOuMHj0agwYNgrl5zj2Pu7Z9VkJCAqZMmQJXV1cYGRmhfPnyCAgI0FgmIiICv/32GwoWLAhjY2PUqFEDly9f1lhm3LhxGDVqFFQqVYY/Jw1CYtu3bxdyuVysW7dO3Lp1S/Tq1UtYWVmJN2/eJLv82bNnhb6+vvjjjz/E7du3xbhx44ShoaG4ceNGmh4vLCxMABBhYWEZ+TRStsJJPB1rKWoU7iuASeqfpk23itDQqKzJQERE3xQTEyNu374tYmJipI6iNS8vL9G4cWPx+vVr8ezZM7F7925hYWEhRowYobHc4sWLhZ6enhg9erS4deuWePDggZg7d65QKBTi999/11h2zJgxQl9fXwwbNkycPXtWPHnyRBw+fFi0atVKLFy4MMueW1xcXKZt++nTp8LQ0FC8ePHiu7aTmRm/l7Z9lhBCjBgxQjg6Oor9+/eLR48eiWXLlgkjIyNx9epV9TIeHh6iVKlS4u+//xYPHjwQEydOFBYWFhqvZWJiorC3txf79u1L9nFS+8xp069J3sxWrVpVDBgwQH1bqVQKR0dHMXPmzGSX9/DwED///LNGrVq1aqJPnz5perysbmYfTCkprI1HqptYQ8MpYv78c0KlUmXJ4xMRUdroejPbvHlzjVqrVq1ExYoV1befPXsmDA0NxdChQ5Osv3jxYgFAXLhwQQghxMWLFwWAFJvWDx8+pJjl+fPnol27dsLa2lqYmJiIypUrq7ebXM7BgweLunXrqm/XrVtXDBgwQAwePFjkyZNH1KtXT7Rv3154eHhorBcfHy/y5MkjNmzYIIT41D/MmDFDuLi4CCMjI1GuXDmxY8eOFHMKIcScOXNElSpVNGqhoaGiXbt2wtHRURgbG4syZcqIrVu3aiyTXEYhhLhx44Zo3LixMDU1FXZ2dqJTp04iJCREvd7BgwdFzZo1haWlpbCxsRE///yzePjwYaoZv5e2fZYQQuTLl08sWbJEo9aqVSvRsWNHIYQQ0dHRQl9fP0mTWqlSJTF27FiNWrdu3USnTp2SfZyMamYlPc9sfHw8rly5gtGjR6trenp6cHNzw/nz55Nd5/z58xg6dKhGzd3dHXv27El2+bi4OMTF/Xfqq/Dw8O8ProXCecNRveBzHLhbDC4uVvDxaYOqVZ2yNAMREX2HzVWAqOCsfUxTB6DTP+le/ebNmzh37hwKFiyorvn5+SEhIQHDhg1LsnyfPn0wZswYbNu2DdWqVcOWLVtgZmaG/v37J7t9KyurZOuRkZGoW7cunJyc4O/vDwcHB1y9elXrr5k3bNiAfv364ezZswCAhw8fom3btoiMjFRfLerQoUOIjo5Gy5YtAQAzZ87E5s2bsWLFChQtWhSnTp1Cp06dYGtri7p16yb7OKdPn0aVKlU0arGxsahcuTJGjhwJCwsL7N+/H507d4arqyuqVq2aYsaPHz+iQYMG6NmzJxYsWICYmBiMHDkSHh4eOH78OAAgKioKQ4cORbly5RAZGYkJEyagZcuWCAwMTPGUcDNmzMCMGTNSfb1u376NAgUKJKmnp88CPvVORkZGGjVjY2OcOXMGANSXe05tmc+qVq2KWbNmpZr/e0nazIaGhkKpVMLe3l6jbm9vj7t37ya7TnBwcLLLBwcn/4tm5syZmDx5csYETgc9PWBD+z0Yd7QpZvl7w8rK6NsrERFR9hEVDES+lDrFN+3btw9mZmZITExEXFwc9PT0sGTJEvX99+/fh6WlJfLly5dkXblcjsKFC+P+/fsAgAcPHqBw4cIwNDTUKsPWrVsREhKCy5cvw8bGBgBQpIj2BzgXLVoUf/zxh/q2q6srTE1NsXv3bnTu3Fn9WM2aNYO5uTni4uIwY8YMHD16FNWrVwcAFC5cGGfOnMHKlStTbGafPn2apJl1cnLSaPgHDRqEQ4cOwdfXV6OZ/TrjtGnTULFiRY3Gc926dXB2dsb9+/dRrFgxtG7dWuOx1q1bB1tbW9y+fRtlypRJNmPfvn3h4eGR6uvl6OiYbD09fRbwaZBw/vz5qFOnDlxdXXHs2DHs2rULSqUSAGBubo7q1atj6tSpKFmyJOzt7bFt2zacP38+yf52dHTE8+fPoVKpMu0czjn+CmCjR4/WGMkNDw+Hs7Nz1gUwdUBee2BF74cAG1kiIt1j6qATj1m/fn0sX74cUVFRWLBgAQwMDJI0T2klvjr3eVoFBgaiYsWK6kY2vSpXrqxx28DAAB4eHtiyZQs6d+6MqKgo7N27F9u3bwfwaeQ2OjoaP/74o8Z68fHxqFixYoqPExMTk2R0UalUYsaMGfD19cXLly8RHx+PuLi4JFeF+zrj9evXceLECfXI8ZcePXqEYsWK4cGDB5gwYQIuXryI0NBQ9Yj1s2fPUmxmbWxsvvv11NaiRYvQq1cvlChRAjKZDK6urujWrRvWrVunXmbTpk3o3r07nJycoK+vj0qVKqF9+/a4cuWKxraMjY2hUqkQFxcHY2PjTMkraTObN29e6Ovr482bNxr1N2/ewMEh+Q+yg4ODVssrFAooFIqMCZwe3/E1ERERZQM68nvc1NRUPSq2bt06lC9fHmvXrkWPHj0AAMWKFUNYWBhevXqVZCQvPj4ejx49Qv369dXLnjlzBgkJCVqNzn6rWdHT00vSKCckJCT7XL7WsWNH1K1bF2/fvsWRI0dgbGyMxo0bA/g0vQEA9u/fDycnzal8qfUAefPmxYcPHzRqc+bMwaJFi7Bw4UKULVsWpqam+O233xAfH59qxsjISDRt2hSzZ89O8jifR8ObNm2KggULYvXq1XB0dIRKpUKZMmWSbPtL3zPNID19FvDpMrN79uxBbGws3r17B0dHR4waNQqFCxdWL+Pq6oq///4bUVFRCA8PR758+eDp6amxDAC8f/8epqammdbIAhKfmksul6Ny5co4duyYuqZSqXDs2DH11wRfq169usbyAHDkyJEUlyciIspt9PT0MGbMGIwbNw4xMTEAgNatW8PQ0BDz5s1LsvyKFSsQFRWF9u3bAwA6dOiAyMhILFu2LNntf/z4Mdl6uXLlEBgYmOKpu2xtbfH69WuNWmBgYJqeU40aNeDs7AwfHx9s2bIFbdu2VTfapUqVgkKhwLNnz1CkSBGNn9S+ja1YsSJu376tUTt79iyaN2+OTp06oXz58hrTL1JTqVIl3Lp1Cy4uLkkymJqa4t27d7h37x7GjRuHhg0bomTJkkka6eT07dsXgYGBqf6kNM0gPX3Wl4yMjODk5ITExETs3LkTzZs3T7KMqakp8uXLhw8fPuDQoUNJlrl582aqo+MZ4puHiGWy7du3C4VCIby9vcXt27dF7969hZWVlQgODhZCCNG5c2cxatQo9fJnz54VBgYGYu7cueLOnTti4sSJ2fvUXEREpBNy2tkMEhIShJOTk5gzZ466tmDBAqGnpyfGjBkj7ty5Ix4+fCjmzZuX7Km5RowYIfT19cXw4cPFuXPnRFBQkDh69Kho06ZNimc5iIuLE8WKFRO1a9cWZ86cEY8ePRJ+fn7i3LlzQgghAgIChEwmExs2bBD3798XEyZMEBYWFknOZjB48OBktz927FhRqlQpYWBgIE6fPp3kvjx58ghvb2/x8OFDceXKFbF48WLh7e2d4uvm7+8v7OzsRGJioro2ZMgQ4ezsLM6ePStu374tevbsKSwsLDRe3+Qyvnz5Utja2oo2bdqIS5cuiYcPH4qAgADRtWtXkZiYKJRKpciTJ4/o1KmTePDggTh27Jj44YcfBACxe/fuFDN+r2/1WUIk7bUuXLggdu7cKR49eiROnTolGjRoIAoVKqRxFouAgABx8OBB8fjxY3H48GFRvnx5Ua1aNREfH6/x+HXr1hVTpkxJNluOOTWXEEL8+eefokCBAkIul4uqVauqT+EhxKcXwcvLS2N5X19fUaxYMSGXy0Xp0qXF/v370/xYbGaJiCg5Oa2ZFUKImTNnCltbWxEZGamu7d27V9SuXVuYmpoKIyMjUblyZbFu3bpkt+vj4yPq1KkjzM3NhampqShXrpyYMmVKqqfmCgoKEq1btxYWFhbCxMREVKlSRVy8eFF9/4QJE4S9vb2wtLQUQ4YMEQMHDkxzM3v79m0BQBQsWDDJKS5VKpVYuHChKF68uDA0NBS2trbC3d1d/P333ylmTUhIEI6OjiIgIEBde/funWjevLkwMzMTdnZ2Yty4caJLly7fbGaFEOL+/fuiZcuWwsrKShgbG4sSJUqI3377TZ31yJEjomTJkkKhUIhy5cqJkydPZnozK0Tqfdbn5/Nlr3Xy5El1zjx58ojOnTuLly9faqzj4+MjChcuLORyuXBwcBADBgwQHz9+1FjmxYsXwtDQUDx//jzZXBnVzMqESOcsbx0VHh4OS0tLhIWFwcLCQuo4RESUTcTGxuLJkycoVKhQkoOCKOdaunQp/P39cejQIamj5DgjR47Ehw8fsGrVqmTvT+0zp02/luPPZkBERESUkj59+uDjx4+IiIjI0Ze0lYKdnV2SawNkBjazRERElGsZGBhg7NixUsfIkX7//fcseRxJz2ZARERERPQ92MwSERERkc5iM0tERPSFXHZcNJFkMuqzxmaWiIgIUJ+APzo6WuIkRLnD5yuf6evrf9d2eAAYERERPv2DamVlhbdv3wIATExMIJPJJE5FlDOpVCqEhITAxMQEBgbf146ymSUiIvp/n69X/7mhJaLMo6enhwIFCnz3H41sZomIiP6fTCZDvnz5YGdnh4SEBKnjEOVocrkcenrfP+OVzSwREdFX9PX1v3seHxFlDR4ARkREREQ6i80sEREREeksNrNEREREpLNy3ZzZzyfoDQ8PlzgJERERESXnc5+Wlgsr5LpmNiIiAgDg7OwscRIiIiIiSk1ERAQsLS1TXUYmctl1+1QqFV69egVzc/MsORl2eHg4nJ2d8fz5c1hYWGT641HG4z7UfdyHuo/7ULdx/+m+rN6HQghERETA0dHxm6fvynUjs3p6esifP3+WP66FhQU/wDqO+1D3cR/qPu5D3cb9p/uych9+a0T2Mx4ARkREREQ6i80sEREREeksNrOZTKFQYOLEiVAoFFJHoXTiPtR93Ie6j/tQt3H/6b7svA9z3QFgRERERJRzcGSWiIiIiHQWm1kiIiIi0llsZomIiIhIZ7GZJSIiIiKdxWY2AyxduhQuLi4wMjJCtWrVcOnSpVSX37FjB0qUKAEjIyOULVsWBw4cyKKklBJt9uHq1atRu3ZtWFtbw9raGm5ubt/c55T5tP0cfrZ9+3bIZDK0aNEicwPSN2m7Dz9+/IgBAwYgX758UCgUKFasGH+fSkjb/bdw4UIUL14cxsbGcHZ2xpAhQxAbG5tFaelrp06dQtOmTeHo6AiZTIY9e/Z8c52TJ0+iUqVKUCgUKFKkCLy9vTM9Z7IEfZft27cLuVwu1q1bJ27duiV69eolrKysxJs3b5Jd/uzZs0JfX1/88ccf4vbt22LcuHHC0NBQ3LhxI4uT02fa7sMOHTqIpUuXimvXrok7d+6Irl27CktLS/HixYssTk6fabsPP3vy5IlwcnIStWvXFs2bN8+asJQsbfdhXFycqFKlimjSpIk4c+aMePLkiTh58qQIDAzM4uQkhPb7b8uWLUKhUIgtW7aIJ0+eiEOHDol8+fKJIUOGZHFy+uzAgQNi7NixYteuXQKA2L17d6rLP378WJiYmIihQ4eK27dviz///FPo6+uLgICArAn8BTaz36lq1apiwIAB6ttKpVI4OjqKmTNnJru8h4eH+PnnnzVq1apVE3369MnUnJQybffh1xITE4W5ubnYsGFDZkWkb0jPPkxMTBQ1atQQa9asEV5eXmxmJabtPly+fLkoXLiwiI+Pz6qIlApt99+AAQNEgwYNNGpDhw4VNWvWzNSclDZpaWZHjBghSpcurVHz9PQU7u7umZgseZxm8B3i4+Nx5coVuLm5qWt6enpwc3PD+fPnk13n/PnzGssDgLu7e4rLU+ZKzz78WnR0NBISEmBjY5NZMSkV6d2HU6ZMgZ2dHXr06JEVMSkV6dmH/v7+qF69OgYMGAB7e3uUKVMGM2bMgFKpzKrY9P/Ss/9q1KiBK1euqKciPH78GAcOHECTJk2yJDN9v+zUzxhk+SPmIKGhoVAqlbC3t9eo29vb4+7du8muExwcnOzywcHBmZaTUpaeffi1kSNHwtHRMcmHmrJGevbhmTNnsHbtWgQGBmZBQvqW9OzDx48f4/jx4+jYsSMOHDiAhw8fon///khISMDEiROzIjb9v/Tsvw4dOiA0NBS1atWCEAKJiYno27cvxowZkxWRKQOk1M+Eh4cjJiYGxsbGWZaFI7NE32HWrFnYvn07du/eDSMjI6njUBpERESgc+fOWL16NfLmzSt1HEonlUoFOzs7rFq1CpUrV4anpyfGjh2LFStWSB2N0uDkyZOYMWMGli1bhqtXr2LXrl3Yv38/pk6dKnU00kEcmf0OefPmhb6+Pt68eaNRf/PmDRwcHJJdx8HBQavlKXOlZx9+NnfuXMyaNQtHjx5FuXLlMjMmpULbffjo0SMEBQWhadOm6ppKpQIAGBgY4N69e3B1dc3c0KQhPZ/DfPnywdDQEPr6+upayZIlERwcjPj4eMjl8kzNTP9Jz/4bP348OnfujJ49ewIAypYti6ioKPTu3Rtjx46Fnh7H2rK7lPoZCwuLLB2VBTgy+13kcjkqV66MY8eOqWsqlQrHjh1D9erVk12nevXqGssDwJEjR1JcnjJXevYhAPzxxx+YOnUqAgICUKVKlayISinQdh+WKFECN27cQGBgoPqnWbNmqF+/PgIDA+Hs7JyV8Qnp+xzWrFkTDx8+VP8hAgD3799Hvnz52MhmsfTsv+jo6CQN6+c/TIQQmReWMky26mey/JCzHGb79u1CoVAIb29vcfv2bdG7d29hZWUlgoODhRBCdO7cWYwaNUq9/NmzZ4WBgYGYO3euuHPnjpg4cSJPzSUxbffhrFmzhFwuF35+fuL169fqn4iICKmeQq6n7T78Gs9mID1t9+GzZ8+Eubm5GDhwoLh3757Yt2+fsLOzE9OmTZPqKeRq2u6/iRMnCnNzc7Ft2zbx+PFjcfjwYeHq6io8PDykegq5XkREhLh27Zq4du2aACDmz58vrl27Jp4+fSqEEGLUqFGic+fO6uU/n5pr+PDh4s6dO2Lp0qU8NZcu+/PPP0WBAgWEXC4XVatWFRcuXFDfV7duXeHl5aWxvK+vryhWrJiQy+WidOnSYv/+/VmcmL6mzT4sWLCgAJDkZ+LEiVkfnNS0/Rx+ic1s9qDtPjx37pyoVq2aUCgUonDhwmL69OkiMTExi1PTZ9rsv4SEBDFp0iTh6uoqjIyMhLOzs+jfv7/48OFD1gcnIYQQJ06cSPbfts/7zcvLS9StWzfJOhUqVBByuVwULlxYrF+/PstzCyGETAiO5xMRERGRbuKcWSIiIiLSWWxmiYiIiEhnsZklIiIiIp3FZpaIiIiIdBabWSIiIiLSWWxmiYiIiEhnsZklIiIiIp3FZpaIiIiIdBabWSIiAN7e3rCyspI6RrrJZDLs2bMn1WW6du2KFi1aZEkeIqKswmaWiHKMrl27QiaTJfl5+PCh1NHg7e2tzqOnp4f8+fOjW7duePv2bYZs//Xr1/jpp58AAEFBQZDJZAgMDNRYZtGiRfD29s6Qx0vJpEmT1M9TX18fzs7O6N27N96/f6/Vdth4E1FaGUgdgIgoIzVu3Bjr16/XqNna2kqURpOFhQXu3bsHlUqF69evo1u3bnj16hUOHTr03dt2cHD45jKWlpbf/ThpUbp0aRw9ehRKpRJ37txB9+7dERYWBh8fnyx5fCLKXTgyS0Q5ikKhgIODg8aPvr4+5s+fj7Jly8LU1BTOzs7o378/IiMjU9zO9evXUb9+fZibm8PCwgKVK1fGP//8o77/zJkzqF27NoyNjeHs7Ixff/0VUVFRqWaTyWRwcHCAo6MjfvrpJ/z66684evQoYmJioFKpMGXKFOTPnx8KhQIVKlRAQECAet34+HgMHDgQ+fLlg5GREQoWLIiZM2dqbPvzNINChQoBACpWrAiZTIZ69eoB0BztXLVqFRwdHaFSqTQyNm/eHN27d1ff3rt3LypVqgQjIyMULlwYkydPRmJiYqrP08DAAA4ODnBycoKbmxvatm2LI0eOqO9XKpXo0aMHChUqBGNjYxQvXhyLFi1S3z9p0iRs2LABe/fuVY/ynjx5EgDw/PlzeHh4wMrKCjY2NmjevDmCgoJSzUNEORubWSLKFfT09LB48WLcunULGzZswPHjxzFixIgUl+/YsSPy58+Py5cv48qVKxg1ahQMDQ0BAI8ePULjxo3RunVr/Pvvv/Dx8cGZM2cwcOBArTIZGxtDpVIhMTERixYtwrx58zB37lz8+++/cHd3R7NmzfDgwQMAwOLFi+Hv7w9fX1/cu3cPW7ZsgYuLS7LbvXTpEgDg6NGjeP36NXbt2pVkmbZt2+Ldu3c4ceKEuvb+/XsEBASgY8eOAIDTp0+jS5cuGDx4MG7fvo2VK1fC29sb06dPT/NzDAoKwqFDhyCXy9U1lUqF/PnzY8eOHbh9+zYmTJiAMWPGwNfXFwAwbNgweHh4oHHjxnj9+jVev36NGjVqICEhAe7u7jA3N8fp06dx9uxZmJmZoXHjxoiPj09zJiLKYQQRUQ7h5eUl9PX1hampqfqnTZs2yS67Y8cOkSdPHvXt9evXC0tLS/Vtc3Nz4e3tney6PXr0EL1799aonT59Wujp6YmYmJhk1/l6+/fv3xfFihUTVapUEUII4ejoKKZPn66xzg8//CD69+8vhBBi0KBBokGDBkKlUiW7fQBi9+7dQgghnjx5IgCIa9euaSzj5eUlmjdvrr7dvHlz0b17d/XtlStXCkdHR6FUKoUQQjRs2FDMmDFDYxubNm0S+fLlSzaDEEJMnDhR6OnpCVNTU2FkZCQACABi/vz5Ka4jhBADBgwQrVu3TjHr58cuXry4xmsQFxcnjI2NxaFDh1LdPhHlXJwzS0Q5Sv369bF8+XL1bVNTUwCfRilnzpyJu3fvIjw8HImJiYiNjUV0dDRMTEySbGfo0KHo2bMnNm3apP6q3NXVFcCnKQj//vsvtmzZol5eCAGVSoUnT56gZMmSyWYLCwuDmZkZVCoVYmNjUatWLaxZswbh4eF49eoVatasqbF8zZo1cf36dQCfpgj8+OOPKF68OBo3boxffvkFjRo1+q7XqmPHjujVqxeWLVsGhUKBLVu2oF27dtDT01M/z7Nnz2qMxCqVylRfNwAoXrw4/P39ERsbi82bNyMwMBCDBg3SWGbp0qVYt24dnj17hpiYGMTHx6NChQqp5r1+/ToePnwIc3NzjXpsbCwePXqUjleAiHICNrNElKOYmpqiSJEiGrWgoCD88ssv6NevH6ZPnw4bGxucOXMGPXr0QHx8fLJN2aRJk9ChQwfs378fBw8exMSJE7F9+3a0bNkSkZGR6NOnD3799dck6xUoUCDFbObm5rh69Sr09PSQL18+GBsbAwDCw8O/+bwqVaqEJ0+e4ODBgzh69Cg8PDzg5uYGPz+/b66bkqZNm0IIgf379+OHH37A6dOnsWDBAvX9kZGRmDx5Mlq1apVkXSMjoxS3K5fL1ftg1qxZ+PnnnzF58mRMnToVALB9+3YMGzYM8+bNQ/Xq1WFubo45c+bg4sWLqeaNjIxE5cqVNf6I+Cy7HORHRFmPzSwR5XhXrlyBSqXCvHnz1KOOn+dnpqZYsWIoVqwYhgwZgvbt22P9+vVo2bIlKlWqhNu3bydpmr9FT08v2XUsLCzg6OiIs2fPom7duur62bNnUbVqVY3lPD094enpiTZt2qBx48Z4//49bGxsNLb3eX6qUqlMNY+RkRFatWqFLVu24OHDhyhevDgqVaqkvr9SpUq4d++e1s/za+PGjUODBg3Qr18/9fOsUaMG+vfvr17m65FVuVyeJH+lSpXg4+MDOzs7WFhYfFcmIso5eAAYEeV4RYoUQUJCAv788088fvwYmzZtwooVK1JcPiYmBgMHDsTJkyfx9OlTnD17FpcvX1ZPHxg5ciTOnTuHgQMHIjAwEA8ePMDevXu1PgDsS8OHD8fs2bPh4+ODe/fuYdSoUQgMDMTgwYMBAPPnz8e2bdtw9+5d3L9/Hzt27ICDg0OyF3qws7ODsbExAgIC8ObNG4SFhaX4uB07dsT+/fuxbt069YFfn02YMAEbN27E5MmTcevWLdy5cwfbt2/HuHHjtHpu1atXR7ly5TBjxgwAQNGiRfHPP//g0KFDuH//PsaPH4/Lly9rrPN/7dy/y6lhHMfxz4nJKBYDFqT8SBGLxSKTMlCURbIp7MrIQNmU1T8gSSY/8g/YWChGdmXwDKdOx6kzPM9yup/zfq3XcH3v7d23q9vtdmu/3+twOOh2u+n5fKpUKslmsymbzWq73ep0Omm1Wqler+t6vX5qJgDfBzEL4NsLh8Pq9/vqdrsKBAKaTCZvv7X6k8lk0v1+V7lcltfrVT6fVyaTUafTkSSFQiGt12sdj0clk0lFIhG12205HI4vz1iv19VsNtVqtRQMBrVYLDSdTuXxeCT9fKLQ6/UUjUYVi8V0Pp81n89/bZp/ZzabNRwONRqN5HA4lM1m/3pvKpWS1WrV4XBQsVh8O0un05rNZloul4rFYkokEhoMBnK5XJ/+vkajofF4rMvlolqtplwup0KhoHg8rvv9/rallaRqtSqfz6doNCq73a7dbieLxaLNZiOn06lcLie/369KpaLH48GmFviP/Xi9Xq9/PQQAAADwFWxmAQAAYFjELAAAAAyLmAUAAIBhEbMAAAAwLGIWAAAAhkXMAgAAwLCIWQAAABgWMQsAAADDImYBAABgWMQsAAAADIuYBQAAgGF9AAA36gPp415HAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = LinearSVC(penalty='l1', dual=\"auto\", random_state=0, max_iter=100000, C=0.2, loss = 'squared_hinge')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_label_normed, y_label, stratify=y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_score = clf.decision_function(X_test)\n",
    "\n",
    "plot_roc_curve(y_test, y_score)\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. Unsupervised Learning\n",
    "\n",
    "Run k-means algorithm on the whole training set. Ignore the labels of the data, and assume k = 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Run the k-means algorithm multiple times. Make sure that you initialize the algoritm randomly. How do you make sure that the algorithm was not trapped in a local minimum?\n",
    "\n",
    "Ref: https://www.linkedin.com/advice/1/how-can-you-overcome-limitations-k-means-clustering#:~:text=To%20overcome%20local%20minima%20and,simulated%20annealing%20or%20genetic%20algorithms;\n",
    "https://www.chegg.com/homework-help/questions-and-answers/question-3-1-point-saved-k-means-right-minus-wrong-correct-concerned-k-means-would-get-stu-q47049076\n",
    "\n",
    "1.  Try to use multiple random initializations\n",
    "2.  Running the algorithm multiple times with different initial centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n",
      "inertia: 11575.082807048526\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    n_init = i*10+1\n",
    "    kmeans = KMeans(n_clusters=2, n_init=n_init).fit(X_normed)\n",
    "    print(f'inertia: {kmeans.inertia_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Compute the centers of the two clusters and find the closest 30 data\n",
    "points to each center. Read the true labels of those 30 data points and\n",
    "take a majority poll within them. The majority poll becomes the label\n",
    "predicted by k-means for the members of each cluster. Then compare the\n",
    "labels provided by k-means with the true labels of the training data and\n",
    "report the average accuracy, precision, recall, F1-score, and AUC over M\n",
    "runs, and ROC and the confusion matrix for one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kmeans(X, y, n_clusters=2, n_runs=30):\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "    y=np.array(y)\n",
    "\n",
    "    for _ in range(n_runs):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "        closest_points_indices = []\n",
    "        for center in cluster_centers:\n",
    "            distances = np.linalg.norm(X - center, axis=1)\n",
    "            closest_points_indices.extend(np.argsort(distances)[:30])\n",
    "\n",
    "        y_closest = y[closest_points_indices]\n",
    "\n",
    "        cluster1_label = np.bincount(y_closest[:30]).argmax()\n",
    "        cluster2_label = np.bincount(y_closest[30:]).argmax()\n",
    "\n",
    "        y_pred = np.array([cluster1_label if label == 0 else cluster2_label for label in kmeans.labels_])\n",
    "\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        precision = precision_score(y, y_pred)\n",
    "        recall = recall_score(y, y_pred)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "        auc = roc_auc_score(y, kmeans.labels_)\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"Average Precision:\", avg_precision)\n",
    "    print(\"Average Recall:\", avg_recall)\n",
    "    print(\"Average F1-Score:\", avg_f1)\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    y_pred_one_run = np.array([cluster1_label if label == 0 else cluster2_label for label in kmeans.labels_])\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y, kmeans.labels_)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y, kmeans.labels_):.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    cm = confusion_matrix(y, y_pred_one_run)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.9089630931458699\n",
      "Average Precision: 0.9209367443044421\n",
      "Average Recall: 0.8267295597484277\n",
      "Average F1-Score: 0.8712651867867276\n",
      "Average AUC: 0.4477991825661081\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 18 339]\n",
      " [176  36]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_kmeans(X_normed, y, n_clusters=2, n_runs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Classify test data based on their proximity to the centers of the clusters.\n",
    "Report the average accuracy, precision, recall, F1-score, and AUC over\n",
    "M runs, and ROC and the confusion matrix for one of the runs for the\n",
    "test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_data(X, y):\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "\n",
    "    for i in range(30):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "        \n",
    "        X_train_normed = (X_train - X_train.mean())/X_train.std()\n",
    "        X_train_normed = X_train_normed.reset_index(drop=True)\n",
    "        X_test_normed = (X_test - X_test.mean())/X_test.std()\n",
    "        X_test_normed = X_test_normed.reset_index(drop=True)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "        kmeans.fit(X_train_normed)\n",
    "\n",
    "        distances = kmeans.transform(X_test_normed)\n",
    "        probabilities = softmax(-distances, axis=1)\n",
    "        y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"Average Precision:\", avg_precision)\n",
    "    print(\"Average Recall:\", avg_recall)\n",
    "    print(\"Average F1-Score:\", avg_f1)\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=0)\n",
    "        \n",
    "    X_train_normed = (X_train - X_train.mean())/X_train.std()\n",
    "    X_train_normed = X_train_normed.reset_index(drop=True)\n",
    "    X_test_normed = (X_test - X_test.mean())/X_test.std()\n",
    "    X_test_normed = X_test_normed.reset_index(drop=True)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10, random_state=0)\n",
    "    kmeans.fit(X_train_normed)\n",
    "    distances_one_run = kmeans.transform(X_test_normed)\n",
    "    probabilities = softmax(-distances_one_run, axis=1)\n",
    "\n",
    "    y_pred = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, probabilities[:, 1])\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y_test, probabilities[:, 1]):.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Test Data')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.6330409356725146\n",
      "Average Precision: 0.6420227554451294\n",
      "Average Recall: 0.6071428571428571\n",
      "Average F1-Score: 0.6187927478642449\n",
      "Average AUC: 0.6276455026455027\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[65  7]\n",
      " [ 7 35]]\n"
     ]
    }
   ],
   "source": [
    "classify_test_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Spectral Clustering\n",
    "\n",
    "Repeat 1(b)iii using spectral clustering, which is clustering based on kernels.3 Research what spectral clustering is. Use RBF\n",
    "kernel with gamma=1 or find a gamma for which the two clutsres have the\n",
    "same balance as the one in original data set (if the positive class has p and the\n",
    "negative class has n samples, the two clusters must have p and n members).\n",
    "Do not label data based on their proximity to cluster center, because spectral\n",
    "clustering may give you non-convex clusters . Instead, use fit − predict\n",
    "method.\n",
    "\n",
    "Ref: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Spectralspectral(X, y):\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    auc_scores = []\n",
    "    y=np.array(y)\n",
    "    \n",
    "\n",
    "    for _ in range(30):\n",
    "        spectral = SpectralClustering(n_clusters=2, assign_labels='discretize', affinity='rbf', gamma=1.0)\n",
    "        labels = spectral.fit_predict(X)\n",
    "\n",
    "        cluster_labels = []\n",
    "        for cluster_idx in range(2):\n",
    "            cluster_mask = labels == cluster_idx\n",
    "            true_labels_in_cluster = y[cluster_mask]\n",
    "\n",
    "            majority_label = np.bincount(true_labels_in_cluster).argmax()\n",
    "            cluster_labels.append(majority_label)\n",
    "\n",
    "        y_pred = np.array([cluster_labels[0] if label == 0 else cluster_labels[1] for label in spectral.labels_])\n",
    "\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        precision = precision_score(y, y_pred, zero_division=0)\n",
    "        recall = recall_score(y, y_pred)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "        auc = roc_auc_score(y, spectral.labels_)\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"Average Precision:\", avg_precision)\n",
    "    print(\"Average Recall:\", avg_recall)\n",
    "    print(\"Average F1-Score:\", avg_f1)\n",
    "    print(\"Average AUC:\", avg_auc)\n",
    "\n",
    "\n",
    "    #For one run\n",
    "\n",
    "    spectral = SpectralClustering(n_clusters=2, assign_labels='discretize', affinity='rbf', gamma=1.0, random_state=100)\n",
    "    labels = spectral.fit_predict(X)\n",
    "\n",
    "    decision_values = spectral.affinity_matrix_\n",
    "    probabilities = np.exp(decision_values) / np.sum(np.exp(decision_values), axis=1, keepdims=True)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y, probabilities[:, 1])\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y, labels):.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion Matrix for one run\n",
    "    cm = confusion_matrix(y, labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.6298769771528999\n",
      "Average Precision: 0.6611111111111112\n",
      "Average Recall: 0.00880503144654088\n",
      "Average F1-Score: 0.017358800342444435\n",
      "Average AUC: 0.501394834663425\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[356   1]\n",
      " [210   2]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_Spectralspectral(X_normed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. One can expect that supervised learning on the full data set works better than\n",
    "semi-supervised learning with half of the data set labeled.One can expect that\n",
    "unsupervised learning underperforms in such situations. Compare the results\n",
    "you obtained by those methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, supervise learning and semi-supervise learning both gave good performance. Unsupervise model's result is not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Active Learning Using Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
    "banknote=pd.read_csv('../hw8-data/data_banknote_authentication.csv', names=columns_name)\n",
    "test_data = banknote.sample(n=472, random_state=42)\n",
    "train_data = banknote.drop(test_data.index)\n",
    "\n",
    "test_data_X = test_data.iloc[:,:-1]\n",
    "test_data_y = test_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Repeat each of the following two procedures 50 times. You will have 50 errors for 90 SVMs per each procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Select the penalty parameter using 5-fold cross validation.4 Repeat this process by adding 10 other randomly selected data points to the pool, until you use all the 900 points. \n",
    "\n",
    "Do NOT replace the samples back into the training set at each step. Calculate the test error for each SVM. You will have 90 SVMs that were trained using 10, 20, 30, ... , 900 data points and their 90 test errors. You have implemented passive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675     0\n",
       "1205    1\n",
       "1050    1\n",
       "27      0\n",
       "744     0\n",
       "393     0\n",
       "57      0\n",
       "983     1\n",
       "926     1\n",
       "702     0\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_10_train = train_data.sample(n=10, random_state=100)\n",
    "train_data = train_data.drop(select_10_train.index)\n",
    "\n",
    "select_10_train_X = select_10_train.iloc[:,:-1]\n",
    "select_10_train_y = select_10_train.iloc[:,-1]\n",
    "select_10_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_base_l1 = {'penalty':[\"l1\"],\n",
    "                   'loss': ['squared_hinge'],\n",
    "                   'dual':['auto'],\n",
    "                   'max_iter':[10000],\n",
    "                   'C':[10**n for n in range(-3,4)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  param_C  mean_test_score\n",
      "0   0.001              0.6\n",
      "1    0.01              0.6\n",
      "2     0.1              0.7\n",
      "3       1              0.7\n",
      "4      10              0.7\n",
      "5     100              0.7\n",
      "6    1000              0.7\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(penalty='l1', dual=\"auto\", random_state=0)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=clf, \n",
    "                            param_grid=parameters_base_l1, \n",
    "                            scoring = 'accuracy', \n",
    "                            cv=5)\n",
    "grid_search.fit(select_10_train_X, select_10_train_y)\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)[['param_C','mean_test_score']]\n",
    "results_range_df = results_df[results_df['mean_test_score']>0.5]\n",
    "print(results_range_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Iteration 0 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 20 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 30 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 40 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 50 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 60 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 70 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 80 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 90 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 100 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 110 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 120 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 130 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 140 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 170 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 180 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 190 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 200 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 210 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 270 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 280 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 290 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 300 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 310 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 320 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 360 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 370 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 410 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 420 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 1 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.11864406779661019\n",
      "Training Data: 20 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 30 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 40 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 50 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 60 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 70 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 80 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 90 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 100 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 110 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 120 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 130 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 140 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 150 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 160 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 170 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 180 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 190 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 270 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 280 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 290 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 300 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 310 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 320 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 350 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 360 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 370 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 380 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 410 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 420 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 430 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 440 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 450 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 460 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 470 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 480 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 490 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 500 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 880 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 890 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 900 ---> Missclassification: 0.004237288135593209\n",
      "----- Iteration 2 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.2055084745762712\n",
      "Training Data: 20 ---> Missclassification: 0.20127118644067798\n",
      "Training Data: 30 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 40 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 50 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 60 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 70 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 80 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 90 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 100 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 110 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 120 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 130 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 140 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 150 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 160 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 170 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 180 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 190 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 200 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 210 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 220 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 230 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 240 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 250 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 260 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 270 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 280 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 290 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 300 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 310 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 320 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 330 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 340 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 350 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 360 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 370 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 420 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 430 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 3 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 20 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 30 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 40 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 50 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 60 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 70 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 80 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 90 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 100 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 110 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 120 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 130 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 140 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 150 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 160 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 170 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 180 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 190 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 200 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 210 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 220 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 230 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 240 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 250 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 260 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 270 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 280 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 290 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 300 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 310 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 320 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 330 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 340 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 350 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 360 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 370 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 380 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 390 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 400 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 410 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 420 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 430 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 440 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 450 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 460 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 470 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 480 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 490 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 500 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 4 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 20 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 30 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 40 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 50 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 60 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 70 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 80 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 90 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 100 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 110 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 120 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 130 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 140 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 150 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 160 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 170 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 180 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 230 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 240 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 250 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 260 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 270 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 280 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 290 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 300 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 310 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 320 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 330 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 340 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 350 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 360 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 370 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 380 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 390 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 400 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 410 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 490 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 500 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 510 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 520 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 530 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 540 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 550 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 600 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 610 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 620 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 630 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 640 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 650 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 660 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 670 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 680 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 690 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 700 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 710 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 720 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 730 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 740 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 750 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 760 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 770 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 780 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 790 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 800 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 810 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 820 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 830 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 840 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 850 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 860 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 870 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 880 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 890 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 900 ---> Missclassification: 0.016949152542372836\n",
      "----- Iteration 5 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.0826271186440678\n",
      "Training Data: 20 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 30 ---> Missclassification: 0.0826271186440678\n",
      "Training Data: 40 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 50 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 60 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 70 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 80 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 90 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 100 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 110 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 120 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 130 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 140 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 150 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 160 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 170 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 180 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 190 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 200 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 210 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 220 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 230 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 310 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 320 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 330 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 340 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 350 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 360 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 370 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 410 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 420 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 6 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.10805084745762716\n",
      "Training Data: 20 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 30 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 40 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 50 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 60 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 70 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 80 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 90 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 100 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 110 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 120 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 130 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 140 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 150 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 160 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 170 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 180 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 190 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 200 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 210 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 220 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 230 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 240 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 250 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 260 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 270 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 280 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 290 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 300 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 310 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 320 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 330 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 340 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 350 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 360 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 370 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 7 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 20 ---> Missclassification: 0.14830508474576276\n",
      "Training Data: 30 ---> Missclassification: 0.11864406779661019\n",
      "Training Data: 40 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 50 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 60 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 70 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 80 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 90 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 100 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 110 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 120 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 130 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 140 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 150 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 160 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 170 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 180 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 190 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 200 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 210 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 220 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 230 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 240 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 250 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 260 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 270 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 280 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 290 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 300 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 310 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 320 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 330 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 340 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 350 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 360 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 370 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 380 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 390 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 400 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 410 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 420 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 430 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 470 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 480 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 490 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 500 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 8 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 20 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 30 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 40 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 50 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 60 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 70 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 80 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 90 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 100 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 110 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 120 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 130 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 140 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 150 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 160 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 170 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 180 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 190 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 200 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 210 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 220 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 230 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 240 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 250 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 260 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 270 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 280 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 290 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 300 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 310 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 320 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 330 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 340 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 350 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 360 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 370 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 380 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 390 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 400 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 410 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 420 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 430 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 460 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 580 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 590 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 600 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 610 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 620 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 630 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 640 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 650 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 660 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 670 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 680 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 690 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 700 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 710 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 720 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 730 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 740 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 750 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 760 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 770 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 780 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 790 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 840 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 850 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 860 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 890 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 900 ---> Missclassification: 0.014830508474576232\n",
      "----- Iteration 9 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 20 ---> Missclassification: 0.07627118644067798\n",
      "Training Data: 30 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 40 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 50 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 60 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 70 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 80 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 90 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 100 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 110 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 120 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 130 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 140 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 150 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 160 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 170 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 180 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 190 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 200 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 210 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 220 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 270 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 280 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 290 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 300 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 310 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 320 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 390 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 420 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 430 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 10 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.11864406779661019\n",
      "Training Data: 20 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 30 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 40 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 50 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 60 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 70 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 80 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 90 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 100 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 110 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 120 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 130 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 140 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 150 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 160 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 170 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 180 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 190 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 200 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 210 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 220 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 230 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 240 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 250 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 260 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 270 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 280 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 290 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 300 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 310 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 320 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 330 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 340 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 350 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 360 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 370 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 380 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 390 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 400 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 410 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 420 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 430 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 440 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 450 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 460 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 470 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 480 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 490 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 500 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 510 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 520 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 530 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 540 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 550 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 560 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 570 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 580 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 590 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 600 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 610 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 620 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 630 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 640 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 650 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 660 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 670 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 680 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 690 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 700 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 710 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 720 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 730 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 740 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 750 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 760 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 770 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 780 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 790 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 800 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 810 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 820 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 830 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 840 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 850 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 860 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 870 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 880 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 890 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 900 ---> Missclassification: 0.01906779661016944\n",
      "----- Iteration 11 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 20 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 30 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 40 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 50 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 60 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 70 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 80 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 90 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 100 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 110 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 120 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 130 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 140 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 150 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 160 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 170 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 180 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 260 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 270 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 280 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 290 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 300 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 310 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 320 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 330 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 340 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 350 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 360 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 370 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 380 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 12 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.2521186440677966\n",
      "Training Data: 20 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 30 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 40 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 50 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 60 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 70 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 80 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 90 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 100 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 110 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 120 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 130 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 140 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 150 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 160 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 170 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 180 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 190 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 200 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 210 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 220 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 230 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 240 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 250 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 260 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 270 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 280 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 290 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 300 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 310 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 320 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 330 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 340 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 350 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 360 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 390 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 400 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 410 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 420 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 430 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 460 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 840 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 850 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 860 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 890 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 900 ---> Missclassification: 0.014830508474576232\n",
      "----- Iteration 13 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 20 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 30 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 40 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 50 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 60 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 70 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 80 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 90 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 100 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 110 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 120 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 130 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 140 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 170 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 180 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 190 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 200 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 210 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 220 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 230 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 240 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 250 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 260 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 270 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 280 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 290 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 300 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 310 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 320 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 330 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 340 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 350 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 360 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 370 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 380 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 390 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 400 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 410 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 420 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 430 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 440 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 450 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 460 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 470 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 14 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 20 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 30 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 40 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 50 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 60 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 70 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 80 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 90 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 100 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 110 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 120 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 130 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 140 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 150 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 160 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 170 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 180 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 210 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 270 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 280 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 310 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 320 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 390 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 420 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 15 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 20 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 30 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 40 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 50 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 60 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 70 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 80 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 90 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 100 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 110 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 120 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 130 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 140 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 150 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 160 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 170 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 180 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 190 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 200 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 210 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 220 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 230 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 240 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 250 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 260 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 270 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 280 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 290 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 300 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 310 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 320 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 370 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 410 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 420 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 16 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 20 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 30 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 40 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 50 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 60 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 70 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 80 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 90 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 100 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 110 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 120 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 130 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 140 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 150 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 160 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 170 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 180 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 190 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 200 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 210 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 220 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 230 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 240 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 250 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 260 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 270 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 280 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 290 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 300 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 310 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 320 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 330 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 340 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 350 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 360 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 370 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 380 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 390 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 420 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 430 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 17 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.14830508474576276\n",
      "Training Data: 20 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 30 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 40 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 50 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 60 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 70 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 80 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 90 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 100 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 110 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 120 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 130 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 140 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 170 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 180 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 270 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 280 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 290 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 300 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 310 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 320 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 330 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 340 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 350 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 390 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 420 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 430 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 840 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 850 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 18 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 20 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 30 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 40 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 50 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 60 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 70 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 80 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 90 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 100 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 110 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 120 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 130 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 140 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 150 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 160 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 170 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 180 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 190 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 200 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 210 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 220 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 230 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 240 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 250 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 260 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 310 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 320 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 330 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 340 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 350 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 360 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 370 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 380 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 390 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 400 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 410 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 420 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 430 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 460 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 580 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 590 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 600 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 680 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 690 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 700 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 710 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 840 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 850 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 860 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 890 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 900 ---> Missclassification: 0.014830508474576232\n",
      "----- Iteration 19 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.13559322033898302\n",
      "Training Data: 20 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 30 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 40 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 50 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 60 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 70 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 80 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 90 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 100 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 110 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 120 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 130 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 140 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 150 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 160 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 170 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 180 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 190 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 200 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 210 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 220 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 310 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 320 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 390 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 420 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 20 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 20 ---> Missclassification: 0.10169491525423724\n",
      "Training Data: 30 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 40 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 50 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 60 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 70 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 80 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 90 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 100 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 110 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 120 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 130 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 140 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 150 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 160 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 170 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 180 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 190 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 200 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 210 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 270 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 280 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 290 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 300 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 310 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 320 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 600 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 21 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 20 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 30 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 40 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 50 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 60 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 70 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 80 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 90 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 100 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 110 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 120 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 130 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 140 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 150 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 160 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 170 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 180 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 190 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 200 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 210 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 220 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 230 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 240 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 250 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 260 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 310 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 320 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 330 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 340 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 350 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 360 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 370 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 380 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 410 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 420 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 430 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 440 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 22 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 20 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 30 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 40 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 50 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 60 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 70 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 80 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 90 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 100 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 110 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 120 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 130 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 140 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 150 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 160 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 170 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 180 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 190 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 200 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 210 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 220 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 230 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 240 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 250 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 260 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 270 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 280 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 290 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 300 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 310 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 320 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 330 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 340 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 350 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 360 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 370 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 380 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 390 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 400 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 410 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 420 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 430 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 440 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 450 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 460 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 470 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 480 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 490 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 520 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 530 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 23 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 20 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 30 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 40 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 50 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 60 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 70 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 80 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 90 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 100 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 110 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 120 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 130 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 140 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 150 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 160 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 170 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 180 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 190 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 200 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 210 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 220 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 230 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 240 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 250 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 260 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 270 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 310 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 320 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 330 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 340 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 350 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 390 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 420 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 430 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 24 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 20 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 30 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 40 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 50 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 60 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 70 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 80 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 90 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 100 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 110 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 120 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 130 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 140 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 150 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 160 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 170 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 180 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 190 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 200 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 210 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 220 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 230 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 240 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 250 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 260 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 270 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 280 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 290 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 300 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 310 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 320 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 330 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 340 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 350 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 360 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 370 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 380 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 410 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 420 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 430 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 25 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 20 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 30 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 40 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 50 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 60 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 70 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 80 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 90 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 100 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 110 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 120 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 130 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 140 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 170 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 180 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 240 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 250 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 260 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 270 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 280 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 310 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 320 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 350 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 360 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 370 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 410 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 420 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 840 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 850 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 860 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 890 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 900 ---> Missclassification: 0.014830508474576232\n",
      "----- Iteration 26 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 20 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 30 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 40 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 50 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 60 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 70 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 80 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 90 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 100 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 110 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 120 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 130 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 140 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 170 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 180 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 190 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 200 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 210 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 220 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 230 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 240 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 250 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 260 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 270 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 280 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 290 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 300 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 310 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 320 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 330 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 340 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 350 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 360 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 370 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 470 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 27 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 20 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 30 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 40 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 50 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 60 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 70 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 80 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 90 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 100 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 110 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 120 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 130 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 140 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 170 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 180 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 210 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 220 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 230 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 240 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 250 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 260 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 270 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 280 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 290 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 300 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 310 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 320 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 330 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 410 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 420 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 28 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 20 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 30 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 40 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 50 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 60 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 70 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 80 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 90 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 100 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 110 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 120 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 130 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 140 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 150 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 160 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 170 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 180 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 190 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 200 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 210 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 220 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 230 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 240 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 250 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 260 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 270 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 280 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 290 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 300 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 310 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 320 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 330 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 340 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 350 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 360 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 370 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 380 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 390 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 400 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 410 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 420 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 430 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 440 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 450 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 460 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 470 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 480 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 490 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 530 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 540 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 550 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 560 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 570 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 580 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 590 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 600 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 610 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 620 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 630 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 640 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 650 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 660 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 670 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 680 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 690 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 700 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 710 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 760 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 770 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 780 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 790 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 800 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 810 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 820 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 830 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 840 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 850 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 860 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 890 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 900 ---> Missclassification: 0.014830508474576232\n",
      "----- Iteration 29 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 20 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 30 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 40 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 50 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 60 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 70 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 80 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 90 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 100 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 110 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 120 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 130 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 140 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 170 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 180 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 240 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 250 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 260 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 310 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 320 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 330 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 340 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 350 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 360 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 370 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 380 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 30 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.19703389830508478\n",
      "Training Data: 20 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 30 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 40 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 50 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 60 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 70 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 80 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 90 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 100 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 110 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 120 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 130 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 140 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 150 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 160 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 170 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 180 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 190 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 200 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 210 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 220 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 230 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 240 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 250 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 260 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 270 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 280 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 290 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 300 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 310 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 320 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 330 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 340 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 350 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 360 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 370 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 380 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 390 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 400 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 410 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 420 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 430 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 440 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 450 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 460 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 470 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 480 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 490 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 520 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 530 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 540 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 550 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 580 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 590 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 600 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 610 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 620 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 630 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 640 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 650 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 660 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 830 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 840 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 850 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 860 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 870 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 31 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.08686440677966101\n",
      "Training Data: 20 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 30 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 40 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 50 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 60 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 70 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 80 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 90 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 100 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 110 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 120 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 130 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 140 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 150 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 160 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 170 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 180 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 190 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 200 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 210 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 220 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 230 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 240 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 250 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 260 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 310 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 320 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 330 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 340 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 350 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 360 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 370 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 380 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 390 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 400 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 410 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 420 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 430 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 510 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 520 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 830 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 840 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 850 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 860 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 870 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 880 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 890 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 900 ---> Missclassification: 0.021186440677966156\n",
      "----- Iteration 32 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.1313559322033898\n",
      "Training Data: 20 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 30 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 40 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 50 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 60 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 70 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 80 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 90 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 100 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 110 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 120 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 130 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 140 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 150 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 160 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 170 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 180 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 190 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 200 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 210 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 220 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 230 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 240 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 250 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 260 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 270 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 280 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 290 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 300 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 310 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 320 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 330 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 340 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 350 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 360 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 370 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 380 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 430 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 460 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 600 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 680 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 690 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 700 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 710 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 840 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 33 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.17796610169491522\n",
      "Training Data: 20 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 30 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 40 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 50 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 60 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 70 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 80 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 90 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 100 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 110 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 120 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 130 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 140 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 170 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 180 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 190 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 230 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 240 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 250 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 260 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 270 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 280 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 290 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 300 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 310 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 320 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 330 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 410 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 420 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 34 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 20 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 30 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 40 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 50 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 60 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 70 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 80 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 90 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 100 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 110 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 120 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 130 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 140 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 170 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 180 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 270 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 280 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 310 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 320 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 410 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 420 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 450 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 460 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 470 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 35 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 20 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 30 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 40 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 50 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 60 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 70 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 80 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 90 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 100 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 110 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 120 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 130 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 140 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 150 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 160 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 170 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 180 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 190 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 200 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 210 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 220 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 230 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 240 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 250 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 260 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 270 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 280 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 290 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 300 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 310 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 320 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 330 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 340 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 350 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 360 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 370 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 380 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 410 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 420 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 430 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 36 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 20 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 30 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 40 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 50 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 60 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 70 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 80 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 90 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 100 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 110 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 120 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 130 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 140 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 150 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 160 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 170 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 180 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 190 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 200 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 210 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 220 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 230 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 240 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 250 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 260 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 270 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 280 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 290 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 300 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 310 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 320 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 330 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 340 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 350 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 360 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 370 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 380 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 390 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 400 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 410 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 420 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 430 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 460 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 490 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 500 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 600 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 680 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 690 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 700 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 710 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 840 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 850 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 860 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 870 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 880 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 890 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 900 ---> Missclassification: 0.016949152542372836\n",
      "----- Iteration 37 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.13559322033898302\n",
      "Training Data: 20 ---> Missclassification: 0.14830508474576276\n",
      "Training Data: 30 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 40 ---> Missclassification: 0.07627118644067798\n",
      "Training Data: 50 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 60 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 70 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 80 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 90 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 100 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 110 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 120 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 130 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 140 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 150 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 160 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 170 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 180 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 190 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 200 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 210 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 220 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 230 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 240 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 250 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 260 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 270 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 280 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 290 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 300 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 310 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 320 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 330 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 340 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 350 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 360 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 370 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 380 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 390 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 400 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 410 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 420 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 38 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.1673728813559322\n",
      "Training Data: 20 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 30 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 40 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 50 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 60 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 70 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 80 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 90 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 100 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 110 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 120 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 130 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 140 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 150 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 160 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 170 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 180 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 190 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 200 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 210 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 220 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 230 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 240 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 250 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 260 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 310 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 320 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 390 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 530 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 540 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 550 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 680 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 690 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 700 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 710 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 890 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 900 ---> Missclassification: 0.014830508474576232\n",
      "----- Iteration 39 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.30508474576271183\n",
      "Training Data: 20 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 30 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 40 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 50 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 60 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 70 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 80 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 90 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 100 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 110 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 120 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 130 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 140 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 150 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 160 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 170 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 180 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 190 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 200 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 210 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 220 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 230 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 240 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 250 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 260 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 270 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 280 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 290 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 300 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 310 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 320 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 330 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 340 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 350 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 360 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 370 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 380 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 390 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 400 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 410 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 420 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 430 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 440 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 450 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 460 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 470 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 480 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 490 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 520 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 530 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 540 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 550 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 560 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 570 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 580 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 590 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 600 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 610 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 620 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 630 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 640 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 650 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 660 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 670 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 680 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 690 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 700 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 710 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 720 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 730 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 740 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 750 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 760 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 770 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 780 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 790 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 800 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 810 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 820 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 830 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 840 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 850 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 860 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 870 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 880 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 890 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 900 ---> Missclassification: 0.004237288135593209\n",
      "----- Iteration 40 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 20 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 30 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 40 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 50 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 60 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 70 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 80 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 90 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 100 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 110 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 120 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 130 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 140 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 170 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 180 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 250 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 260 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 270 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 280 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 290 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 300 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 310 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 320 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 330 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 340 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 350 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 360 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 370 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 380 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 390 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 400 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 410 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 420 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 430 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 440 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 450 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 460 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 470 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 480 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 490 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 500 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 510 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 520 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 530 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 540 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 550 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 560 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 570 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 580 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 590 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 600 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 610 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 620 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 630 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 640 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 650 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 660 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 670 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 680 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 690 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 700 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 710 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 720 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 730 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 740 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 790 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 800 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 41 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 20 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 30 ---> Missclassification: 0.10805084745762716\n",
      "Training Data: 40 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 50 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 60 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 70 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 80 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 90 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 100 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 110 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 120 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 130 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 140 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 150 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 160 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 170 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 180 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 190 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 200 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 210 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 220 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 230 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 240 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 250 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 260 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 290 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 300 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 310 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 320 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 330 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 340 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 350 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 360 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 370 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 380 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 390 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 400 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 410 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 420 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 430 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 440 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 450 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 460 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 470 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 480 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 490 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 500 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 510 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 520 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 530 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 540 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 550 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 560 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 570 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 580 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 590 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 600 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 610 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 620 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 630 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 640 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 650 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 660 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 670 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 680 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 690 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 700 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 710 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 720 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 730 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 740 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 750 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 760 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 770 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 780 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 790 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 800 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 810 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 820 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 830 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 840 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 850 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 860 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 870 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 880 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 890 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 900 ---> Missclassification: 0.016949152542372836\n",
      "----- Iteration 42 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 20 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 30 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 40 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 50 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 60 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 70 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 80 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 90 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 100 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 110 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 120 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 130 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 140 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 150 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 160 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 170 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 180 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 190 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 200 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 210 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 220 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 230 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 240 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 250 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 260 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 270 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 280 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 310 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 320 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 330 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 340 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 350 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 360 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 370 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 380 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 390 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 400 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 590 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 43 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 20 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 30 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 40 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 50 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 60 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 70 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 80 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 90 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 100 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 110 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 120 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 130 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 140 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 150 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 160 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 170 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 180 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 190 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 200 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 210 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 220 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 230 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 240 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 250 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 260 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 270 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 280 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 310 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 320 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 330 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 340 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 350 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 360 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 370 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 380 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 390 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 400 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 410 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 420 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 430 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 44 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 20 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 30 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 40 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 50 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 60 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 70 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 80 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 90 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 100 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 110 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 120 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 130 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 140 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 150 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 160 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 170 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 180 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 270 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 280 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 290 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 300 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 310 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 320 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 390 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 420 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 430 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 45 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 20 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 30 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 40 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 50 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 60 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 70 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 80 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 90 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 100 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 110 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 120 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 130 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 140 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 150 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 160 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 170 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 180 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 190 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 200 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 210 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 220 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 230 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 240 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 250 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 260 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 310 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 320 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 330 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 340 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 350 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 360 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 370 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 410 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 420 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 46 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.19703389830508478\n",
      "Training Data: 20 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 30 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 40 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 50 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 60 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 70 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 80 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 90 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 100 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 110 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 120 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 130 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 140 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 150 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 160 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 170 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 180 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 190 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 250 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 260 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 270 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 280 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 290 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 300 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 310 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 320 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 330 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 340 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 350 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 360 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 370 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 380 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 390 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 400 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 410 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 420 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 47 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.11864406779661019\n",
      "Training Data: 20 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 30 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 40 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 50 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 60 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 70 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 80 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 90 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 100 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 110 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 120 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 130 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 140 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 150 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 160 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 170 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 180 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 190 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 200 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 210 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 220 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 230 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 240 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 250 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 260 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 270 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 280 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 290 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 300 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 310 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 320 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 330 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 340 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 350 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 360 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 370 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 380 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 430 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 440 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 450 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 460 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 470 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 480 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 490 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 520 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 530 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 540 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 550 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 560 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 570 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 580 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 590 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 600 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 610 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 620 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 630 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 640 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 650 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 660 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 670 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 680 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 690 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 700 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 710 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 720 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 730 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 740 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 750 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 760 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 48 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 20 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 30 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 40 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 50 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 60 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 70 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 80 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 90 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 100 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 110 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 120 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 130 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 140 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 150 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 160 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 170 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 180 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 190 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 200 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 210 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 220 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 230 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 240 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 250 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 260 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 270 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 280 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 290 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 300 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 310 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 320 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 330 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 340 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 350 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 360 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 370 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 380 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 390 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 400 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 410 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 420 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 49 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 20 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 30 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 40 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 50 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 60 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 70 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 80 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 90 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 100 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 110 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 120 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 130 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 140 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 150 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 160 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 170 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 180 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 190 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 200 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 210 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 220 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 230 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 240 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 250 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 260 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 270 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 280 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 290 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 300 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 310 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 320 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 330 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 340 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 350 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 360 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 370 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 380 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 390 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 400 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 410 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 420 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 430 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n"
     ]
    }
   ],
   "source": [
    "df_error = pd.DataFrame(np.zeros((90, 50)), columns=[f\"Iteration_{i}\" for i in range(1, 51)])\n",
    "\n",
    "for i in range(50):\n",
    "    print(f'----- Iteration {i} running -----')\n",
    "\n",
    "    test_data = banknote.sample(n=472)\n",
    "    train_data = banknote.drop(test_data.index)\n",
    "\n",
    "    test_data_X = test_data.iloc[:,:-1]\n",
    "    test_data_y = test_data.iloc[:,-1]\n",
    "\n",
    "    select_10_train = train_data.sample(n=10)\n",
    "    train_data = train_data.drop(select_10_train.index)\n",
    "\n",
    "    data_volume = len(select_10_train)\n",
    "\n",
    "    select_10_train_X = select_10_train.iloc[:,:-1]\n",
    "    select_10_train_y = select_10_train.iloc[:,-1]\n",
    "\n",
    "    lsvc = LinearSVC(penalty='l1', \n",
    "                loss= 'squared_hinge',\n",
    "                dual='auto',\n",
    "                max_iter=10000,\n",
    "                C=1)\n",
    "    \n",
    "    lsvc.fit(select_10_train_X, select_10_train_y)\n",
    "    y_pred = lsvc.predict(test_data_X)\n",
    "    error = 1-accuracy_score(test_data_y, y_pred)\n",
    "    cc=0\n",
    "    df_error.iloc[cc,i] = round(error,6)\n",
    "    \n",
    "    print(f'Training Data: {len(select_10_train)} ---> Missclassification: {error}')\n",
    "    \n",
    "    while len(train_data)>0:\n",
    "        \n",
    "        new_10_train = train_data.sample(n=10)\n",
    "        train_data = train_data.drop(new_10_train.index)\n",
    "        select_10_train = pd.concat([new_10_train,select_10_train])\n",
    "\n",
    "        data_volume = len(select_10_train)\n",
    "\n",
    "        select_10_train_X = select_10_train.iloc[:,:-1]\n",
    "        select_10_train_y = select_10_train.iloc[:,-1]\n",
    "\n",
    "        lsvc = LinearSVC(penalty='l1', \n",
    "                    loss= 'squared_hinge',\n",
    "                    dual='auto',\n",
    "                    max_iter=10000,\n",
    "                    C=1)\n",
    "        \n",
    "        lsvc.fit(select_10_train_X, select_10_train_y)\n",
    "        y_pred = lsvc.predict(test_data_X)\n",
    "        error = 1-accuracy_score(test_data_y, y_pred)\n",
    "        \n",
    "        cc+=1\n",
    "        df_error.iloc[cc,i] = round(error,6)\n",
    "        \n",
    "        print(f'Training Data: {len(select_10_train)} ---> Missclassification: {error}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration_1</th>\n",
       "      <th>Iteration_2</th>\n",
       "      <th>Iteration_3</th>\n",
       "      <th>Iteration_4</th>\n",
       "      <th>Iteration_5</th>\n",
       "      <th>Iteration_6</th>\n",
       "      <th>Iteration_7</th>\n",
       "      <th>Iteration_8</th>\n",
       "      <th>Iteration_9</th>\n",
       "      <th>Iteration_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Iteration_41</th>\n",
       "      <th>Iteration_42</th>\n",
       "      <th>Iteration_43</th>\n",
       "      <th>Iteration_44</th>\n",
       "      <th>Iteration_45</th>\n",
       "      <th>Iteration_46</th>\n",
       "      <th>Iteration_47</th>\n",
       "      <th>Iteration_48</th>\n",
       "      <th>Iteration_49</th>\n",
       "      <th>Iteration_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.205508</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.046610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.201271</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.031780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.031780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iteration_1  Iteration_2  Iteration_3  Iteration_4  Iteration_5  \\\n",
       "0      0.014831     0.118644     0.205508     0.021186     0.025424   \n",
       "1      0.025424     0.029661     0.201271     0.019068     0.012712   \n",
       "2      0.025424     0.029661     0.074153     0.019068     0.025424   \n",
       "3      0.025424     0.019068     0.027542     0.012712     0.025424   \n",
       "4      0.014831     0.014831     0.027542     0.012712     0.008475   \n",
       "..          ...          ...          ...          ...          ...   \n",
       "85     0.008475     0.006356     0.010593     0.006356     0.016949   \n",
       "86     0.008475     0.004237     0.010593     0.006356     0.016949   \n",
       "87     0.008475     0.004237     0.010593     0.006356     0.016949   \n",
       "88     0.010593     0.004237     0.010593     0.006356     0.016949   \n",
       "89     0.010593     0.004237     0.010593     0.006356     0.016949   \n",
       "\n",
       "    Iteration_6  Iteration_7  Iteration_8  Iteration_9  Iteration_10  ...  \\\n",
       "0      0.082627     0.108051     0.097458     0.105932      0.114407  ...   \n",
       "1      0.080508     0.012712     0.148305     0.027542      0.076271  ...   \n",
       "2      0.082627     0.027542     0.118644     0.025424      0.019068  ...   \n",
       "3      0.016949     0.042373     0.040254     0.038136      0.023305  ...   \n",
       "4      0.016949     0.021186     0.033898     0.023305      0.023305  ...   \n",
       "..          ...          ...          ...          ...           ...  ...   \n",
       "85     0.010593     0.012712     0.010593     0.014831      0.010593  ...   \n",
       "86     0.010593     0.012712     0.010593     0.014831      0.008475  ...   \n",
       "87     0.010593     0.012712     0.010593     0.014831      0.008475  ...   \n",
       "88     0.010593     0.012712     0.010593     0.014831      0.008475  ...   \n",
       "89     0.010593     0.012712     0.010593     0.014831      0.008475  ...   \n",
       "\n",
       "    Iteration_41  Iteration_42  Iteration_43  Iteration_44  Iteration_45  \\\n",
       "0       0.103814      0.021186      0.097458      0.067797      0.074153   \n",
       "1       0.014831      0.099576      0.014831      0.042373      0.084746   \n",
       "2       0.014831      0.108051      0.012712      0.010593      0.091102   \n",
       "3       0.014831      0.097458      0.014831      0.014831      0.065678   \n",
       "4       0.012712      0.095339      0.014831      0.014831      0.038136   \n",
       "..           ...           ...           ...           ...           ...   \n",
       "85      0.012712      0.016949      0.012712      0.010593      0.010593   \n",
       "86      0.012712      0.016949      0.012712      0.010593      0.012712   \n",
       "87      0.012712      0.016949      0.012712      0.010593      0.012712   \n",
       "88      0.012712      0.016949      0.012712      0.010593      0.012712   \n",
       "89      0.012712      0.016949      0.012712      0.010593      0.012712   \n",
       "\n",
       "    Iteration_46  Iteration_47  Iteration_48  Iteration_49  Iteration_50  \n",
       "0       0.042373      0.197034      0.118644      0.120763      0.046610  \n",
       "1       0.031780      0.004237      0.006356      0.016949      0.033898  \n",
       "2       0.025424      0.008475      0.059322      0.016949      0.023305  \n",
       "3       0.014831      0.008475      0.031780      0.036017      0.031780  \n",
       "4       0.008475      0.008475      0.031780      0.019068      0.031780  \n",
       "..           ...           ...           ...           ...           ...  \n",
       "85      0.008475      0.010593      0.006356      0.008475      0.010593  \n",
       "86      0.008475      0.010593      0.006356      0.008475      0.010593  \n",
       "87      0.008475      0.010593      0.006356      0.008475      0.010593  \n",
       "88      0.008475      0.010593      0.006356      0.008475      0.010593  \n",
       "89      0.008475      0.010593      0.006356      0.008475      0.010593  \n",
       "\n",
       "[90 rows x 50 columns]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Train a SVM with a pool of 10 randomly selected data points from the training\n",
    "set5 using linear kernel and L1 penalty. Select the parameters of the SVM\n",
    "with 5-fold cross validation. Choose the 10 closest data points in the training\n",
    "set to the hyperplane of the SVM6 and add them to the pool. Do not replace\n",
    "the samples back into the training set. Train a new SVM using the pool.\n",
    "Repeat this process until all training data is used. You will have 90 SVMs\n",
    "that were trained using 10, 20, 30,..., 900 data points and their 90 test errors.\n",
    "You have implemented active learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758     0\n",
      "195     0\n",
      "637     0\n",
      "37      0\n",
      "876     1\n",
      "911     1\n",
      "1332    1\n",
      "623     0\n",
      "633     0\n",
      "412     0\n",
      "Name: class, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\云忆\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  param_C  mean_test_score\n",
      "0   0.001              0.7\n",
      "1    0.01              0.7\n",
      "2     0.1              0.6\n",
      "3       1              0.7\n",
      "4      10              0.7\n",
      "5     100              0.7\n",
      "6    1000              0.7\n"
     ]
    }
   ],
   "source": [
    "columns_name = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
    "banknote=pd.read_csv('../hw8-data/data_banknote_authentication.csv', names=columns_name)\n",
    "test_data = banknote.sample(n=472, random_state=42)\n",
    "train_data = banknote.drop(test_data.index)\n",
    "\n",
    "test_data_X = test_data.iloc[:,:-1]\n",
    "test_data_y = test_data.iloc[:,-1]\n",
    "\n",
    "train_data_X = train_data.iloc[:,:-1]\n",
    "train_data_y = train_data.iloc[:,-1]\n",
    "\n",
    "select_10_train = train_data.sample(n=10, random_state=0)\n",
    "train_data = train_data.drop(select_10_train.index)\n",
    "\n",
    "select_10_train_X = select_10_train.iloc[:,:-1]\n",
    "select_10_train_y = select_10_train.iloc[:,-1]\n",
    "print(select_10_train_y)\n",
    "\n",
    "parameters_base_l1 = {'penalty':[\"l1\"],\n",
    "                   'loss': ['squared_hinge'],\n",
    "                   'dual':['auto'],\n",
    "                   'max_iter':[10000],\n",
    "                   'C':[10**n for n in range(-3,4)]}\n",
    "\n",
    "clf = LinearSVC(penalty='l1', dual=\"auto\", random_state=0)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=clf, \n",
    "                            param_grid=parameters_base_l1, \n",
    "                            scoring = 'accuracy', \n",
    "                            cv=5)\n",
    "grid_search.fit(select_10_train_X, select_10_train_y)\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)[['param_C','mean_test_score']]\n",
    "results_range_df = results_df[results_df['mean_test_score']>0.5]\n",
    "print(results_range_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Iteration 0 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 20 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 30 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 40 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 50 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 60 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 70 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 80 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 90 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 100 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 110 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 120 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 130 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 140 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 150 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 160 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 170 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 180 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 190 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 200 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 210 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 220 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 230 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 240 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 250 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 260 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 270 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 280 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 290 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 300 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 310 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 320 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 330 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 340 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 350 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 360 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 370 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 380 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 390 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 400 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 410 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 420 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 430 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 440 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 450 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 460 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 470 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 480 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 490 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 1 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 20 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 30 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 40 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 50 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 60 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 70 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 80 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 90 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 100 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 110 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 120 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 130 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 140 ---> Missclassification: 0.1610169491525424\n",
      "Training Data: 150 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 160 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 170 ---> Missclassification: 0.14406779661016944\n",
      "Training Data: 180 ---> Missclassification: 0.14406779661016944\n",
      "Training Data: 190 ---> Missclassification: 0.13559322033898302\n",
      "Training Data: 200 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 210 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 220 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 230 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 240 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 250 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 260 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 270 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 280 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 290 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 300 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 310 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 320 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 330 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 340 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 350 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 360 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 370 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 380 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 390 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 400 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 410 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 420 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 430 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 440 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 450 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 460 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 500 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 2 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 20 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 30 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 40 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 50 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 60 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 70 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 80 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 90 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 100 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 110 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 120 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 130 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 140 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 150 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 160 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 170 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 180 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 190 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 200 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 210 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 220 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 230 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 240 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 250 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 260 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 270 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 280 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 290 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 300 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 310 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 320 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 330 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 340 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 350 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 360 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 370 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 380 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 390 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 400 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 410 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 420 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 430 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 440 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 450 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 460 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 470 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 480 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 490 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 500 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 600 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 3 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 20 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 30 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 40 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 50 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 60 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 70 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 80 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 90 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 100 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 110 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 120 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 130 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 140 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 150 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 160 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 170 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 180 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 190 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 200 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 210 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 220 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 230 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 240 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 250 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 260 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 270 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 280 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 290 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 300 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 310 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 320 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 330 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 340 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 350 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 360 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 370 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 380 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 390 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 400 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 410 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 420 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 430 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 440 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 450 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 460 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 470 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 480 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 490 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 500 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 510 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 4 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 20 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 30 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 40 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 50 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 60 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 70 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 80 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 90 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 100 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 110 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 120 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 130 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 140 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 150 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 160 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 170 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 180 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 190 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 200 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 210 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 220 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 230 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 240 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 250 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 260 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 270 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 280 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 290 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 300 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 310 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 320 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 330 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 340 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 350 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 360 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 370 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 380 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 390 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 400 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 410 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 420 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 430 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 440 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 450 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 460 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 470 ---> Missclassification: 0.11864406779661019\n",
      "Training Data: 480 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 490 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 520 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 5 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 20 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 30 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 40 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 50 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 60 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 70 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 80 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 90 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 100 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 110 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 120 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 130 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 140 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 150 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 160 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 170 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 180 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 190 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 200 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 210 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 220 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 230 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 240 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 250 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 260 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 270 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 280 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 290 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 300 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 310 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 320 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 330 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 340 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 350 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 360 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 370 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 380 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 390 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 400 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 530 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 540 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 550 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 560 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 570 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 580 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 590 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 600 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 610 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 620 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 630 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 640 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 650 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 660 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 670 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 680 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 690 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 700 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 710 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 720 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 730 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 740 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 750 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 760 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 770 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 780 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 790 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 800 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 810 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 820 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 830 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 840 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 850 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 860 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 870 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 880 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 890 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 900 ---> Missclassification: 0.004237288135593209\n",
      "----- Iteration 6 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 20 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 30 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 40 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 50 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 60 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 70 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 80 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 90 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 100 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 110 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 120 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 130 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 140 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 150 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 160 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 170 ---> Missclassification: 0.14830508474576276\n",
      "Training Data: 180 ---> Missclassification: 0.14830508474576276\n",
      "Training Data: 190 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 200 ---> Missclassification: 0.13347457627118642\n",
      "Training Data: 210 ---> Missclassification: 0.1313559322033898\n",
      "Training Data: 220 ---> Missclassification: 0.1313559322033898\n",
      "Training Data: 230 ---> Missclassification: 0.1228813559322034\n",
      "Training Data: 240 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 250 ---> Missclassification: 0.12076271186440679\n",
      "Training Data: 260 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 270 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 280 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 290 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 300 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 310 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 320 ---> Missclassification: 0.10805084745762716\n",
      "Training Data: 330 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 340 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 350 ---> Missclassification: 0.10805084745762716\n",
      "Training Data: 360 ---> Missclassification: 0.1228813559322034\n",
      "Training Data: 370 ---> Missclassification: 0.10169491525423724\n",
      "Training Data: 380 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 390 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 400 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 410 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 420 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 430 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 440 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 450 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 460 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 470 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 480 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 490 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 600 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 680 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 690 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 700 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 710 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 840 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 850 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 860 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 890 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 900 ---> Missclassification: 0.014830508474576232\n",
      "----- Iteration 7 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 20 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 30 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 40 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 50 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 60 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 70 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 80 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 90 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 100 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 110 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 120 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 130 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 140 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 150 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 160 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 170 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 180 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 190 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 200 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 210 ---> Missclassification: 0.18855932203389836\n",
      "Training Data: 220 ---> Missclassification: 0.18644067796610164\n",
      "Training Data: 230 ---> Missclassification: 0.18644067796610164\n",
      "Training Data: 240 ---> Missclassification: 0.18644067796610164\n",
      "Training Data: 250 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 260 ---> Missclassification: 0.15042372881355937\n",
      "Training Data: 270 ---> Missclassification: 0.13983050847457623\n",
      "Training Data: 280 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 290 ---> Missclassification: 0.13559322033898302\n",
      "Training Data: 300 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 310 ---> Missclassification: 0.11864406779661019\n",
      "Training Data: 320 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 330 ---> Missclassification: 0.10805084745762716\n",
      "Training Data: 340 ---> Missclassification: 0.10805084745762716\n",
      "Training Data: 350 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 360 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 370 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 380 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 390 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 400 ---> Missclassification: 0.0826271186440678\n",
      "Training Data: 410 ---> Missclassification: 0.0826271186440678\n",
      "Training Data: 420 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 430 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 440 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 540 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 550 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 560 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 570 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 580 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 590 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 600 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 610 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 620 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 630 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 640 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 650 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 660 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 670 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 680 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 690 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 700 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 710 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 720 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 730 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 740 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 750 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 760 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 770 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 780 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 790 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 800 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 810 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 820 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 830 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 840 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 850 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 860 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 870 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 880 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 890 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 900 ---> Missclassification: 0.016949152542372836\n",
      "----- Iteration 8 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.21822033898305082\n",
      "Training Data: 20 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 30 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 40 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 50 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 60 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 70 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 80 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 90 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 100 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 110 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 120 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 130 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 140 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 150 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 160 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 170 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 180 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 190 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 200 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 210 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 220 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 230 ---> Missclassification: 0.15677966101694918\n",
      "Training Data: 240 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 250 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 260 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 310 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 320 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 330 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 340 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 350 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 360 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 370 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 380 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 480 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 490 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 520 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 530 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 540 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 550 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 560 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 570 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 580 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 590 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 600 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 610 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 620 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 630 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 640 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 650 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 660 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 670 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 680 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 690 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 700 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 710 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 720 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 730 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 740 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 750 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 760 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 770 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 780 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 790 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 800 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 810 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 820 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 830 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 840 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 850 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 860 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 870 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 880 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 890 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 900 ---> Missclassification: 0.004237288135593209\n",
      "----- Iteration 9 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 20 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 30 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 40 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 50 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 60 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 70 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 80 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 90 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 100 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 110 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 120 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 130 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 140 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 150 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 160 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 170 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 180 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 190 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 200 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 210 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 220 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 230 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 240 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 250 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 260 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 270 ---> Missclassification: 0.13771186440677963\n",
      "Training Data: 280 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 290 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 300 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 310 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 320 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 330 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 340 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 350 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 360 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 370 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 380 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 390 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 400 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 410 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 420 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 430 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 440 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 450 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 460 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 500 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 510 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 10 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 20 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 30 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 40 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 50 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 60 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 70 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 80 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 90 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 100 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 110 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 120 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 130 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 140 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 150 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 160 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 170 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 180 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 190 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 200 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 210 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 220 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 230 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 240 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 250 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 260 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 270 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 280 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 290 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 300 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 310 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 320 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 330 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 340 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 350 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 360 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 370 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 380 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 390 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 400 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 410 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 420 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 430 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 440 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 450 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 460 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 470 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 480 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 490 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 11 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 20 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 30 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 40 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 50 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 60 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 70 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 80 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 90 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 100 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 110 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 120 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 130 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 140 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 150 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 160 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 170 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 180 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 190 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 200 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 210 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 220 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 230 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 240 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 250 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 260 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 270 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 280 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 290 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 300 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 310 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 320 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 330 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 340 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 350 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 360 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 370 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 380 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 390 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 400 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 410 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 420 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 430 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 440 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 450 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 460 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 470 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 480 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 490 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 12 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 20 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 30 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 40 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 50 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 60 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 70 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 80 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 90 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 100 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 110 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 120 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 130 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 140 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 150 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 160 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 170 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 180 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 190 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 200 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 210 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 220 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 230 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 240 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 250 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 260 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 270 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 280 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 290 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 300 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 310 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 320 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 330 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 340 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 350 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 360 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 370 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 380 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 390 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 400 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 410 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 420 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 430 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 440 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 450 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 460 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 13 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 20 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 30 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 40 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 50 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 60 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 70 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 80 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 90 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 100 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 110 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 120 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 130 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 140 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 150 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 160 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 170 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 180 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 190 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 200 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 210 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 220 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 230 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 240 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 250 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 260 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 270 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 280 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 290 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 300 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 310 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 320 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 330 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 340 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 350 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 360 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 370 ---> Missclassification: 0.14194915254237284\n",
      "Training Data: 380 ---> Missclassification: 0.14830508474576276\n",
      "Training Data: 390 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 400 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 410 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 420 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 430 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 440 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 450 ---> Missclassification: 0.0826271186440678\n",
      "Training Data: 460 ---> Missclassification: 0.0826271186440678\n",
      "Training Data: 470 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 480 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 490 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 500 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 510 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 14 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 20 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 30 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 40 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 50 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 60 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 70 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 80 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 90 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 100 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 110 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 120 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 130 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 140 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 150 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 160 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 170 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 180 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 190 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 200 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 210 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 220 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 230 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 240 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 250 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 260 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 270 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 280 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 290 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 300 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 310 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 320 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 330 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 340 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 350 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 360 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 370 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 380 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 390 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 400 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 410 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 420 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 430 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 440 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 450 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 460 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 470 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 480 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 490 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 500 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 510 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 600 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 680 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 690 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 700 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 710 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 840 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 850 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 860 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 890 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 900 ---> Missclassification: 0.014830508474576232\n",
      "----- Iteration 15 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 20 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 30 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 40 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 50 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 60 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 70 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 80 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 90 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 100 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 110 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 120 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 130 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 140 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 150 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 160 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 170 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 180 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 190 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 200 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 210 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 220 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 230 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 240 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 250 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 260 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 270 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 280 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 290 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 300 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 310 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 320 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 330 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 340 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 350 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 360 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 370 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 380 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 390 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 400 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 410 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 420 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 430 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 440 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 450 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 460 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 470 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 16 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 20 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 30 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 40 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 50 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 60 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 70 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 80 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 90 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 100 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 110 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 120 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 130 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 140 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 150 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 160 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 170 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 180 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 190 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 200 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 210 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 220 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 230 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 240 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 250 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 260 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 270 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 280 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 290 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 300 ---> Missclassification: 0.11228813559322037\n",
      "Training Data: 310 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 320 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 330 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 340 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 350 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 360 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 370 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 380 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 390 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 400 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 410 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 420 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 430 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 440 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 450 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 480 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 490 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 520 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 530 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 540 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 550 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 560 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 570 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 580 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 590 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 600 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 610 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 620 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 630 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 640 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 650 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 660 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 670 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 680 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 690 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 700 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 710 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 720 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 730 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 740 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 750 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 760 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 770 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 780 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 790 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 800 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 810 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 820 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 830 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 840 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 850 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 860 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 870 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 880 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 890 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 900 ---> Missclassification: 0.004237288135593209\n",
      "----- Iteration 17 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.2033898305084746\n",
      "Training Data: 20 ---> Missclassification: 0.19915254237288138\n",
      "Training Data: 30 ---> Missclassification: 0.2033898305084746\n",
      "Training Data: 40 ---> Missclassification: 0.2055084745762712\n",
      "Training Data: 50 ---> Missclassification: 0.20127118644067798\n",
      "Training Data: 60 ---> Missclassification: 0.2055084745762712\n",
      "Training Data: 70 ---> Missclassification: 0.2033898305084746\n",
      "Training Data: 80 ---> Missclassification: 0.20127118644067798\n",
      "Training Data: 90 ---> Missclassification: 0.2033898305084746\n",
      "Training Data: 100 ---> Missclassification: 0.19915254237288138\n",
      "Training Data: 110 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 120 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 130 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 140 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 150 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 160 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 170 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 180 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 190 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 200 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 210 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 220 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 230 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 240 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 250 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 260 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 270 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 280 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 290 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 300 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 310 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 320 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 330 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 340 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 350 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 360 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 370 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 380 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 390 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 400 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 410 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 420 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 430 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 460 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 470 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 480 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 600 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 680 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 690 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 700 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 710 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 840 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 850 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 860 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 890 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 900 ---> Missclassification: 0.014830508474576232\n",
      "----- Iteration 18 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.19067796610169496\n",
      "Training Data: 20 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 30 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 40 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 50 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 60 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 70 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 80 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 90 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 100 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 110 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 120 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 130 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 140 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 150 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 160 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 170 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 180 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 190 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 200 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 210 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 220 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 230 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 240 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 250 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 260 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 270 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 280 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 290 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 300 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 310 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 320 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 330 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 340 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 350 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 360 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 370 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 380 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 390 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 400 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 410 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 420 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 430 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 440 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 450 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 460 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 19 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 20 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 30 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 40 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 50 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 60 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 70 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 80 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 90 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 100 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 110 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 120 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 130 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 140 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 150 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 160 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 170 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 180 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 190 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 200 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 210 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 220 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 230 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 240 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 250 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 260 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 270 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 280 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 290 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 300 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 310 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 320 ---> Missclassification: 0.19491525423728817\n",
      "Training Data: 330 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 340 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 350 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 360 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 370 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 380 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 390 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 400 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 410 ---> Missclassification: 0.10593220338983056\n",
      "Training Data: 420 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 430 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 440 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 450 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 460 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 470 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 480 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 490 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 500 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 20 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 20 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 30 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 40 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 50 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 60 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 70 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 80 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 90 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 100 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 110 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 120 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 130 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 140 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 150 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 160 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 170 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 180 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 190 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 200 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 210 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 220 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 230 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 240 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 250 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 260 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 270 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 280 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 290 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 300 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 310 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 320 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 330 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 340 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 350 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 360 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 370 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 380 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 390 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 400 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 410 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 420 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 430 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 440 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 450 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 460 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 470 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 480 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 490 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 500 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 530 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 21 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.18644067796610164\n",
      "Training Data: 20 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 30 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 40 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 50 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 60 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 70 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 80 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 90 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 100 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 110 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 120 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 130 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 140 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 150 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 160 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 170 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 180 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 190 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 200 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 210 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 220 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 230 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 240 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 250 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 260 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 270 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 280 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 290 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 300 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 310 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 320 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 330 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 340 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 350 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 360 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 370 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 380 ---> Missclassification: 0.09745762711864403\n",
      "Training Data: 390 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 400 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 410 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 420 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 430 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 440 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 450 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 460 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 470 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 480 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 22 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 20 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 30 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 40 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 50 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 60 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 70 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 80 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 90 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 100 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 110 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 120 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 130 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 140 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 150 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 160 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 170 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 180 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 190 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 200 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 210 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 220 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 230 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 240 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 250 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 260 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 270 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 280 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 290 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 300 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 310 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 320 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 330 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 340 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 350 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 360 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 370 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 380 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 390 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 400 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 410 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 420 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 430 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 440 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 450 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 460 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 470 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 480 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 490 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 500 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 23 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 20 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 30 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 40 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 50 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 60 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 70 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 80 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 90 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 100 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 110 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 120 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 130 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 140 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 150 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 160 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 170 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 180 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 190 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 200 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 210 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 220 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 230 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 240 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 250 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 260 ---> Missclassification: 0.18432203389830504\n",
      "Training Data: 270 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 280 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 290 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 300 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 310 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 320 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 330 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 340 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 350 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 360 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 370 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 380 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 390 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 400 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 410 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 420 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 430 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 440 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 450 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 480 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 490 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 500 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 24 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 20 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 30 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 40 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 50 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 60 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 70 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 80 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 90 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 100 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 110 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 120 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 130 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 140 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 150 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 160 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 170 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 180 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 190 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 200 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 210 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 220 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 230 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 240 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 250 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 260 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 270 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 280 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 290 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 300 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 310 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 320 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 330 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 340 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 350 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 360 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 370 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 380 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 390 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 400 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 410 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 420 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 430 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 440 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 450 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 460 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 470 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 480 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 540 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 550 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 560 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 570 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 580 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 590 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 600 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 610 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 620 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 630 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 640 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 650 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 660 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 670 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 680 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 690 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 700 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 710 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 720 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 730 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 740 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 750 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 760 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 770 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 780 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 790 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 800 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 810 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 820 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 830 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 840 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 850 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 860 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 870 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 880 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 890 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 900 ---> Missclassification: 0.016949152542372836\n",
      "----- Iteration 25 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 20 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 30 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 40 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 50 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 60 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 70 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 80 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 90 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 100 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 110 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 120 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 130 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 140 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 150 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 160 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 170 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 180 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 190 ---> Missclassification: 0.07627118644067798\n",
      "Training Data: 200 ---> Missclassification: 0.07627118644067798\n",
      "Training Data: 210 ---> Missclassification: 0.07627118644067798\n",
      "Training Data: 220 ---> Missclassification: 0.07627118644067798\n",
      "Training Data: 230 ---> Missclassification: 0.07627118644067798\n",
      "Training Data: 240 ---> Missclassification: 0.08686440677966101\n",
      "Training Data: 250 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 260 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 270 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 280 ---> Missclassification: 0.08686440677966101\n",
      "Training Data: 290 ---> Missclassification: 0.08686440677966101\n",
      "Training Data: 300 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 310 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 320 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 330 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 340 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 350 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 360 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 370 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 380 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 390 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 400 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 410 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 420 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 430 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 440 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 450 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 26 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 20 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 30 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 40 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 50 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 60 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 70 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 80 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 90 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 100 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 110 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 120 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 130 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 140 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 150 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 160 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 170 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 180 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 190 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 200 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 210 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 220 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 230 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 240 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 250 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 260 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 270 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 280 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 290 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 300 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 310 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 320 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 330 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 340 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 350 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 360 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 370 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 380 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 390 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 400 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 410 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 27 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 20 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 30 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 40 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 50 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 60 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 70 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 80 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 90 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 100 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 110 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 120 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 130 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 140 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 150 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 160 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 170 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 180 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 190 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 200 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 210 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 220 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 230 ---> Missclassification: 0.1292372881355932\n",
      "Training Data: 240 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 250 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 260 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 270 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 280 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 290 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 300 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 310 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 320 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 330 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 340 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 350 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 360 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 370 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 380 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 390 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 400 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 410 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 420 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 430 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 440 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 450 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 460 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 870 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.010593220338983023\n",
      "----- Iteration 28 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 20 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 30 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 40 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 50 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 60 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 70 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 80 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 90 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 100 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 110 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 120 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 130 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 140 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 150 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 160 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 170 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 180 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 190 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 200 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 210 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 220 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 230 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 240 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 250 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 260 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 270 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 280 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 290 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 300 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 310 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 320 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 330 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 340 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 350 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 360 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 370 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 380 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 390 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 400 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 410 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 420 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 430 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 440 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 450 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 460 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 470 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 480 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 490 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 500 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 510 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 740 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 770 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 780 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 820 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 830 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 840 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 850 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 860 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 870 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 880 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 890 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 900 ---> Missclassification: 0.016949152542372836\n",
      "----- Iteration 29 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 20 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 30 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 40 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 50 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 60 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 70 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 80 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 90 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 100 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 110 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 120 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 130 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 140 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 150 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 160 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 170 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 180 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 190 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 200 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 210 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 220 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 230 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 240 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 250 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 260 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 270 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 280 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 290 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 300 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 310 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 320 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 330 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 340 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 350 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 360 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 370 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 380 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 390 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 400 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 410 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 420 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 430 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 440 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 450 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 460 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 490 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 500 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 30 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 20 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 30 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 40 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 50 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 60 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 70 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 80 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 90 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 100 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 110 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 120 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 130 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 140 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 150 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 160 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 170 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 180 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 190 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 200 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 210 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 220 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 230 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 240 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 250 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 260 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 270 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 280 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 290 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 300 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 310 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 320 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 330 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 340 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 350 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 360 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 370 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 380 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 390 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 400 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 410 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 420 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 430 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 440 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 450 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 460 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 490 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 500 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 510 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 520 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 31 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 20 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 30 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 40 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 50 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 60 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 70 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 80 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 90 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 100 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 110 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 120 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 130 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 140 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 150 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 160 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 170 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 180 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 190 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 200 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 210 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 220 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 230 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 240 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 250 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 260 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 270 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 280 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 290 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 300 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 310 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 320 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 330 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 340 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 350 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 360 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 370 ---> Missclassification: 0.09957627118644063\n",
      "Training Data: 380 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 390 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 400 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 410 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 420 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 430 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 440 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 450 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 460 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 32 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 20 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 30 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 40 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 50 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 60 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 70 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 80 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 90 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 100 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 110 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 120 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 130 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 140 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 150 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 160 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 170 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 180 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 190 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 200 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 210 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 220 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 230 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 240 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 250 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 260 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 270 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 280 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 290 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 300 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 310 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 320 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 330 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 340 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 350 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 360 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 370 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 380 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 390 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 400 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 410 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 420 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 430 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 440 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 450 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 460 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 470 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 480 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 490 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 600 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 680 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 690 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 700 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 710 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 840 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 850 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 860 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 890 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 900 ---> Missclassification: 0.014830508474576232\n",
      "----- Iteration 33 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 20 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 30 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 40 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 50 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 60 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 70 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 80 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 90 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 100 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 110 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 120 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 130 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 140 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 150 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 160 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 170 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 180 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 190 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 200 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 210 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 220 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 230 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 240 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 250 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 260 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 270 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 280 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 290 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 300 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 310 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 320 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 330 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 340 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 350 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 360 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 370 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 380 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 390 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 400 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 410 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 420 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 430 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 440 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 450 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 460 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 470 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 480 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 490 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 34 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 20 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 30 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 40 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 50 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 60 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 70 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 80 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 90 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 100 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 110 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 120 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 130 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 140 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 150 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 160 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 170 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 180 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 190 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 200 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 210 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 220 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 230 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 240 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 250 ---> Missclassification: 0.1716101694915254\n",
      "Training Data: 260 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 270 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 280 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 290 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 300 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 310 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 320 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 330 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 340 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 350 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 360 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 370 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 380 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 390 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 400 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 410 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 420 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 430 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 440 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 450 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 460 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 470 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 500 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 510 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 540 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 550 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 560 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 570 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 580 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 590 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 600 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 610 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 620 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 630 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 640 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 650 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 660 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 670 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 680 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 690 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 700 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 710 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 720 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 730 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 740 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 750 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 760 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 770 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 780 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 790 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 800 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 810 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 820 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 830 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 840 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 850 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 860 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 870 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 880 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 890 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 900 ---> Missclassification: 0.02330508474576276\n",
      "----- Iteration 35 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 20 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 30 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 40 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 50 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 60 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 70 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 80 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 90 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 100 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 110 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 120 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 130 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 140 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 150 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 160 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 170 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 180 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 190 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 200 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 210 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 220 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 230 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 240 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 250 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 260 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 270 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 280 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 290 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 300 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 310 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 320 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 330 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 340 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 350 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 360 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 370 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 380 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 390 ---> Missclassification: 0.15889830508474578\n",
      "Training Data: 400 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 410 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 420 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 430 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 440 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 450 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 460 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 470 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 480 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 490 ---> Missclassification: 0.10381355932203384\n",
      "Training Data: 500 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 510 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 520 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 36 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 20 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 30 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 40 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 50 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 60 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 70 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 80 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 90 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 100 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 110 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 120 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 130 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 140 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 150 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 160 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 170 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 180 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 190 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 200 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 210 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 220 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 230 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 240 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 250 ---> Missclassification: 0.14618644067796616\n",
      "Training Data: 260 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 270 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 280 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 290 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 300 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 310 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 320 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 330 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 340 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 350 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 360 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 370 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 380 ---> Missclassification: 0.06567796610169496\n",
      "Training Data: 390 ---> Missclassification: 0.06144067796610164\n",
      "Training Data: 400 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 410 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 420 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 430 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 440 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 450 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 460 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 470 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 480 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 490 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 500 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 510 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 520 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 530 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 540 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 750 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 780 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 790 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 800 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 810 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 820 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 830 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 860 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 870 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 880 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 890 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 900 ---> Missclassification: 0.004237288135593209\n",
      "----- Iteration 37 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 20 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 30 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 40 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 50 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 60 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 70 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 80 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 90 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 100 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 110 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 120 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 130 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 140 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 150 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 160 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 170 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 180 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 190 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 200 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 210 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 220 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 230 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 240 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 250 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 260 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 270 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 280 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 290 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 300 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 310 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 320 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 330 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 340 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 350 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 360 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 370 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 380 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 390 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 400 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 410 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 420 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 430 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 440 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 450 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 460 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 470 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 480 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 490 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 500 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 38 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 20 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 30 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 40 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 50 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 60 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 70 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 80 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 90 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 100 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 110 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 120 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 130 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 140 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 150 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 160 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 170 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 180 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 190 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 200 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 210 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 220 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 230 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 240 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 250 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 260 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 270 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 280 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 290 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 300 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 310 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 320 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 330 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 340 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 350 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 360 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 370 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 380 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 390 ---> Missclassification: 0.08686440677966101\n",
      "Training Data: 400 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 410 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 420 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 430 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 440 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 450 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 460 ---> Missclassification: 0.0826271186440678\n",
      "Training Data: 470 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 480 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 490 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 39 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 20 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 30 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 40 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 50 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 60 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 70 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 80 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 90 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 100 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 110 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 120 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 130 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 140 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 150 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 160 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 170 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 180 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 190 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 200 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 210 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 220 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 230 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 240 ---> Missclassification: 0.05508474576271183\n",
      "Training Data: 250 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 260 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 270 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 280 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 290 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 300 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 310 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 320 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 330 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 340 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 350 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 360 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 370 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 380 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 390 ---> Missclassification: 0.03813559322033899\n",
      "Training Data: 400 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 410 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 420 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 430 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 440 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 450 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 460 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 470 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 480 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 490 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 500 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 510 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 550 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 560 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 570 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 580 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 590 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 600 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 610 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 620 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 630 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 640 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 650 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 660 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 670 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 680 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 690 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 700 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 710 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 720 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 730 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 740 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 750 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 760 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 770 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 780 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 790 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 800 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 810 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 820 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 830 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 840 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 850 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 860 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 870 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 880 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 890 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 900 ---> Missclassification: 0.016949152542372836\n",
      "----- Iteration 40 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 20 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 30 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 40 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 50 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 60 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 70 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 80 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 90 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 100 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 110 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 120 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 130 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 140 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 150 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 160 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 170 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 180 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 190 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 200 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 210 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 220 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 230 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 240 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 250 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 260 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 270 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 280 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 290 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 300 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 310 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 320 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 330 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 340 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 350 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 360 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 370 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 380 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 390 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 400 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 410 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 420 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 430 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 440 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 450 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 460 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 470 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 480 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 490 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 500 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 510 ---> Missclassification: 0.048728813559322015\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 41 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 20 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 30 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 40 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 50 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 60 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 70 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 80 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 90 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 100 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 110 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 120 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 130 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 140 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 150 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 160 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 170 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 180 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 190 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 200 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 210 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 220 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 230 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 240 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 250 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 260 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 270 ---> Missclassification: 0.163135593220339\n",
      "Training Data: 280 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 290 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 300 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 310 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 320 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 330 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 340 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 350 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 360 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 370 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 380 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 390 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 400 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 410 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 420 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 430 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 440 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 450 ---> Missclassification: 0.1271186440677966\n",
      "Training Data: 460 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 470 ---> Missclassification: 0.052966101694915224\n",
      "Training Data: 480 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 42 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 20 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 30 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 40 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 50 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 60 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 70 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 80 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 90 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 100 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 110 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 120 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 130 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 140 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 150 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 160 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 170 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 180 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 190 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 200 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 210 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 220 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 230 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 240 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 250 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 260 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 270 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 280 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 290 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 300 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 310 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 320 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 330 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 340 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 350 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 360 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 370 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 380 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 390 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 400 ---> Missclassification: 0.03177966101694918\n",
      "Training Data: 410 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 420 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 430 ---> Missclassification: 0.0402542372881356\n",
      "Training Data: 440 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 450 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 480 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 490 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 550 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 560 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 570 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 580 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 590 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 600 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 610 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 620 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 630 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 640 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 650 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 660 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 670 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 680 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 690 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 700 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 800 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 43 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 20 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 30 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 40 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 50 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 60 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 70 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 80 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 90 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 100 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 110 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 120 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 130 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 140 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 150 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 160 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 170 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 180 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 190 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 200 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 210 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 220 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 230 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 240 ---> Missclassification: 0.08898305084745761\n",
      "Training Data: 250 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 260 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 270 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 280 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 290 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 300 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 310 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 320 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 330 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 340 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 350 ---> Missclassification: 0.09110169491525422\n",
      "Training Data: 360 ---> Missclassification: 0.17372881355932202\n",
      "Training Data: 370 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 380 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 390 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 400 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 410 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 420 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 430 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 440 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 450 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 460 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 470 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 480 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 490 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 520 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 530 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 540 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 550 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 560 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 570 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 580 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 590 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 600 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 610 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 620 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 630 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 640 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 650 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 660 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 670 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 680 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 690 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 700 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 710 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 720 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 730 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 740 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 750 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 760 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 770 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 780 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 790 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 800 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 810 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 820 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 830 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 840 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 850 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 860 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 870 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 880 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 890 ---> Missclassification: 0.0021186440677966045\n",
      "Training Data: 900 ---> Missclassification: 0.0021186440677966045\n",
      "----- Iteration 44 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 20 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 30 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 40 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 50 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 60 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 70 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 80 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 90 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 100 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 110 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 120 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 130 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 140 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 150 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 160 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 170 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 180 ---> Missclassification: 0.15254237288135597\n",
      "Training Data: 190 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 200 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 210 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 220 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 230 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 240 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 250 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 260 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 270 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 280 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 290 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 300 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 310 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 320 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 330 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 340 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 350 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 360 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 370 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 380 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 390 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 400 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 410 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 420 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 430 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 440 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 450 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 460 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 470 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 480 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 490 ---> Missclassification: 0.03601694915254239\n",
      "Training Data: 500 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 45 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 20 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 30 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 40 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 50 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 60 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 70 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 80 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 90 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 100 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 110 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 120 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 130 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 140 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 150 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 160 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 170 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 180 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 190 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 200 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 210 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 220 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 230 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 240 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 250 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 260 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 270 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 280 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 290 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 300 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 310 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 320 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 330 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 340 ---> Missclassification: 0.11440677966101698\n",
      "Training Data: 350 ---> Missclassification: 0.11652542372881358\n",
      "Training Data: 360 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 370 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 380 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 390 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 400 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 410 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 420 ---> Missclassification: 0.06991525423728817\n",
      "Training Data: 430 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 440 ---> Missclassification: 0.07203389830508478\n",
      "Training Data: 450 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 460 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 470 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 480 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 490 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 500 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 510 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 520 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 530 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 540 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 550 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 560 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 570 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 580 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 590 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 600 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 610 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 620 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 630 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 640 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 650 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 660 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 670 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 680 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 690 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 700 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 710 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 720 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 730 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 740 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 750 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 760 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 770 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 780 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 790 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 800 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 810 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 820 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 46 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 20 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 30 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 40 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 50 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 60 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 70 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 80 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 90 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 100 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 110 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 120 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 130 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 140 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 150 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 160 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 170 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 180 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 190 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 200 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 210 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 220 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 230 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 240 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 250 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 260 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 270 ---> Missclassification: 0.05932203389830504\n",
      "Training Data: 280 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 290 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 300 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 310 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 320 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 330 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 340 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 350 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 360 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 370 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 380 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 390 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 400 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 410 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 420 ---> Missclassification: 0.021186440677966156\n",
      "Training Data: 430 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 440 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 450 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 460 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 470 ---> Missclassification: 0.01906779661016944\n",
      "Training Data: 480 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 490 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 500 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 510 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 520 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 550 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 560 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 570 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 580 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 590 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 600 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 610 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 620 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 630 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 640 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 650 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 660 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 670 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 680 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 690 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 700 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 710 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 720 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 730 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 740 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 750 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 760 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 770 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 780 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 790 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 800 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 810 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 820 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 830 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 840 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 850 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 860 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 870 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 880 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 890 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 900 ---> Missclassification: 0.006355932203389814\n",
      "----- Iteration 47 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 20 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 30 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 40 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 50 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 60 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 70 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 80 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 90 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 100 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 110 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 120 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 130 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 140 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 150 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 160 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 170 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 180 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 190 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 200 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 210 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 220 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 230 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 240 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 250 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 260 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 270 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 280 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 290 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 300 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 310 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 320 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 330 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 340 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 350 ---> Missclassification: 0.09533898305084743\n",
      "Training Data: 360 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 370 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 380 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 390 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 400 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 410 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 420 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 430 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 440 ---> Missclassification: 0.11016949152542377\n",
      "Training Data: 450 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 460 ---> Missclassification: 0.07838983050847459\n",
      "Training Data: 470 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 480 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 490 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 500 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 510 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 520 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 530 ---> Missclassification: 0.010593220338983023\n",
      "Training Data: 540 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 550 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 560 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 570 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 580 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 590 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 600 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 610 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 620 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 630 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 640 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 650 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 660 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 670 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 680 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 690 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 700 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 710 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 720 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 730 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 740 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 750 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 760 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 770 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 780 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 790 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 800 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 810 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 820 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 830 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 840 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 850 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 860 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 870 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 880 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 890 ---> Missclassification: 0.012711864406779627\n",
      "Training Data: 900 ---> Missclassification: 0.012711864406779627\n",
      "----- Iteration 48 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 20 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 30 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 40 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 50 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 60 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 70 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 80 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 90 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 100 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 110 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 120 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 130 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 140 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 150 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 160 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 170 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 180 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 190 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 200 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 210 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 220 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 230 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 240 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 250 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 260 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 270 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 280 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 290 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 300 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 310 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 320 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 330 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 340 ---> Missclassification: 0.09322033898305082\n",
      "Training Data: 350 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 360 ---> Missclassification: 0.0805084745762712\n",
      "Training Data: 370 ---> Missclassification: 0.07627118644067798\n",
      "Training Data: 380 ---> Missclassification: 0.07415254237288138\n",
      "Training Data: 390 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 400 ---> Missclassification: 0.06355932203389836\n",
      "Training Data: 410 ---> Missclassification: 0.05720338983050843\n",
      "Training Data: 420 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 430 ---> Missclassification: 0.05084745762711862\n",
      "Training Data: 440 ---> Missclassification: 0.04661016949152541\n",
      "Training Data: 450 ---> Missclassification: 0.03389830508474578\n",
      "Training Data: 460 ---> Missclassification: 0.02754237288135597\n",
      "Training Data: 470 ---> Missclassification: 0.016949152542372836\n",
      "Training Data: 480 ---> Missclassification: 0.029661016949152574\n",
      "Training Data: 490 ---> Missclassification: 0.07627118644067798\n",
      "Training Data: 500 ---> Missclassification: 0.014830508474576232\n",
      "Training Data: 510 ---> Missclassification: 0.044491525423728806\n",
      "Training Data: 520 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 530 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n",
      "----- Iteration 49 running -----\n",
      "Training Data: 10 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 20 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 30 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 40 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 50 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 60 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 70 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 80 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 90 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 100 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 110 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 120 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 130 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 140 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 150 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 160 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 170 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 180 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 190 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 200 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 210 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 220 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 230 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 240 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 250 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 260 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 270 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 280 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 290 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 300 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 310 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 320 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 330 ---> Missclassification: 0.0847457627118644\n",
      "Training Data: 340 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 350 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 360 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 370 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 380 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 390 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 400 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 410 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 420 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 430 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 440 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 450 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 460 ---> Missclassification: 0.025423728813559365\n",
      "Training Data: 470 ---> Missclassification: 0.0423728813559322\n",
      "Training Data: 480 ---> Missclassification: 0.02330508474576276\n",
      "Training Data: 490 ---> Missclassification: 0.06779661016949157\n",
      "Training Data: 500 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 510 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 520 ---> Missclassification: 0.004237288135593209\n",
      "Training Data: 530 ---> Missclassification: 0.006355932203389814\n",
      "Training Data: 540 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 550 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 560 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 570 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 580 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 590 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 600 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 610 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 620 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 630 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 640 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 650 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 660 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 670 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 680 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 690 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 700 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 710 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 720 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 730 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 740 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 750 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 760 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 770 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 780 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 790 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 800 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 810 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 820 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 830 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 840 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 850 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 860 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 870 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 880 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 890 ---> Missclassification: 0.008474576271186418\n",
      "Training Data: 900 ---> Missclassification: 0.008474576271186418\n"
     ]
    }
   ],
   "source": [
    "df_error_2 = pd.DataFrame(np.zeros((90, 50)), columns=[f\"Iteration_{i}\" for i in range(1, 51)])\n",
    "\n",
    "for i in range(50):\n",
    "    print(f'----- Iteration {i} running -----')\n",
    "    test_data = banknote.sample(n=472)\n",
    "    train_data = banknote.drop(test_data.index)\n",
    "\n",
    "    test_data_X = test_data.iloc[:,:-1]\n",
    "    test_data_y = test_data.iloc[:,-1]\n",
    "\n",
    "    select_10_train = train_data.sample(n=10)\n",
    "    train_data = train_data.drop(select_10_train.index)\n",
    "\n",
    "    cc = 0\n",
    "\n",
    "    select_10_train_X = select_10_train.iloc[:,:-1]\n",
    "    select_10_train_y = select_10_train.iloc[:,-1]\n",
    "\n",
    "    lsvc = LinearSVC(penalty='l1', \n",
    "                loss= 'squared_hinge',\n",
    "                dual='auto',\n",
    "                max_iter=10000,\n",
    "                C=1)\n",
    "    \n",
    "    lsvc.fit(select_10_train_X, select_10_train_y)\n",
    "    y_pred = lsvc.predict(test_data_X)\n",
    "    error = 1-accuracy_score(test_data_y, y_pred)\n",
    "    df_error_2.iloc[cc,i] = round(error,6)\n",
    "    print(f'Training Data: {len(select_10_train)} ---> Missclassification: {error}')\n",
    "\n",
    "    while len(train_data)>0:\n",
    "\n",
    "        train_data_X = train_data.iloc[:,:-1]\n",
    "        train_distance = lsvc.decision_function(train_data_X)\n",
    "\n",
    "        if len(train_data)>10:\n",
    "            add_train_index = np.argpartition(train_distance, 10)[:10]\n",
    "            new_10_train = train_data.iloc[add_train_index]\n",
    "        else:\n",
    "            new_10_train = train_data\n",
    "\n",
    "        train_data = train_data.drop(new_10_train.index)\n",
    "        select_10_train = pd.concat([new_10_train,select_10_train])\n",
    "\n",
    "        \n",
    "\n",
    "        select_10_train_X = select_10_train.iloc[:,:-1]\n",
    "        select_10_train_y = select_10_train.iloc[:,-1]\n",
    "\n",
    "        lsvc = LinearSVC(penalty='l1', \n",
    "                    loss= 'squared_hinge',\n",
    "                    dual='auto',\n",
    "                    max_iter=10000,\n",
    "                    C=1)\n",
    "        \n",
    "        lsvc.fit(select_10_train_X, select_10_train_y)\n",
    "        y_pred = lsvc.predict(test_data_X)\n",
    "        error = 1-accuracy_score(test_data_y, y_pred)\n",
    "\n",
    "        cc+=1\n",
    "        df_error_2.iloc[cc,i] = round(error,6)\n",
    "\n",
    "        data_volume = len(select_10_train)\n",
    "        print(f'Training Data: {len(select_10_train)} ---> Missclassification: {error}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration_1</th>\n",
       "      <th>Iteration_2</th>\n",
       "      <th>Iteration_3</th>\n",
       "      <th>Iteration_4</th>\n",
       "      <th>Iteration_5</th>\n",
       "      <th>Iteration_6</th>\n",
       "      <th>Iteration_7</th>\n",
       "      <th>Iteration_8</th>\n",
       "      <th>Iteration_9</th>\n",
       "      <th>Iteration_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Iteration_41</th>\n",
       "      <th>Iteration_42</th>\n",
       "      <th>Iteration_43</th>\n",
       "      <th>Iteration_44</th>\n",
       "      <th>Iteration_45</th>\n",
       "      <th>Iteration_46</th>\n",
       "      <th>Iteration_47</th>\n",
       "      <th>Iteration_48</th>\n",
       "      <th>Iteration_49</th>\n",
       "      <th>Iteration_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.218220</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.163136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.084746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.163136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.084746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.163136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.084746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.163136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.084746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.163136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.084746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iteration_1  Iteration_2  Iteration_3  Iteration_4  Iteration_5  \\\n",
       "0      0.057203     0.158898     0.103814     0.120763     0.088983   \n",
       "1      0.057203     0.158898     0.103814     0.120763     0.088983   \n",
       "2      0.057203     0.156780     0.103814     0.120763     0.088983   \n",
       "3      0.057203     0.158898     0.103814     0.120763     0.088983   \n",
       "4      0.057203     0.158898     0.103814     0.120763     0.088983   \n",
       "..          ...          ...          ...          ...          ...   \n",
       "85     0.012712     0.012712     0.012712     0.008475     0.008475   \n",
       "86     0.012712     0.012712     0.012712     0.008475     0.008475   \n",
       "87     0.012712     0.012712     0.012712     0.008475     0.008475   \n",
       "88     0.012712     0.012712     0.012712     0.008475     0.008475   \n",
       "89     0.012712     0.012712     0.012712     0.008475     0.008475   \n",
       "\n",
       "    Iteration_6  Iteration_7  Iteration_8  Iteration_9  Iteration_10  ...  \\\n",
       "0      0.031780     0.152542     0.188559     0.218220      0.137712  ...   \n",
       "1      0.031780     0.152542     0.188559     0.156780      0.137712  ...   \n",
       "2      0.031780     0.152542     0.188559     0.156780      0.137712  ...   \n",
       "3      0.031780     0.152542     0.188559     0.156780      0.137712  ...   \n",
       "4      0.031780     0.152542     0.188559     0.156780      0.137712  ...   \n",
       "..          ...          ...          ...          ...           ...  ...   \n",
       "85     0.004237     0.014831     0.016949     0.004237      0.012712  ...   \n",
       "86     0.004237     0.014831     0.016949     0.004237      0.012712  ...   \n",
       "87     0.004237     0.014831     0.016949     0.004237      0.012712  ...   \n",
       "88     0.004237     0.014831     0.016949     0.004237      0.012712  ...   \n",
       "89     0.004237     0.014831     0.016949     0.004237      0.012712  ...   \n",
       "\n",
       "    Iteration_41  Iteration_42  Iteration_43  Iteration_44  Iteration_45  \\\n",
       "0       0.080508      0.163136      0.033898      0.091102      0.152542   \n",
       "1       0.080508      0.163136      0.033898      0.091102      0.152542   \n",
       "2       0.080508      0.163136      0.033898      0.091102      0.152542   \n",
       "3       0.080508      0.163136      0.033898      0.091102      0.152542   \n",
       "4       0.080508      0.163136      0.033898      0.088983      0.152542   \n",
       "..           ...           ...           ...           ...           ...   \n",
       "85      0.012712      0.008475      0.008475      0.002119      0.006356   \n",
       "86      0.012712      0.008475      0.008475      0.002119      0.006356   \n",
       "87      0.012712      0.008475      0.010593      0.002119      0.006356   \n",
       "88      0.012712      0.008475      0.008475      0.002119      0.006356   \n",
       "89      0.012712      0.008475      0.008475      0.002119      0.006356   \n",
       "\n",
       "    Iteration_46  Iteration_47  Iteration_48  Iteration_49  Iteration_50  \n",
       "0       0.114407      0.059322      0.095339      0.093220      0.084746  \n",
       "1       0.114407      0.059322      0.095339      0.093220      0.084746  \n",
       "2       0.114407      0.059322      0.095339      0.093220      0.084746  \n",
       "3       0.114407      0.059322      0.095339      0.093220      0.084746  \n",
       "4       0.114407      0.059322      0.095339      0.093220      0.084746  \n",
       "..           ...           ...           ...           ...           ...  \n",
       "85      0.012712      0.006356      0.012712      0.008475      0.008475  \n",
       "86      0.014831      0.006356      0.012712      0.008475      0.008475  \n",
       "87      0.012712      0.006356      0.012712      0.008475      0.008475  \n",
       "88      0.012712      0.006356      0.012712      0.008475      0.008475  \n",
       "89      0.012712      0.006356      0.012712      0.008475      0.008475  \n",
       "\n",
       "[90 rows x 50 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_error_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i\n",
    "and 2(b)ii. By doing so, you are performing a Monte Carlo simulation. Plot\n",
    "average test error versus number of training instances for both active and passive\n",
    "learners on the same figure and report your conclusions. Here, you are actually\n",
    "obtaining a learning curve by Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "passive_mean = df_error.mean(axis=1)\n",
    "active_mean = df_error_2.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAACIkElEQVR4nO3deXhTZdrH8V/SvZaWvaVQCiIKFZRNEBRQZCkiDG4IgiAyLiiiMqOAG6KO4IKi4ri9KjKACK6oWEGUTUCUTRYRxLIIbRERSoXSJef9IyQQmrZpm/Qk6fdzXVxNzvPk5D7tAXL3fhaLYRiGAAAAAAAVYjU7AAAAAAAIBiRXAAAAAOAFJFcAAAAA4AUkVwAAAADgBSRXAAAAAOAFJFcAAAAA4AUkVwAAAADgBSRXAAAAAOAFJFcAAAAA4AUkVwAABIHHHntMFovF7DAAoEojuQKAKmr69OmyWCyyWCxasWJFkXbDMJSUlCSLxaKrrrrKhAjLprCwUO+8844uu+wy1axZUxEREWrUqJGGDx+uH3/80ezwAABVAMkVAFRxkZGRmj17dpHjS5cu1e+//66IiAgToiqb48eP66qrrtItt9wiwzD04IMP6tVXX9XQoUO1atUqtW/fXr///rvZYfrUww8/rOPHj5sdBgBUaaFmBwAAMNeVV16pefPm6aWXXlJo6Kn/FmbPnq22bdvq4MGDJkbnmfvvv19paWl64YUXdO+997q0TZgwQS+88II5gVWCv//+W2eddZZCQ0Ndfn4AgMpH5QoAqrhBgwbpzz//1KJFi5zH8vLy9MEHH+jGG290+xqbzaapU6fq/PPPV2RkpOLj43X77bfrr7/+cun36aefqk+fPkpMTFRERISaNGmiJ554QoWFhS79LrvsMrVo0UJbt27V5ZdfrujoaNWvX1/PPPNMqfH//vvvev3119WjR48iiZUkhYSE6N///rcaNGjgPLZ+/Xr17t1bsbGxiomJ0RVXXKHVq1e7vM4xbHLFihUaPXq06tSpo+rVq+v2229XXl6eDh8+rKFDh6pGjRqqUaOGHnjgARmG4Xz9rl27ZLFY9Nxzz+mFF15QcnKyoqKi1LVrV23evNnlvX766SfdfPPNOvvssxUZGamEhATdcsst+vPPP136OeZVbd26VTfeeKNq1KihSy+91KXtdIsWLdKll16q6tWrKyYmRuedd54efPBBlz4HDhzQiBEjFB8fr8jISF144YV69913Xfqcfi1vvPGGmjRpooiICF100UX64YcfSvkJAUDVwa+4AKCKa9SokTp27Kj33ntPvXv3liR9+eWXOnLkiAYOHKiXXnqpyGtuv/12TZ8+XcOHD9fo0aOVnp6uadOmaf369fruu+8UFhYmyZ6gxMTEaMyYMYqJidE333yjRx99VNnZ2Xr22WddzvnXX38pNTVV11xzjQYMGKAPPvhAY8eOVcuWLZ1xufPll1+qoKBAN910k0fXu2XLFnXu3FmxsbF64IEHFBYWptdff12XXXaZli5dqg4dOrj0v/vuu5WQkKCJEydq9erVeuONN1S9enWtXLlSDRs21FNPPaUFCxbo2WefVYsWLTR06FCX18+YMUNHjx7VXXfdpdzcXL344ovq1q2bNm3apPj4eEn2JOi3337T8OHDlZCQoC1btuiNN97Qli1btHr16iJJ0/XXX6+mTZvqqaeecknozrzOq666ShdccIEef/xxRURE6Ndff9V3333n7HP8+HFddtll+vXXXzVq1Cg1btxY8+bN080336zDhw/rnnvucTnn7NmzdfToUd1+++2yWCx65plndM011+i3335z/swBoEozAABV0jvvvGNIMn744Qdj2rRpRrVq1Yxjx44ZhmEY119/vXH55ZcbhmEYycnJRp8+fZyvW758uSHJmDVrlsv50tLSihx3nO90t99+uxEdHW3k5uY6j3Xt2tWQZMyYMcN57MSJE0ZCQoJx7bXXlngd9913nyHJWL9+vUfX3b9/fyM8PNzYuXOn89j+/fuNatWqGV26dHEec3x/evXqZdhsNufxjh07GhaLxbjjjjucxwoKCowGDRoYXbt2dR5LT083JBlRUVHG77//7jz+/fffG5KM++67z3nM3ffpvffeMyQZy5Ytcx6bMGGCIckYNGhQkf6ONocXXnjBkGT88ccfxX4vpk6dakgyZs6c6TyWl5dndOzY0YiJiTGys7NdrqVWrVrGoUOHnH0//fRTQ5Lx2WefFfseAFCVMCwQAKABAwbo+PHj+vzzz3X06FF9/vnnxQ4JnDdvnuLi4tSjRw8dPHjQ+adt27aKiYnRt99+6+wbFRXlfHz06FEdPHhQnTt31rFjx7Rt2zaX88bExGjIkCHO5+Hh4Wrfvr1+++23EmPPzs6WJFWrVq3U6ywsLNTChQvVv39/nX322c7j9erV04033qgVK1Y4z+cwYsQIl8pRhw4dZBiGRowY4TwWEhKidu3auY21f//+ql+/vvN5+/bt1aFDBy1YsMB57PTvU25urg4ePKiLL75YkrRu3boi57zjjjtKvdbq1atLsg/NtNlsbvssWLBACQkJGjRokPNYWFiYRo8erZycHC1dutSl/w033KAaNWo4n3fu3FmSSv0ZAUBVQXIFAFCdOnXUvXt3zZ49Wx999JEKCwt13XXXue27Y8cOHTlyRHXr1lWdOnVc/uTk5OjAgQPOvlu2bNHVV1+tuLg4xcbGqk6dOs4E6siRIy7nbdCgQZHhbzVq1Cgyj+tMsbGxkuzJW2n++OMPHTt2TOedd16RtubNm8tms2nv3r0uxxs2bOjyPC4uTpKUlJRU5Li7WJs2bVrk2Lnnnqtdu3Y5nx86dEj33HOP4uPjFRUVpTp16qhx48aSin6fJDnbSnLDDTfokksu0T//+U/Fx8dr4MCBmjt3rkuitXv3bjVt2lRWq+vHgebNmzvbT3fm98KRaJX2MwKAqoI5VwAASdKNN96oW2+9VZmZmerdu7ez8nEmm82munXratasWW7b69SpI0k6fPiwunbtqtjYWD3++ONq0qSJIiMjtW7dOo0dO7ZINSUkJMTt+Yxi5hQ5NGvWTJK0adMmtWrVqsS+5VFcXO6OlxZrcQYMGKCVK1fq/vvvV6tWrRQTEyObzabU1FS3VafTK13FiYqK0rJly/Ttt9/qiy++UFpamt5//31169ZNCxcuLPa6SlLenxEAVBUkVwAASdLVV1+t22+/XatXr9b7779fbL8mTZro66+/1iWXXFLih/wlS5bozz//1EcffaQuXbo4j6enp3s17t69eyskJEQzZ84sdVGLOnXqKDo6Wr/88kuRtm3btslqtRapSFXUjh07ihzbvn27GjVqJMle9Vm8eLEmTpyoRx99tMTXlZXVatUVV1yhK664Qs8//7yeeuopPfTQQ/r222/VvXt3JScn66effpLNZnOpXjmGbCYnJ1c4BgCoShgWCACQZJ/z9Oqrr+qxxx5T3759i+03YMAAFRYW6oknnijSVlBQoMOHD0s6VeU4vaqRl5en//73v16NOykpSbfeeqsWLlyol19+uUi7zWbTlClT9PvvvyskJEQ9e/bUp59+6jIsLysrS7Nnz9all17qHGboLZ988on27dvnfL5mzRp9//33zhUQ3X2fJGnq1KkVet9Dhw4VOeao7J04cUKSfY+zzMxMl2S6oKBAL7/8smJiYtS1a9cKxQAAVQ2VKwCA07Bhw0rt07VrV91+++2aNGmSNmzYoJ49eyosLEw7duzQvHnz9OKLL+q6665Tp06dVKNGDQ0bNkyjR4+WxWLR//73P58MIZsyZYp27typ0aNH66OPPtJVV12lGjVqaM+ePZo3b562bdumgQMHSpKefPJJ5/5Pd955p0JDQ/X666/rxIkTHu2rVVbnnHOOLr30Uo0cOVInTpzQ1KlTVatWLT3wwAOS7HPGunTpomeeeUb5+fmqX7++Fi5cWOEK3+OPP65ly5apT58+Sk5O1oEDB/Tf//5XDRo0cO6Nddttt+n111/XzTffrLVr16pRo0b64IMP9N1332nq1KkeLRICADiF5AoAUGavvfaa2rZtq9dff10PPvigQkND1ahRIw0ZMkSXXHKJJKlWrVr6/PPP9a9//UsPP/ywatSooSFDhuiKK65Qr169vBpPdHS0vvzyS02fPl3vvvuunnjiCR07dkyJiYnq1q2bZs2a5Vyx7/zzz9fy5cs1fvx4TZo0STabTR06dNDMmTOL7HHlDUOHDpXVatXUqVN14MABtW/fXtOmTVO9evWcfWbPnq27775br7zyigzDUM+ePfXll18qMTGx3O/br18/7dq1S2+//bYOHjyo2rVrq2vXrpo4caJzUY6oqCgtWbJE48aN07vvvqvs7Gydd955euedd3TzzTdX9NIBoMqxGMxCBQDA63bt2qXGjRvr2Wef1b///W+zwwEAVALmXAEAAACAF5BcAQAAAIAXkFwBAAAAgBcw5woAAAAAvIDKFQAAAAB4AckVAAAAAHgB+1y5YbPZtH//flWrVk0Wi8XscAAAAACYxDAMHT16VImJibJaS65NkVy5sX//fiUlJZkdBgAAAAA/sXfvXjVo0KDEPiRXblSrVk2S/RsYGxvr0/fKz8/XwoUL1bNnT4WFhfn0vYDTce/BDNx3MAP3HczCvRccsrOzlZSU5MwRSkJy5YZjKGBsbGylJFfR0dGKjY3lLx0qFfcezMB9BzNw38Es3HvBxZPpQixoAQAAAABeQHIFAAAAAF5AcgUAAAAAXsCcKwAAAASVwsJC5efnmx2G8vPzFRoaqtzcXBUWFpodDooREhKi0NBQr2zBRHIFAACAoJGTk6Pff/9dhmGYHYoMw1BCQoL27t3L3ql+Ljo6WvXq1VN4eHiFzkNyBQAAgKBQWFio33//XdHR0apTp47pCY3NZlNOTo5iYmJK3XwW5jAMQ3l5efrjjz+Unp6upk2bVuhnRXIFAACAoJCfny/DMFSnTh1FRUWZHY5sNpvy8vIUGRlJcuXHoqKiFBYWpt27dzt/XuXFTxkAAABBxeyKFQKPt5JfkisAAAAA8AKSKwAAAADwApIrAAAA4DSFNkOrdv6pTzfs06qdf6rQZv7Kg76wa9cuWSwWbdiwwexQggbJFQAAAHBS2uYMXfr0Nxr05mrdM2eDBr25Wpc+/Y3SNmf47D1vvvlmWSwWWSwWhYeH65xzztHjjz+ugoICn72nJCUlJSkjI0MtWrTw6fs4rm316tUux0+cOKFatWrJYrFoyZIlPo2hspBcAQAAALInViNnrlPGkVyX45lHcjVy5jqfJlipqanKyMjQjh079K9//UuPPfaYnn32WZ+9n2TfPDchIUGhob5fQDwpKUnvvPOOy7GPP/5YMTExPn/vykRy5c9shVL6cmnTB/avNnb2BgAA8JRhGDqWV+DRn6O5+Zowf4vcDQB0HHts/lYdzc336Hxl3cQ4IiJCCQkJSk5O1siRI9W9e3fNnz9fkvT888+rZcuWOuuss5SUlKQ777xTOTk5ztfu3r1bffv2VY0aNXTWWWfp/PPP14IFCyRJf/31lwYPHuxcnr5p06bOJOf0YYE2m00NGjTQq6++6hLX+vXrZbVatXv3bknS4cOH9c9//lN16tRRbGysunXrpo0bN5Z6fcOGDdOcOXN0/Phx57G3335bw4YNK9J37969GjBggKpXr66aNWvqH//4h3bt2uVs/+GHH9SjRw/Vrl1bcXFx6tq1q9atW+dyDovFov/7v//T1VdfrejoaDVt2tT5/fQl9rnyV1vnS2ljpez9p47FJkqpT0sp/cyLCwAAIEAczy9UyqNfeeVchqTM7Fy1fGyhR/23Pt5LkaHlr2NERUXpzz//lGRfJvyll15S48aN9dtvv+nOO+/UAw88oP/+97+SpLvuukt5eXlatmyZzjrrLG3dutVZEXrkkUe0detWffnll6pdu7Z+/fVXlwTHwWq1atCgQZo9e7ZGjhzpPD5r1ixdcsklSk5OliRdf/31ioqK0pdffqm4uDi9/vrruuKKK7R9+3bVrFmz2Otp27atGjVqpA8//FBDhgzRnj17tGzZMr3yyit64oknnP3y8/PVq1cvdezYUcuXL1doaKiefPJJpaam6qefflJ4eLiOHj2qYcOG6eWXX5ZhGJoyZYquvPJK7dixQ9WqVXOea+LEiXrmmWf07LPP6uWXX9bgwYO1e/fuEuOsKCpX/mjrfGnuUNfESpKyM+zHt/o+6wYAAEDlMwxDX3/9tb766it169ZNknTvvffq8ssvV6NGjdStWzc9+eSTmjt3rvM1e/bs0SWXXKKWLVvq7LPP1lVXXaUuXbo421q3bq127dqpUaNG6t69u/r27ev2vQcPHqzvvvtOe/bskWTfBHnOnDkaPHiwJGnFihVas2aN5s2bp3bt2qlp06Z67rnnVL16dX3wwQelXtstt9yit99+W5I0ffp0XXnllapTp45Ln/fff182m03/93//p5YtW6p58+Z65513tGfPHue8rG7dumnIkCFq1qyZmjdvrjfeeEPHjh3T0qVLXc518803a9CgQTrnnHP01FNPKScnR2vWrCk1zoqgcuVvbIX2ilWxRWmLlDZOatZHsoZUcnAAAACBIyosRFsf7+VR3zXph3TzOz+U2m/68IvUvnHplY+osJAyDQ38/PPPFRMTo/z8fNlsNt1444167LHHJElff/21Jk2apG3btik7O1sFBQXKzc3VsWPHFB0drdGjR2vkyJFauHChunfvrmuvvVYXXHCBJGnkyJG69tprtW7dOvXs2VP9+/dXp06d3MbQqlUrNW/eXLNnz9a4ceO0dOlSHThwQNdff70kaePGjcrJyVGtWrVcXnf8+HHt3Lmz1GscMmSIxo0bp99++03Tp0/XSy+9VKTPxo0b9euvv7pUoCQpNzfX+R5ZWVl6+OGHtWTJEh04cECFhYU6duyYMyl0cHwPJOmss85SbGysDhw4UGqcFUFy5W92ryxasXJhSNn77P0ad660sAAAAAKNxWJRdLhnH3c7N62jenGRyjyS6/ZX3BZJCXGR6ty0jkKsFo/OWZbk6vLLL9err76q8PBwJSYmOheZ2LVrl6666iqNHDlS//nPf1SzZk2tWLFCI0aMUF5enqKjo/XPf/5TvXr10hdffKGFCxdq0qRJmjJliu6++2717t1bu3fv1oIFC7Ro0SJdccUVuuuuu/Tcc8+5jWPw4MHO5Gr27NlKTU11JlM5OTmqV6+e25X9qlevXuo11qpVS1dddZVGjBih3Nxc9e7dW0ePHnXpk5OTo7Zt22rWrFlFXu+ocg0bNkx//vmnXnzxRSUnJysiIkIdO3ZUXl6eS/+wsDCX5xaLRTabrdQ4K4Jhgf4mJ8u7/QAAAFCqEKtFE/qmSLInUqdzPJ/QN8XjxKqszjrrLJ1zzjlq2LChy+p9a9eulc1m05QpU3TxxRfr3HPP1f79RX8Rn5SUpDvuuEMfffSR/vWvf+nNN990ttWpU0fDhg3TzJkzNXXqVL3xxhvFxnHjjTdq8+bNWrt2rT744APnkEBJatOmjTIzMxUaGqpzzjnH5U/t2rU9us5bbrlFS5Ys0dChQxUSUnQUVps2bbRjxw7VrVu3yHvExcVJkr777juNHj1aV155pc4//3xFRETo4MGDHr2/r5Fc+ZuYeO/2AwAAgEdSW9TTq0PaKCEu0uV4QlykXh3SRqkt6lV6TOecc47y8/P18ssv67ffftP//vc/vfbaay597r33Xn311VdKT0/XunXr9O2336p58+aSpEcffVSffvqpfv31V23ZskWff/65s82dRo0aqVOnThoxYoQKCwvVr9+phdS6d++ujh07qn///lq4cKF27dqllStX6qGHHtKPP/7o0fWkpqbqjz/+0OOPP+62ffDgwapdu7b+8Y9/aPny5UpPT9eSJUs0evRo/f7775Kkpk2b6n//+59+/vlnff/99xo8eLCioqI8en9fI7nyN8md7KsCFvmdiYNFiq1v7wcAAACvSm1RTyvGdtN7t16sFwe20nu3XqwVY7uZklhJ0oUXXqjnn39eTz/9tFq0aKFZs2Zp0qRJLn0KCwt11113qXnz5kpNTdW5557rXEkwPDxc48eP1wUXXKAuXbooJCREc+bMKfE9Bw8erI0bN+rqq692SVosFosWLFigLl26aPjw4Tr33HM1cOBA7d69W/Hxnv3i32KxqHbt2goPD3fbHh0drWXLlqlhw4a65ppr1Lx5c+cwwtjYWEnSW2+9pb/++ktt2rTRTTfdpNGjR6tu3boevb+vWYyyLsJfBWRnZysuLk5Hjhxx/hB9JT8/XwsWLNCVV155alyoY7VASUUXtrBIA2awHDsqzO29B/gY9x3MwH1XdeTm5io9PV2NGzdWZGRk6S/wMZvNpuzsbMXGxspqpabhz0q6d8qSG/BT9kcp/ewJVOwZvyEJCSexAgAAAPwUqwX6q5R+9uXWd6+UDmyVvnxAKsyXki8xOzIAAAAAblC58mfWEPty6x1ul+JbSjKkXxeZHRUAAAAAN0iuAsV5qfav29PMjQMAAACAWyRXgeLck8nVr4ulgryS+wIAAACodCRXgSKxjXRWHelEtrRnldnRAAAAADgDyVWgsFqlpr3sj7d/ZW4sAAAAAIoguQok5zqSqy8lticDAAAA/ArJVSBpcrl9r6tDv0l//mp2NAAAAABOQ3IVSCKqSY0utT/+5UtzYwEAAAhWtkIpfbm06QP7V1uh2RGVy/Tp01W9enWzw6hSSK4CjWPVQOZdAQAAeN/W+dLUFtK7V0kfjrB/ndrCftzHVq1apZCQEPXp06fMr23UqJGmTp3qcuyGG27Q9u3bvRSde0uWLJHFYlGNGjWUm5vr0vbDDz/IYrHIYrH4NAZ/QnIVaBzzrvasko7/ZW4sAAAAwWTrfGnuUCl7v+vx7Az7cR8nWG+99ZbuvvtuLVu2TPv37y/9BaWIiopS3bp1vRBZ6apVq6aPP/7Y5dhbb72lhg0bVsr7+wuSq0BTo5FUp7lkFNr3vAIAAIB7hiHl/e3Zn9xs6csHJLlbNOzksbSx9n6enK+Mi4/l5OTo/fff18iRI9WnTx9Nnz69SJ/PPvtMF110kSIjI1W7dm1dffXVkqTLLrtMu3fv1n333edSKTp9WOD27dtlsVi0bds2l3O+8MILatKkifP55s2b1bt3b8XExCg+Pl433XSTDh48WGr8w4YN09tvv+18fvz4cc2ZM0fDhg0r0nfFihXq3LmzoqKilJSUpNGjR+vvv/92tv/vf/9Tu3btVK1aNSUkJOjGG2/UgQMHnO2OatnixYvVrl07RUdHq1OnTvrll19KjdPXSK4CkXPVwDRz4wAAAPBn+cekpxI9+zM5STqaUcLJDHtFa3KSZ+fLP1amUOfOnatmzZrpvPPO05AhQ/T222/LOC1B++KLL3T11Vfryiuv1Pr167V48WK1b99ekvTRRx+pQYMGevzxx5WRkaGMjKLXce6556pdu3aaNWuWy/FZs2bpxhtvlCQdPnxY3bp1U+vWrfXjjz8qLS1NWVlZGjBgQKnx33TTTVq+fLn27NkjSfrwww/VqFEjtWnTxqXfzp07lZqaqmuvvVY//fST3n//fa1YsUKjRo1y9snPz9cTTzyhjRs36pNPPtGuXbt08803F3nPhx56SFOmTNGPP/6o0NBQ3XLLLaXG6WuhZgeAcjg3VfpuqrRjkVRYIIXwYwQAAAhkb731loYMGSJJSk1N1ZEjR7R06VJddtllkqT//Oc/GjhwoCZOnOh8zYUXXihJqlmzpkJCQpyVnuIMHjxY06ZN0xNPPCHJXs1au3atZs6cKUmaNm2aWrduraeeesr5mrfffltJSUnavn27zj333GLPXbduXfXu3VvTp0/Xo48+qrffftttsjNp0iQNHjxY9957rySpadOmeumll9S1a1e9+uqrioyMdHnd2WefrZdeekkXXXSRcnJyFBMT42z7z3/+o65du0qSxo0bpz59+ig3N1eRkZHFxulrfCoPREntpaga9jlXe7+XGl1idkQAAAD+JyxaetDDuUu7V0qzriu93+APpOROnr23h0MDf/nlF61Zs8Y5Zyk0NFQ33HCD3nrrLWdytWHDBt16660ena84AwcO1L///W+tXr1aF198sWbNmqU2bdqoWbNmkqSNGzfq22+/dUlgHHbu3FliciVJt9xyi+655x4NGTJEq1at0rx587R8+XKXPhs3btRPP/3kUkEzDEM2m03p6elq3ry51q5dq8cee0wbN27UX3/9JZvNJknas2ePUlJSnK+74IILnI/r1asnSTpw4ICp87xIrgKRNURq2lP66X370ECSKwAAgKIsFin8LM/6NukmxSbaF69wO+/KYm9v0s3+WcwTHiZXb731lgoKCpSYmHjaSw1FRERo2rRpiouLU1RUlGfvWYKEhAR169ZNs2fP1sUXX6zZs2dr5MiRzvacnBz17dtXTz/9dJHXOpKXkvTu3Vu33XabRowYob59+6pWrVpF+uTk5Oj222/X6NGji7Q1bNhQf//9t3r16qVevXpp1qxZqlOnjvbs2aNevXopLy/PpX9YWJjzsWOemSMRMwvJVaA6t5c9udr8sVTvQikm3v5blNP/stsK7b+FyckqXzsAAEBVYQ2RUp+2rwooi1wTrJNLiadO9vpnpYKCAs2YMUNTpkxRz549Xdr69++v9957T3fccYcuuOACLV68WMOHD3d7nvDwcBUWlr4f1+DBg/XAAw9o0KBB+u233zRw4EBnW5s2bZxzpUJDy54mhIaGaujQoXrmmWf05Zfu92Rt06aNtm7dqnPOOcdt+6ZNm/Tnn39q8uTJSkpKkiT9+OOPZY7FLCxoEagKTmbu2Xvd78FQ2h4NJu7hAAAA4JdS+kkDZkixZ1RpYhPtx1P6ef0tP//8c/31118aMWKEWrRo4fLn2muv1VtvvSVJmjBhgt577z1NmDBBP//8szZt2uRSYWrUqJGWLVumffv2lbi63zXXXKOjR49q5MiRuvzyy12qZXfddZcOHTqkQYMG6YcfftDOnTv11Vdfafjw4R4lbpL0xBNP6I8//lCvXr3cto8dO1YrV67UqFGjtGHDBu3YsUOffvqpc0GLhg0bKjw8XC+//LJ+++03zZ8/3zlHLBCQXAWirfOlT0YWPe7Yg2HhIyXv0VBaOwkWAACoqlL6SfduloZ9Ll37lv3rvZt8klhJ9iGB3bt3V1xcXJG2a6+9Vj/++KN++uknXXbZZZo3b57mz5+vVq1aqVu3blqzZo2z7+OPP65du3apSZMmqlOnTrHvV61aNfXt21cbN27U4MGDXdoSExP13XffqbCwUD179lTLli117733qnr16rJaPUsbwsPDVbt27WI3Dr7gggu0dOlSbd++XZ07d1br1q316KOPOpO8OnXqaPr06Zo3b55SUlI0efJkPffccx69tz+wGEYZF+GvArKzsxUXF6cjR44oNjbWp++Vn5+vBQsW6Morr3QZN1osW6G9wnRmYnQ6i1UyShhvWmL7yfHE925iiGCQK/O9B3gB9x3MwH1XdeTm5io9PV2NGzc2dcU4B5vNpuzsbMXGxnqcnMAcJd07ZckNmHMVaHavLDmxkkpOrEptN6Tsffb3Se5UsTlbvm4HAAAA/AjJVaDJyaqc9/llgfTxba6JXGyifaJnSj/70MG0sea1SyRfAAAA8CskV4EmJr5y3mf1f4sec8zJ6nS3tPJlFVmmtLLaB8ywPy8t+QIAAAAqEYM/A01yJ3sSIfeTBCXZ51RVpL1Yhv3Pqmlyv/9DZbRL+uweFuQAAACA3yG5CjSOPRgkFU2QLPY/HUdVoN0DFZrTVdF2Qzp+SCUmX2nj7EMGAQBAlcR6bSgrb90zJFeBqLQ9GHo+Uf72i+/0bew+d9qCHAAAoEoJCbHPvc7LyzM5EgSaY8eOSVKFVxRlzlWgSuknNetT/IIO5W3fvdL9fKtAU1kLfwAAAL8RGhqq6Oho/fHHHwoLCzN9+XObzaa8vDzl5uaaHgvcMwxDx44d04EDB1S9enVngl5eJFeBzBoiNe7s3XbHnK7sDLkfeqeT+2QZ5rV7orIW/gAAAH7DYrGoXr16Sk9P1+7du80OR4Zh6Pjx44qKiip2U134h+rVqyshIaHC5yG5givHnK65Q2Wfg3V6gnPyH4WOo06u5mdGuyFF1ZSO/6Vik6/Y+vYkEQAAVDnh4eFq2rSpXwwNzM/P17Jly9SlSxc2sPZjYWFhFa5YOZBcoSjHnC63S51Ptrc3uMi8dqmY5O+klgPY7woAgCrMarUqMjLS7DAUEhKigoICRUZGklxVESRXcM9Xc7q81e4u+QuLlvKPSd+/Jp3XWyrMK36DYTYgBgAAgJeRXKF4vpjT5a12d8lXg4uk94dIvy6S3kl1XdL99A2Gt85nA2IAAAB4HckVApe75OuCAfbk6sy9shwbDHe6++R8LsN9+4AZJFgAAAAoF9aERPCwFUpfTyim8eTqg+4SK2e72IAYAAAA5UZyheCxe6XrUD+3SlrenQ2IAQAAUH6mJ1evvPKKGjVqpMjISHXo0EFr1qwptu+WLVt07bXXqlGjRrJYLJo6dWqFz4kg4q2Ng9mAGAAAAOVganL1/vvva8yYMZowYYLWrVunCy+8UL169dKBAwfc9j927JjOPvtsTZ48udhNvsp6TgQRb20c7Ml5bIVS+nJp0wf2rwwlBAAAqPJMTa6ef/553XrrrRo+fLhSUlL02muvKTo6Wm+//bbb/hdddJGeffZZDRw4UBEREV45J4JIcif7qn8qYQd0i7XkdlkkS0jJydPW+dLUFtK7V0kfjrB/ndrCftyB5AsAAKDKMW21wLy8PK1du1bjx493HrNarerevbtWrVpVqec8ceKETpw44XyenZ0tyb6rdn5+frli8ZTj/L5+n6rC0uMphXw4XJJFltPmVxknEypbhztlXf1KMe2GLDJkzOgnRcTIcvyvU+3VElXY8ylJOnl+wyVFM06uNlh47Tv2PgsflOXo/iKvN5pdZT9gK5Rl7yrnMvJGUsci+3BVqN0D3HswA/cdzMB9B7Nw7wWHsvz8TEuuDh48qMLCQsXHuw7Bio+P17Zt2yr1nJMmTdLEiROLHF+4cKGio6PLFUtZLVq0qMgxmyHtzLYoO1+KDZOaxBqyllR0gSSr6jUepZa/z1JU/iHn0eNhNbS5wWBlnLhI9Rpb3LZvTbxBTQ6kqcbxdBmnJVaSpKP7FfLhzcoLiVHIGYmVJHtSJqnwk1EKL8wpGtbJ1//Q+G5JcvP+NbWpwWBlVL9I9Q7/UKH2snJ37wG+xn0HM3DfwSzce4Ht2LFjHvdlnytJ48eP15gxY5zPs7OzlZSUpJ49eyo2Ntan752fn69FixapR48eCgsLcx7/akuWJi3YpszsUxW1hNgIPXxlM/U630tzi4LWlZLtYRWcVtkJS+qo1tYQtS6h/UJJodM+kaGiAwftdS0pwl3idFqfiMKcEl5v0UWZs6Tjf+nMVQsj8//SRenTZLv4LlnTXyl3e+G175yqjpWiuHsP8CXuO5iB+w5m4d4LDo5RbZ4wLbmqXbu2QkJClJXlujJbVlZWsYtV+OqcERERbudwhYWFVdpfhNPfK21zhu6es7HIouFZ2Sd095yNenVIG6W2qFcpcQWuMOmcy8vWnr5cOppR7Cs8LRoW188iQzp+qPg2SSHf/1fulov3rN2i0EUPSef3K9MQwcq8zwEH7juYgfsOZuHeC2xl+dmZtqBFeHi42rZtq8WLFzuP2Ww2LV68WB07dvSbc1a2QpuhiZ9tLWmbW038bKsKbSXt14Ry8Ycl2A1bBdrZpwsAAMBMpg4LHDNmjIYNG6Z27dqpffv2mjp1qv7++28NHz5ckjR06FDVr19fkyZNkmRfsGLr1q3Ox/v27dOGDRsUExOjc845x6Nz+rs16YeUcSS32HZDUsaRXK1JP6SOTWpVXmBVgbeWcjebPySJAAAAVZCpydUNN9ygP/74Q48++qgyMzPVqlUrpaWlORek2LNnj6zWU8W1/fv3q3Xr1s7nzz33nJ577jl17dpVS5Ys8eic/u7A0eITq/L0Qxk4lnLPzpC7oXeSRYqqcXLOlM7oc3JWVlRNt3OqKlWwJIkAAAABxvQFLUaNGqVRo0a5bXMkTA6NGjWSYZT+obWkc/q7utUivdoPZWANkVKfluYO1aklLBxOzqTq+6L9a9pYKfvUUuuKTZRSJ9sfF/t6D5Ivi1UyjHK2W+xxJHcq4SIBAADgK6YnV3DVvnFN1YuLVOaR3OI+PishLlLtG9es7NCqhpR+0oAZxSdPKf3sz5v1sc9tOrnaoJI7nVpEoqTXSyUnbx1HSStfLkf7SamTy7zfFQAAALyD5MrPhFgtmtA3RSNnrivu47Um9E1RCBte+U5Kv5KTJ8n+uHHn8r2+tOStwUVlb5dF6v/qqeQPAAAAlY7kyg+ltqinV4e00cTPtrosbpEQF6kJfVNYhr0ylJQ8VfT1pSVfZWk/mil986R0eJd05PfyxwsAAIAKI7nyU6kt6qlHSoJumb5GS7cf1MCLkvSfq1tSsQoWpSVvZWm3WKQPR0ir/yt1vFMKP8u7sQIAAMAjpu1zhdKFWC1qWNP+QblubCSJFdxL6S/VaGTfoHjdDLOjAQAAqLJIrvxcRKj9R3SioNDkSOC3QkKlS+6xP175slSQZ248AAAAVRTJlZ+LDLPPszmRbzM5Evi1C2+UYhKk7H3ST++bHQ0AAECVRHLl56hcwSNhkVLHu+yPv5sq2bhfAAAAKhvJlZ9zVK5yqVyhNO2GS5HVpT9/lX6eb3Y0AAAAVQ7JlZ+LCKNyBQ9FVJM63G5/vGyKlL5M2vSBlL6cShYAAEAlYCl2PxcZSuUKZdDhDmnFC1LWJundvqeOxyZKqU+zyTAAAIAPUbnyc1SuUCa7VkiFblYLzM6Q5g6Vtp4cLmgrlGX3CtU/tEqW3SuobAEAAHgBlSs/F0HlCp6yFUppY4tpNCRZpLRxkmGTvhqv0Oz9aidJu1+lsgUAAOAFVK78HJUreGz3Sil7fwkdDPtS7fOGFe3nprKl9OXM2QIAACgDKld+jjlX8FhOVgVeXLSy5ZKAnV7ZshXaE7mcLCkmXkruJFlDKho9AABAwCO58nNUruCxmPgKnuC0ytaZHJWtTndLmz8oPvECAACowhgW6OccmwhTuUKpkjvZEx1ZfHByw/5n5UulDykEAACookiu/JxjE+ET+VSuUApriL2CJKloguWLhMvBsH9JG8fcLAAAUKWRXPk5Z+WqgMoVPJDSTxowQ4qt53o8NlG67l3fVray99nnYgEAAFRRzLnyc47KVV6BTYZhyGLxZQUCQSGln9Ssj/tFJ6xW+xA+WeSsOElunpdThRbVAAAACGxUrvyco3IlSSeoXsFT1hCpcWep5XX2r47V/Hxd2arwohoAAACBi8qVn3NUriTpRL7N5TlQLicrWwW/LdOG5V+pVedeCj27SymVLQ+ERUlJHXwRMQAAQECgcuXnQq0WWU8WEnJZjh3eYg2RkXyp9tXsKCP5Ug8qW/WlTqNlT7qKqWzlH5e+uE8qyGcDYgAAUCVRufJzFotFkWEhOpZXqBMsx47KUNKcrQYXSWljz9jnqr50/tXS6v9K62dKWz6W8v4+rZ19sAAAQNVAchUAIkKtOpZXSOUKlccxZ+tMJSVehfnSmtddEyvp1D5YA2aQYAEAgKBGchUA7POs8qlcwT+4S7xshdK2z4p5gSHJYt8Hq1mfU0MQAQAAggxzrgLAqb2uqFzBT+1e6TpUsAj2wQIAAMGP5CoAOFYIpHIFv+Xp/lbsgwUAAIIYyVUAcFau8qlcwU95ur8V+2ABAIAgRnIVACIclSs2EYa/Su5UygbEFvuqgsmdKjMqAACASkVyFQCoXMHvWUPsy61Lcp9gGVLqZBazAAAAQY3kKgBEUrlCIChuA2JJCgmXktpXfkwAAACViKXYAwCVKwSMIvtg1ZW+nijt+1Fa+ox01fNmRwgAAOAzVK4CAJUrBBTHPlgtr5Mad5F6TLQfX/eu9OdOc2MDAADwIZKrAEDlCgGt0aVS056SrUD65kmzowEAAPAZkqsAQOUKAe+KCZIs0paPpP3rzY4GAADAJ0iuAgCVKwS8hBbSBQPsj79+zNRQAAAAfIXkKgBQuUJQuPxByRom/bZE+u4ladMHUvpyycYvDQAAQHBgtcAA4KhcnSjgQygCWI1GUpNu0o6vpEWPnDoem2jfIyuln2mhAQAAeAOVqwDgrFzlU7lCANs6X9qxsOjx7Axp7lB7u2SvZKUvp7IFAAACDpWrAEDlCgHPViiljZVkuGk0JFmktHGSYZO+Gi9l7z/VTGULAAAECCpXAcBRucqlcoVAtXula8JUhCFl75PmDSva78zKFgAAgJ8iuQoAVK4Q8HKyKvDik9WutHEMEQQAAH6N5CoAULlCwIuJr+AJTla2dq/0SjgAAAC+QHIVAKhcIeAld7LPnZKlYuepUAUMAADAt0iuAkAElSsEOmuIfVEKSUUTrDIkXBWugAEAAPgOyVUAoHKFoJDSTxowQ4qt53o8NlG67t1SKlsWKba+vQIGAADgp1iKPQAw5wpBI6Wf1KyPfe5UTpa9EpXcyV7ZslrtqwLKIrdLtqdOtvcDAADwUyRXAYDKFYKKNURq3LnocUdlK23sGcuxW6Rr32KfKwAA4PdIrgLA6ZUrwzBksVRwUQDAX51e2TqaIaWNl44dlLjnAQBAAGDOVQCICDv1Y8orZGgggpyjsnXBAKndcPux9TPNjQkAAMADJFcBIDL01DwT5l2hSml1o/3rzm+kI7+bGwsAAEApSK4CQFiIxTkqinlXqFJqni0lXyrJkDa+Z3Y0AAAAJSK5CgAWi8VZvTpB5QpVTesh9q/rZ0o27n8AAOC/SK4ChGPeFZUrVDkp/aTwatJfu6Q9K82OBgAAoFgkVwHCUblizhWqnPCzpBZX2x+vn2VuLAAAACUguQoQVK5QpbW+yf516ydSbrapoQAAABSH5CpAULlCldbgIqn2uVL+MWnLx2ZHAwAA4BbJVYCgcoUqzWKRWg22P2bPKwAA4KdIrgIElStUeRcOlCwh0u9rpD+2mx0NAABAESRXAYLKFaq8aglS0572x0snS5s+kNKXSzb+TgAAAP8QanYA8EwElStAqnOutP1LafOH9j+SFJsopT5tX7IdAADARFSuAoSzcpXPb+lRRW2dL333UtHj2RnS3KH2dgAAABORXAUI55yrAipXqIJshVLaWEmGm8aTx9LGMUQQAACYiuQqQJyqXJFcoQravVLK3l9CB0PK3mfvBwAAYBKSqwBxqnLFb+ZRBeVkebcfAACAD5BcBQgqV6jSYuK92w8AAMAHSK4CBJUrVGnJneyrAspSTAeLFFvf3g8AAMAkJFcBgsoVqjRriH25dUnFJlipk+39AAAATEJyFSAiQu0/KipXqLJS+kkDZkix9Yq2JXdinysAAGA6NhEOEJFh9t/IU7lClZbST2rWx74qYE6WlJstfXGf/fmBbVLdZmZHCAAAqjAqVwHCUbk6QeUKVZ01RGrcWWp5nXTRLVKzqyQZ0rf/MTsyAABQxZmeXL3yyitq1KiRIiMj1aFDB61Zs6bE/vPmzVOzZs0UGRmpli1basGCBS7tOTk5GjVqlBo0aKCoqCilpKTotdde8+UlVAoqV0AxLn9IkkX6eb60f4PZ0QAAgCrM1OTq/fff15gxYzRhwgStW7dOF154oXr16qUDBw647b9y5UoNGjRII0aM0Pr169W/f3/1799fmzdvdvYZM2aM0tLSNHPmTP3888+69957NWrUKM2fP7+yLssnmHMFFCM+xV7FkqheAQAAU5maXD3//PO69dZbNXz4cGeFKTo6Wm+//bbb/i+++KJSU1N1//33q3nz5nriiSfUpk0bTZs2zdln5cqVGjZsmC677DI1atRIt912my688MJSK2L+jsoVUILLxkuWEGnHQmnP92ZHAwAAqijTFrTIy8vT2rVrNX78eOcxq9Wq7t27a9WqVW5fs2rVKo0ZM8blWK9evfTJJ584n3fq1Enz58/XLbfcosTERC1ZskTbt2/XCy+8UGwsJ06c0IkTJ5zPs7OzJUn5+fnKz88vz+V5zHH+0t4nRPak6nh+gc9jQtXg6b0XEGIbKuSCgbJunCXb1xNl63K/fcGLmHgZSR1Zot2PBNV9h4DBfQezcO8Fh7L8/ExLrg4ePKjCwkLFx8e7HI+Pj9e2bdvcviYzM9Nt/8zMTOfzl19+WbfddpsaNGig0NBQWa1Wvfnmm+rSpUuxsUyaNEkTJ04scnzhwoWKjo4uy2WV26JFi0ps35sjSaE6knOsyDwzoCJKu/cCRVRBW12h9xSy5ztZZ37nPH48rKY2NRisjOoXmRgdzhQs9x0CC/cdzMK9F9iOHTvmcd+gW4r95Zdf1urVqzV//nwlJydr2bJluuuuu5SYmKju3bu7fc348eNdKmLZ2dlKSkpSz549FRsb69N48/PztWjRIvXo0UNhYWHF9ttxIEfPbVopS0i4rrzycp/GhKrB03svUFi2fS7rlqLDZiPz/9JF6dNUeO07MppdZUJkOF2w3XcIDNx3MAv3XnBwjGrzhGnJVe3atRUSEqKsrCyX41lZWUpISHD7moSEhBL7Hz9+XA8++KA+/vhj9enTR5J0wQUXaMOGDXruueeKTa4iIiIUERFR5HhYWFil/UUo7b2qRdnjO1Fg4y8nvKoy73OfsRVKix5022SRIcmi0EUPSef3Y4ignwiK+w4Bh/sOZuHeC2xl+dmZtqBFeHi42rZtq8WLFzuP2Ww2LV68WB07dnT7mo4dO7r0l+xlVkd/xxwpq9X1skJCQmSzBfZCEKevFmgYhsnRAH5m90ope38JHQwpe5+9HwAAgI+YOixwzJgxGjZsmNq1a6f27dtr6tSp+vvvvzV8+HBJ0tChQ1W/fn1NmjRJknTPPfeoa9eumjJlivr06aM5c+boxx9/1BtvvCFJio2NVdeuXXX//fcrKipKycnJWrp0qWbMmKHnn3/etOv0hoiTqwUahpRfaCg81GJyRIAfyckqvU9Z+gEAAJSDqcnVDTfcoD/++EOPPvqoMjMz1apVK6WlpTkXrdizZ49LFapTp06aPXu2Hn74YT344INq2rSpPvnkE7Vo0cLZZ86cORo/frwGDx6sQ4cOKTk5Wf/5z390xx13VPr1eZOjciXZq1fhoabv/wz4j5j40vuUpR8AAEA5mL6gxahRozRq1Ci3bUuWLCly7Prrr9f1119f7PkSEhL0zjvveCs8v3F6cnUi3yZFmhgM4G+SO0mxiVJ2hiR3w2Yt9vbkTpUdGQAAqEIofwQIi8Vyat5VfqHJ0QB+xhoipT598kkxQ2ZTJ7OYBQAA8CmSqwASeXLe1YmCwF6cA/CJlH7SgBlSbL2ibZfeZ28HAADwIdOHBcJzVK6AUqT0k5r1sa8KmJMl/bJA2vyh9Nu3kvGoZGEhGAAA4DskVwGEyhXgAWuI1Liz/XHjrtIvX0r710u/fi017WFubAAAIKgxLDCAOCpXJ6hcAZ6JqSO1u8X+eMlk+14GAAAAPkJyFUCoXAHlcMk9UmiktO9Haefi0vsDAACUE8lVAGHOFVAOMXVPq149TfUKAAD4DMlVAKFyBZSTo3r1+xrptyVmRwMAAIIUyVUAoXIFlFO1BKntzfbHSyZL6cukTR9I6cslG3+fAACAd7BaYAChcgVUwCX3SD/8n7R3tfRu31PHYxPtGxCzDxYAAKggKlcBhMoVUAG//yjZCooez86Q5g6Vts6v/JgAAEBQIbkKIBFUroDysRVKaWOLaTy5wEXaOHs/W6F9uCDDBgEAQBkxLDCAULkCymn3Sil7fwkdDCl7n7TsOWnddNe+DBsEAAAeonIVQJhzBZRTTpZn/ZY8VTQJO3PYIJUtAABQDCpXAYTKFVBOMfEVeLEhyWIfNmjYpK/GU9kCAABuUbkKIFSugHJK7mRPgmQp5wlODhucN6z0yhYAAKiySK4CCJUroJysIfbqkqSiCVZ5Ey6HMxbEAAAAVRbJVQChcgVUQEo/acAMKbae6/HYROmyByt48pOVrd0rK3geAAAQyJhzFUCoXAEVlNJPatbHngTlZNnnYiV3sretm24f4ueoRJWHpwtnAACAoERyFUCoXAFeYA2RGncuejz1afvcKVnkmmCd+bwEMfH2oYFnJm/WkIrHDQAA/B7JVQBxVK5OULkCvM8xbDBtbNHVAHs+JS0cX3JlKzJO+vugNLUFqwkCAFBFkVwFECpXgI8VN2zQGiJZrcVUtk7KPSJ9cHPR447VBAfMsJ+fyhYAAEGL5CqARIQx5wrwueKGDRZb2aovNekmrf9fMSdknywAAKoKkqsAEhlK5QowVXGVrd0rS0iuJJd9ss50ZmULAAAELJKrAELlCvAD7ipbFVol8LTKVrM+DBEEACCAsc9VAKFyBfipmPgKnoB9sgAACAYkVwHk9MqVYVRgLx4A3pXcyT53SpaKnYd9sgAACGgkVwHEUbmyGVKBjeQK8BvWEPuiFJKKJlhlSLgqXAEDAABmIrkKII7KlcS8K8DvOFYTjK3nejw2Ubru3VIqWxb7qoPJnXwdJQAA8CEWtAggjk2EJfu8q2omxgLAjYrsk5U6mcUsAAAIcCRXAcRisSg81Kq8AhuLWgD+qqz7ZFlCpOveYRl2AACCAMlVgIk8mVwxLBAIQKdXtv7aLX35gJT/txR+ltmRAQAAL2DOVYCJCDu5HHs+lSsgIDkqW22GSG1Pbiq85nVzYwIAAF5BchVgIh3LsRdQuQIC3kX/tH/dsUj6c6e5sQAAgAojuQowEaFUroCgUauJdE4PSYb0w1tmRwMAACqI5CrAULkCgkyH2+1f18+UTuSYGwsAAKgQkqsAQ+UKCDJNrpBqNpFOHJF+et/saAAAQAWQXAUYx15XJ6hcAcHBapXa32p/vOZNyXCzBxYAAAgI5Uqudu7cqYcffliDBg3SgQMHJElffvmltmzZ4tXgUFQkqwUCwafVjVLYWdIfP0vpy8yOBgAAlFOZk6ulS5eqZcuW+v777/XRRx8pJ8c+R2Djxo2aMGGC1wOEK0flijlXQBCJjJMuHGh//P3rUvpyadMH9q82/q4DABAoypxcjRs3Tk8++aQWLVqk8PBw5/Fu3bpp9erVXg0ORVG5AoJU+9vsX3/5Qnr3KunDEfavU1tIW+ebGxsAAPBImZOrTZs26eqrry5yvG7dujp48KBXgkLxnJWrfH6bDQSVg9vdH8/OkOYOJcECACAAlDm5ql69ujIyMoocX79+verXr++VoFA8Z+WqgMoVEDRshVLa2GIaTy5wkTaOIYIAAPi5MidXAwcO1NixY5WZmSmLxSKbzabvvvtO//73vzV06FBfxIjTULkCgtDulVL2/hI6GFL2Pns/AADgt8qcXD311FNq1qyZkpKSlJOTo5SUFHXp0kWdOnXSww8/7IsYcZoIKldA8MnJ8m4/AABgitCyviA8PFxvvvmmHnnkEW3evFk5OTlq3bq1mjZt6ov4cAYqV0AQion3rF90bfsKgjlZ9tckd5KsIb6NDQAAeKzMyZVDw4YN1bBhQ2/GAg8w5woIQsmdpNhE++IVKmYT4bBo6ZOR0tHThg/GJkqpT0sp/SolTAAAULIyJ1e33HJLie1vv/12uYNB6ahcAUHIGmJPkuYOlWSR2wQr/5j9z+kcKwkOmEGCBQCAHyhzcvXXX3+5PM/Pz9fmzZt1+PBhdevWzWuBwT0qV0CQSulnT5LSxroublEtUco9XDSxkmRPwiz2lQSb9Sl9iKCt0L4oBsMKAQDwiTInVx9//HGRYzabTSNHjlSTJk28EhSKR+UKCGIp/exJ0ukJkGGTZpRUlTptJcHkTsUnT1vnF03cGFYIAIBXlXvO1emsVqvGjBmjyy67TA888IA3ToliULkCgpw1RGrc+dTzTR949rpfFkgf3+Y+eZJODjk8Y7ghwwoBAPAqryRXkrRz504VFBR463QoBpUroIrxdCXB1f8teiw7Q5p7kxRVU+4XyijjsEIAAFCiMidXY8aMcXluGIYyMjL0xRdfaNiwYV4LDO45Kld5VK6AqsGTlQSLdbL/8UMl93EMKzy9YgYAAMqszMnV+vXrXZ5brVbVqVNHU6ZMKXUlQVQclSugiilxJcFiVhYsDzYoBgCgwsqcXH377be+iAMeYs4VUAUVt5JgbKKU8g/3QwLLytPhhwAAoFhem3OFykHlCqii3K0k6FgdsKLJVWx9+7kAAECFeJRctW7dWhaLxaMTrlu3rkIBoWRUroAq7MyVBCUP5mRZpKga0nHHHoVu+vSaxGIWAAB4gUfJVf/+/X0cBjzlqFwV2AwVFNoUGmI1OSIApip1Tpakvi/av545rNDRPy+nUkIFACDYeZRcTZgwwddxwEOOypVkr16RXAEocU5W6uRTe1idOaxw31rp6wn2P836SFHVTQkfAIBgwZyrAOOoXEn2eVdnRfAjBKDi52SdPtzvzGGFSR2kDbOkg9ulJZOl3pMrP24AAIJImcsehYWFeu6559S+fXslJCSoZs2aLn/gW1arReEnq1XMuwLgwpE8tbzO/rW0eVSh4VLvp+2P17whZW31fYwAAASxMidXEydO1PPPP68bbrhBR44c0ZgxY3TNNdfIarXqscce80GIOBMrBgLwmibdpOZ9JaNQWnC/lL5M2vSBlL5csvFvDAAAZVHmMWWzZs3Sm2++qT59+uixxx7ToEGD1KRJE11wwQVavXq1Ro8e7Ys4cZqIsBAdPVFA5QqAd/R6SvolTdq9Qnp3xanjsYn2xTIcc7YAAECJyly5yszMVMuWLSVJMTExOnLkiCTpqquu0hdffOHd6OAWlSsAXrV/g2TLL3o8O8O+CuHW+ZUeEgAAgajMyVWDBg2UkZEhSWrSpIkWLlwoSfrhhx8UERHh3ejgVmQYc64AeImt0L7KoFsnl3VPG8cQQQAAPFDm5Orqq6/W4sWLJUl33323HnnkETVt2lRDhw7VLbfc4vUAUVREqH2SOpUrABW2e+UZe1+dyZCy99n7AQCAEnk852ratGkaMmSIJk8+tVTvDTfcoIYNG2rVqlVq2rSp+vbt65Mg4YrKFQCvycnybj8AAKowjytXDz30kBITEzV48GB98803zuMdO3bUmDFjSKwqEZUrAF4TE+/dfgAAVGEeJ1eZmZl67bXXtH//fvXo0UONGzfWE088ob179/oyPrhB5QqA1yR3sq8KKEsxHSxSbH17PwAAUCKPk6uoqCgNHTpU3377rXbs2KGbbrpJb731lho3bqzU1FTNmzdP+fluVpuC1zkqVyeoXAGoKGuIfbl1ScUmWKmTS9+QGAAAlH1BC0k6++yz9fjjjys9PV1ffvmlatWqpZtvvln169f3dnxwg8oVAK9K6ScNmCHF1ivadsWj7HMFAICHyryJ8OksFotCQ0NlsVhkGAaVq0rCnCsAXpfST2rWx74qYE6WtOE9aefX0h/bzI4MAICAUa7K1d69e/X444/r7LPPVo8ePbR//369+eabzv2vyuKVV15Ro0aNFBkZqQ4dOmjNmjUl9p83b56aNWumyMhItWzZUgsWLCjS5+eff1a/fv0UFxens846SxdddJH27NlT5tj8FZUrAD5hDZEad5ZaXid1e9B+bMvHUs4f5sYFAECA8Di5ysvL05w5c9SzZ081btxYb775pm688UZt375d33zzjQYPHqzIyMgyvfn777+vMWPGaMKECVq3bp0uvPBC9erVSwcOHHDbf+XKlRo0aJBGjBih9evXq3///urfv782b97s7LNz505deumlatasmZYsWaKffvpJjzzySJlj82cRYVSuAPhY/bZSYhupME9a/z+zowEAICB4nFwlJCTo5ptvVmxsrD777DPt3r1bTz75pM4+++xyv/nzzz+vW2+9VcOHD1dKSopee+01RUdH6+2333bb/8UXX1Rqaqruv/9+NW/eXE888YTatGmjadOmOfs89NBDuvLKK/XMM8+odevWatKkifr166e6deuWO05/ExlK5QpAJWh/q/3rj+9INn6ZAwBAaTyec/Xwww/rpptuUp06dbzyxnl5eVq7dq3Gjx/vPGa1WtW9e3etWrXK7WtWrVqlMWPGuBzr1auXPvnkE0mSzWbTF198oQceeEC9evXS+vXr1bhxY40fP179+/cvNpYTJ07oxIkTzufZ2dmSpPz8fJ/PI3OcvyzvczK30rETBcxzQ7mV595DFXNeX4VGPSTLkT0q+PkLGef2rvApue9gBu47mIV7LziU6XO6px3PTGoq6uDBgyosLFR8vOvGlPHx8dq2zf0E6szMTLf9MzMzJUkHDhxQTk6OJk+erCeffFJPP/200tLSdM011+jbb79V165d3Z530qRJmjhxYpHjCxcuVHR0dHkur8wWLVrkcd/f9lskhSh9z14tWLDbd0GhSijLvYeqJ6VaRzU9/oUOpT2tVb8aXjsv9x3MwH0Hs3DvBbZjx4553LdCqwX6G5vNPkzuH//4h+677z5JUqtWrbRy5Uq99tprxSZX48ePd0kes7OzlZSUpJ49eyo2NtanMefn52vRokXq0aOHwsLCPHrNX2v26pPdP6tmnQRdeWUrn8aH4FWeew9V0OHzZbyyQHWPbtaVHc6Vap1TodNx38EM3HcwC/decHCMavOEaclV7dq1FRISoqysLJfjWVlZSkhIcPuahISEEvvXrl1boaGhSklJcenTvHlzrVixothYIiIiFBERUeR4WFhYpf1FKMt7nRVh75dvM/iLigqrzPscAajOOVLTntKOrxS24X9S6lNeOS33HczAfQezcO8FtrL87Mq1FLs3hIeHq23btlq8eLHzmM1m0+LFi9WxY0e3r+nYsaNLf8leZnX0Dw8P10UXXaRffvnFpc/27duVnJzs5SswD6sFAqhUjoUtNsyU8jwfGgEAQFVj6rDAMWPGaNiwYWrXrp3at2+vqVOn6u+//9bw4cMlSUOHDlX9+vU1adIkSdI999yjrl27asqUKerTp4/mzJmjH3/8UW+88YbznPfff79uuOEGdenSRZdffrnS0tL02WefacmSJWZcok+wWiCAStXkCqlGY+mvdOnbp6TEVlJMvJTcyb43FgAAkFSO5KqwsFDTp0/X4sWLdeDAAec8J4dvvvnG43PdcMMN+uOPP/Too48qMzNTrVq1UlpamnPRij179shqPVVc69Spk2bPnq2HH35YDz74oJo2bapPPvlELVq0cPa5+uqr9dprr2nSpEkaPXq0zjvvPH344Ye69NJLy3qpfutU5YrkCkAlsFqlhhfbk6tVL586HpsopT4tpfQzLzYAAPxImZOre+65R9OnT1efPn3UokULWSyWCgUwatQojRo1ym2bu2rT9ddfr+uvv77Ec95yyy265ZZbKhSXPztVuWJYIIBKsHW+tHFO0ePZGdLcodKAGSRYAACoHMnVnDlzNHfuXF155ZW+iAcecFSuTlC5AuBrtkIpbawkd8uwG5IsUto4qVkfhggCAKq8Mi9oER4ernPOqdhSvKiYyDAqVwAqye6VUvb+EjoYUvY+ez8AAKq4MidX//rXv/Tiiy/KMLy3mSTKJiKUOVcAKklOVul9HP1shVL6cmnTB/avNn4BBACoWso8LHDFihX69ttv9eWXX+r8888vsu77Rx995LXg4B6VKwCVJibes35Zm6VFj7hWuVjwAgBQxZQ5uapevbquvvpqX8QCDzkqV/mFhgpthkKsFVtUBACKldzJniRlZ8j9vKuTVrxQ9BgLXgAAqpgyJ1fvvPOOL+JAGTgqV5K9ehUdbup2ZQCCmTXEXn2aO1SSRa4J1snnFqtkuBumzIIXAICqpcxzrmC+8JBTPzbmXQHwuZR+9upTbD3X47GJ0mUPFpNYObDgBQCg6ihXyeODDz7Q3LlztWfPHuXl5bm0rVu3ziuBoXihIVaFWi0qsBnMuwJQOVL62atPu1faF6+IibcPGdzysWev93RhDAAAAliZK1cvvfSShg8frvj4eK1fv17t27dXrVq19Ntvv6l3796+iBFuRIaxYiCASmYNkRp3llpeZ/9qDfF8wQtP+wEAEMDKnFz997//1RtvvKGXX35Z4eHheuCBB7Ro0SKNHj1aR44c8UWMcCMilBUDAfgBx4IXKm5hHYsUW9/eDwCAIFfm5GrPnj3q1Mn+n2RUVJSOHj0qSbrpppv03nvveTc6FIvKFQC/4FjwQlKxCVbqZBazAABUCWVOrhISEnTo0CFJUsOGDbV69WpJUnp6OhsLVyJn5SqfyhUAkxW34EVIGMuwAwCqlDInV926ddP8+fMlScOHD9d9992nHj166IYbbmD/q0oU4ahcFVC5AuAHUvpJ926Whn0u9XlekkUqzJfqXWB2ZAAAVJoyrxb4xhtvyGazf6C/6667VKtWLa1cuVL9+vXT7bff7vUA4R6VKwB+x7HgRePO0tZPpPRl9tUEL73P7MgAAKgUZU6urFarrNZTBa+BAwdq4MCBXg0KpXNsJEzlCoBfanGtPbna/CHJFQCgyijXJsLLly/XkCFD1LFjR+3bt0+S9L///U8rVqzwanAoXkSofVgglSsAfql5P8kaKmVukg7+anY0AABUijInVx9++KF69eqlqKgorV+/XidOnJAkHTlyRE899ZTXA4R7VK4A+LXomtLZl9kfb/nI1FAAAKgsZU6unnzySb322mt68803FRYW5jx+ySWXaN26dV4NDsWjcgXA77W41v5184fmxgEAQCUpc3L1yy+/qEuXLkWOx8XF6fDhw96ICR5wVK5OULkC4K+a9ZFCwqU/tklZW82OBgAAnyvXPle//lp0/PyKFSt09tlneyUolI7KFQC/FxknndPD/pihgQCAKqDMydWtt96qe+65R99//70sFov279+vWbNm6d///rdGjhzpixjhBpUrAAGhxTX2r5s/lNhoHgAQ5Mq8FPu4ceNks9l0xRVX6NixY+rSpYsiIiL073//W3fffbcvYoQbjspVLpUrAP7s3FQpNEo69JuUsVGqc77ZEQEA4DNlrlxZLBY99NBDOnTokDZv3qzVq1frjz/+0BNPPOGL+FAMKlcAAkJEjHRuL/tjFrYAAAS5cu1zJUnh4eFKSUlR+/btFRMT482Y4AEqVwAChmNo4JZPGBoIAAhqHg8LvOWWWzzq9/bbb5c7GHiOyhWAgNG0pxQeIx3ZI8v+tWZHAwCAz3icXE2fPl3Jyclq3bq1DH7zaDoqVwACRliUdN6V0qa5sqx6RfVzG8iyO1Y6u4tkDTE7OgAAvMbj5GrkyJF67733lJ6eruHDh2vIkCGqWbOmL2NDCSKoXAEIJHENJEkhv3ymdpK0+1UpNlFKfVpK6WdqaAAAeIvHc65eeeUVZWRk6IEHHtBnn32mpKQkDRgwQF999RWVLBNQuQIQMLbOl1a8UPR4doY0d6i9XZJshVL6cmnTB/avNv59AwAEljItxR4REaFBgwZp0KBB2r17t6ZPn64777xTBQUF2rJlCwtbVCLmXAEICLZCKW2sJHe/hDMkWaS0cZJhk74aL2XvP9VMZQsAEGDKvVqg1WqVxWKRYRgqLOS3i5WNyhWAgLB7pWvCVIQhZe+T5g0r2u/MyhYAAH6uTMnViRMn9N5776lHjx4699xztWnTJk2bNk179uyhalXJqFwBCAg5WRV48clqV9o4hggCAAKCx8MC77zzTs2ZM0dJSUm65ZZb9N5776l27dq+jA0lOFW5IrkC4Mdi4it4gpOVrd0rpcadvRISAAC+4nFy9dprr6lhw4Y6++yztXTpUi1dutRtv48++shrwaF4pypX/DYXgB9L7mSfO5WdIffzrjxUoQoYAACVw+PkaujQobJYLL6MBWUQEWavXJ2gcgXAn1lD7ItSzB0qySLXBOvM5yWocAUMAADfK9MmwvAfkaH2ylVeoU02myGrlcQXgJ9K6ScNmGFfNfDM1QB7PiUtHF9CZcti75fcqbKiBQCg3Mq0FDv8h6NyJdkXtYgKDymhNwCYLKWf1KyPCn5bpg3Lv1Krzr0UenYXe2XLai2msnVS6mR7PwAA/Fy5l2KHuRyVK4l5VwAChDVERvKl2lezo4zkS08lTI7KVmy9oq/pOIp9rgAAAYPkKkCFhlgVcnIoICsGAgh4Kf2kezdLwz6Xrn1LunCQ/fj2NKmwwNzYAADwEMlVAHNUr6hcAQgK1hD7custr5N6PyNF15L+3CFtnG12ZAAAeITkKoA55l1RuQIQdCJjpc7/sj9eMlnKP25uPAAAeIDkKoBRuQIQ1NqNkGIb2DcR/uH/zI4GAIBSkVwFMCpXAIJaWKR0+Xj74+XPS7lHzI0HAIBSkFwFsAgqVwCC3QUDpdrnSscPSd+9JKUvlzZ9YP9q498+AIB/YZ+rAEblCkDQCwmVuj0izb1JWj5FWv7cqbbYRCn1aZZqBwD4DSpXAYw5VwCqBuOMrydlZ9g3H946v9IjAgDAHZKrAEblCkDQsxVKaeOKaTyZbKWNY4ggAMAvkFwFMCpXAILe7pVS9v4SOhj21QR3r6y0kAAAKA7JVQCjcgUg6OVkebcfAAA+RHIVwKhcAQh6MfHe7QcAgA+RXAWwiDD7j4/KFYCgldzJviqgLMV0sEix9e39AAAwGclVAIsMtQ8LpHIFIGhZQ+zLrUtyn2AZUupkez8AAExGchXAHJWrE1SuAASzlH7SgBlSbL2ibTH1pKY9Kz8mAADcYBPhAEblCkCVkdJPatbHvipgTpYUHi3Nv0fKyZCWTpa6P2Z2hAAAkFwFMuZcAahSrCFS486nnl9lk94fLH33kpTSX0psZVZkAABIYlhgQIsMo3IFoAprfpV0/tWSUSh9OkrKz5XSl0ubPrB/ZWNhAEAlo3IVwCJCqVwBqOJ6Pyv9tlTK2iQ9d4504uiptthE+2IYKf3Miw8AUKVQuQpgVK4AVHkxdaQLbrA/Pj2xkqTsDGnuUGnr/MqPCwBQJZFcBTAqVwCqPFuh9POnxTQa9i9p4xgiCACoFCRXASyC1QIBVHW7V0rZ+0voYEjZ++z9AADwMZKrAMZqgQCqvJws7/YDAKACWNAigFG5AlDlxcR73s9WeGqfrJh4KbmTfXl3AAC8hOQqgEVSuQJQ1SV3sq8KmJ0h5xyrM0XXko79KU1t4TqEkNUEAQBexrDAAOasXOVTuQJQRVlD7AmSJMnivs+xP6V5w4rOzWI1QQCAl5FcBTBn5aqAyhWAKiylnzRghhRbz/V4bH2pSfcSXshqggAA72JYYABzVK7yCmwyDEMWSzG/tQWAYJfST2rWp+icqt0rpZ1fl/DC01YTbNy50sIFAAQnkqsA5qhcSdKJAptzU2EAqJKsIUUTJFYTBABUIoYFBrBQ66kf34odB1VoK2YyNwBUVWVZTRAAgAoiuQpQaZszdPmUJc7n/5zxoy59+hulbc4wLygA8DeO1QSLW+xCFvvcrOROlRkVACBIkVwFoLTNGRo5c50yj+S6HM88kquRM9eRYAGAgyerCaZOZr8rAIBXkFwFmEKboYmfbXW7m4vj2MTPtjJEEAAciltNUBap/6vscwUA8BqSqwCzJv2QMs6oWJ3OkJRxJFdr0g9VXlAA4O9S+kn3bpaGfS5d86YU11CSIR38xezIAABBhOQqwBw4WnxiVZ5+AFBlOFYTvGCAdOUz9mOrXy26uTAAAOXkF8nVK6+8okaNGikyMlIdOnTQmjVrSuw/b948NWvWTJGRkWrZsqUWLFhQbN877rhDFotFU6dO9XLU5qhbLdKr/QCgSjo3VUq6WCrIlZZMNjsaAECQMD25ev/99zVmzBhNmDBB69at04UXXqhevXrpwIEDbvuvXLlSgwYN0ogRI7R+/Xr1799f/fv31+bNm4v0/fjjj7V69WolJib6+jIqTfvGNVUvLrKkda9ULy5S7RvXrMywACCwWCxSj4n2x+tnSn9sNzceAEBQMD25ev7553Xrrbdq+PDhSklJ0Wuvvabo6Gi9/fbbbvu/+OKLSk1N1f3336/mzZvriSeeUJs2bTRt2jSXfvv27dPdd9+tWbNmKSwsrDIupVKEWC2a0DdFUtF1rxzPJ/RNUYi1uPQLACBJanixdN6VklEoffO42dEAAIJAqJlvnpeXp7Vr12r8+PHOY1arVd27d9eqVavcvmbVqlUaM2aMy7FevXrpk08+cT632Wy66aabdP/99+v8888vNY4TJ07oxIkTzufZ2dmSpPz8fOXn55flksrMcf6yvM8V59XWywMv1JMLtikz+1TctauFa0Kf5rrivNo+jxuBrzz3HlBRfnffdX1QodvTZPn5MxX89p1ky5NysqSYeBlJHVmiPUj43X2HKoN7LziU5ednanJ18OBBFRYWKj4+3uV4fHy8tm3b5vY1mZmZbvtnZmY6nz/99NMKDQ3V6NGjPYpj0qRJmjhxYpHjCxcuVHR0tEfnqKhFixaV+TVjU6Sd2RbN3mnVoRMWXZlwXIW712rBbh8EiKBVnnsPqCh/uu9a1bhEyYeWy/K/fgoxCpzHj4fV1KYGg5VR/SITo4M3+dN9h6qFey+wHTt2zOO+piZXvrB27Vq9+OKLWrdunSwWz4bGjR8/3qUalp2draSkJPXs2VOxsbG+ClWSPRNetGiRevToUe7hiwc+2aK5a/cput45urJHUy9HiGDljXsPKCt/vO8s6/6Q8eVyl8RKkiLz/9JF6dNUeO07MppdZVJ08AZ/vO9QNXDvBQfHqDZPmJpc1a5dWyEhIcrKynI5npWVpYSEBLevSUhIKLH/8uXLdeDAATVs2NDZXlhYqH/961+aOnWqdu3aVeScERERioiIKHI8LCys0v4iVOS9WibV0Ny1+7Q1M4e/uCizyrzPAQe/ue9shdJ3U9w2WWRIsih00UPS+f0YIhgE/Oa+Q5XDvRfYyvKzM3VBi/DwcLVt21aLFy92HrPZbFq8eLE6duzo9jUdO3Z06S/ZS62O/jfddJN++uknbdiwwfknMTFR999/v7766ivfXYyJWiTaq2ub9x2RYRgmRwMAAWT3ylL2uTKk7H32fgAAlML0YYFjxozRsGHD1K5dO7Vv315Tp07V33//reHDh0uShg4dqvr162vSpEmSpHvuuUddu3bVlClT1KdPH82ZM0c//vij3njjDUlSrVq1VKtWLZf3CAsLU0JCgs4777zKvbhK0rxerEKsFv35d54ys3NVLy7K7JAAIDDkZJXepyz9AABVmunJ1Q033KA//vhDjz76qDIzM9WqVSulpaU5F63Ys2ePrNZTBbZOnTpp9uzZevjhh/Xggw+qadOm+uSTT9SiRQuzLsF0kWEhalo3Rtsyj2rzvmySKwDwVEx86X3K0g8AUKWZnlxJ0qhRozRq1Ci3bUuWLCly7Prrr9f111/v8fndzbMKNucnxp1Mro6oRwofAgDAI8mdpNhEKTtDkrth1RZ7e3Knyo4MABCATN9EGN7Ror593tWW/UdMjgQAAog1REp9+uSTYlaYTZ3MYhYAAI+QXAWJFvXjJEmb93m+VCQAQFJKP2nADCm2XtG23k/b2wEA8ADJVZBoXi9WFouUmZ2rP46eMDscAAgsKf2kezdLwz6Xrn1LatDefvy3pebGBQAIKCRXQSImIlSNa58lSdrM0EAAKDtriNS4s9TyOukf0yRLiPTLF9KuFWZHBgAIECRXQaTlyaGBW/aRXAFAhdQ5T2p7s/3xwoclm83UcAAAgYHkKoi0SGTeFQB4zWXjpfBq0v710uYPzI4GABAASK6CyPknVwxkWCAAeEFMHanzffbHX0+Ufv1a2vSBlL5cshWaGxsAwC/5xT5X8I7zT1aufv/ruA4fy1P16HCTIwKAAHfxndLKV6Ts36WZ1546HptoX8KdlQQBAKehchVE4qLC1LBmtCRpy36GBgJAhe1YJB3/s+jx7Axp7lBp6/zKjwkA4LdIroKMYzPhzSxqAQAVYyuU0sYW02jYv6SNY4ggAMCJ5CrIODYT3kRyBQAVs3ullL2/hA6GlL3P3g8AAJFcBR3HioEMCwSACsrJ8m4/AEDQI7kKMucn2ocFph/8W0dz802OBgACWEy8d/sBAIIeyVWQqRUTocS4SEnSVqpXAFB+yZ3sqwLKUkwHixRb394PAACRXAWl80/Ou9pMcgUA5WcNsS+3LqnYBCt1sr0fAAAiuQpKznlXLGoBABWT0k8aMEOKrVe0rcfj7HMFAHDBJsJByLkc+36SKwCosJR+UrM+9lUBc7Kkte9Ku5ZJf6WbHRkAwM9QuQpCLU8OC/z1QI6O5RWYHA0ABAFriNS4s9TyOqnr/fZjP82TTuSYGxcAwK+QXAWhurGRqlMtQjZD+jnjqNnhAEBwadRZqnm2lHdU2vKx2dEAAPwIyVWQanFySfYtDA0EAO+yWKQ2Q+2P171rbiwAAL9CchWkWjhWDGRRCwDwvlaDJWuo9PsPUtZWs6MBAPgJkqsgdX6iI7liOXYA8LqYutJ5ve2PqV4BAE4iuQpSjhUDt2cd1YmCQpOjAYAg1OZm+9eN70n5x00NBQDgH0iuglT96lGqHhWqApuhN5ela9XOP1VoM8wOCwCCR5PLpbiGUu4Raet8s6MBAPgBkqsg9dWWTB3Lt0mSnlv4iwa9uVqXPv2N0jZnmBwZAAQJa4jU5ib7Y4YGAgBEchWU0jZnaOTMdcorsLkczzySq5Ez15FgAYC3tBosWazS7u+kgzvMjgYAYDKSqyBTaDM08bOtcjcA0HFs4mdbGSIIAN4QV19q2tP+mOoVAFR5JFdBZk36IWUcyS223ZCUcSRXa9IPVV5QABDM2gyzf137rrThPSl9uWRjISEAqIpCzQ4A3nXgaPGJVXn6AQBKUXDCPjTwRLb0yR32Y7GJUurTUko/c2MDAFQqKldBpm61SK/2AwCUYOt86YPhkuE6x1XZGdLcoawiCABVDMlVkGnfuKbqxUXKUky7RVK9uEi1b1yzMsMCgOBjK5TSxkolzXJNG8cQQQCoQkiugkyI1aIJfVMkqdgEa0LfFIVYi2sFAHhk90ope38JHQwpe5+9HwCgSiC5CkKpLerp1SFtlBDnOvTPImnKgAuV2qKeOYEBQDDJyfJuPwBAwGNBiyCV2qKeeqQkaE36IR3IztXTX23T/sO5Onws3+zQACA4xMR7tx8AIOBRuQpiIVaLOjappX+0rq87LztHkvTuql2ysccVAFRccif7qoAlzXKNrW/vBwCoEkiuqohr2tRXtchQ7f7zmJZsP2B2OAAQ+Kwh9uXWJRWbYKVOtvcDAFQJJFdVRHR4qAZelCRJmr5yt8nRAECQSOknDZghxbqZy9r7Gfa5AoAqhuSqCrnp4kayWKRl2//QrwdyzA4HAIJDSj/p3s3SsM+la9+S6rWyH8/j31kAqGpIrqqQhrWidUUz+8TqGat2mRsMAAQTa4jUuLPU8jqp7c32Y1s/NTUkAEDlI7mqYoZf0kiS9MHa35Wdy8qBAOB1zftKFquUsUH6a5fZ0QAAKhHJVRXTqUktNa0bo2N5hZr34+9mhwMAwees2lKjS+2Pt843NxYAQKUiuapiLBaLbj5ZvZrBsuwA4Bsp/7B/3fqJqWEAACoXyVUVdHXr+oo9uSz7a0t/1acb9mnVzj9VSKIFAN7RrK8ki7RvrXR4j9nRAAAqSajZAaDyRYeHqsPZtbRoa5ae+Wq783i9uEhN6Jui1BZulhQGAHiuWrx98+Dd30k/fyZ1vMvsiAAAlYDKVRWUtjlDi7ZmFTmeeSRXI2euU9rmDBOiAoAg4xwayKqBAFBVkFxVMYU2QxM/2+q2zTEocOJnWxkiCAAV1fzkBsJ7v5ey95sbCwCgUpBcVTFr0g8p40huse2GpIwjuVqTfqjyggKAYBRbT0q62P7458/MjQUAUClIrqqYA0eLT6zK0w8AUAKGBgJAlUJyVcXUrRbp1X4AgBI072v/unuldDTT3FgAAD5HclXFtG9cU/XiImUppt0i+6qB7RvXrMywACA4VU+S6reTZDA0EACqAJKrKibEatGEvimSVGyCNaFvikKsxbUCAMrEk6GBtkIpfbm06QP7V1th5cQGAPAqkqsqKLVFPb06pI0S4lyH/lkt0is3tmGfKwDwJkdytWuF9MPbRZOnrfOlqS2kd6+SPhxh/zq1hf04ACCgsIlwFZXaop56pCRoTfoh7T98TI/O36K/TxTqrEhuCQDwqoyNkjVMsuVLX9xnPxabKKU+bX88d6hObYZxUnaG/fiAGVJKv0oNFwBQflSuqrAQq0Udm9TStW2TdF2bBpKkeT/uNTkqAAgiW+fbkyRbvuvx7Axp7k3SZ/eoSGIlnTqWNo4hggAQQEiuIEm6vl2SJGnh1iwdOZZfSm8AQKlshVLaWJWYPB0vaU9BQ8reZ19pEAAQEEiuIEk6PzFWzRKqKa/Apvkb95kdDgAEvt0rpez9FT9PTlbFzwEAqBQkV5AkWSwWZ/Vq3trfTY4GAIKAt5KimHjvnAcA4HMkV3Dq3ypRoVaLfvr9iH7JPGp2OAAQ2CqcFFmk2PpScievhAMA8D2SKzjViolQt2Z1JbGwBQBUWHIn+6qAJW3bHlXzZHsxfVInS9YQ38QHAPA6kiu4cAwN/GTDPuUX2kyOBgACmDXk1HLrRZKnk8/7vmhfbj32jP0FI6uzDDsABCCSK7i47Lw6qh0TroM5efp22wGzwwGAwJbSz33yFJt4KnlK6Sfdu1ka9rl03pX29mZXklgBQABix1i4CAux6urW9fXm8nTNW/u7ep6fYHZIABDYUvpJzfrYVw/MybLPxUru5DrczxoiNe4s5f0t/bJA+v1H8+IFAJQblSsU4Rga+O22AzqYc8LkaAAgCDiSp5bX2b8WN4+qwUX2rwe3S8dK2gMLAOCPSK5QxLnx1XRhgzgV2AxN/Xq7Pt2wT6t2/qlCm7uNMAEAXnNWLalWU/vj338wNxYAQJkxLBBuNU+M1cbfj2jm6j2auXqPJKleXKQm9E1Raot6pbwaAFBuSR2kP3dIe7+Xzu1ldjQAgDKgcoUi0jZn6P01RZdizzySq5Ez1yltc4YJUQFAFZHU3v517xpz4wAAlBnJFVwU2gxN/Gyr3A0AdByb+NlWhggCgK8kdbB/3bdWKsw3NxYAQJmQXMHFmvRDyjiSW2y7ISnjSK7WpDPRGgB8ova5UmSclH9MytpsdjQAgDIguYKLA0eLT6zK0w8AUEZWq9SAoYEAEIhIruCibrVIj/sV2gyt2vknqwkCgLc5hgbu/d7cOAAAZcJqgXDRvnFN1YuLVOaRXLfzrhw+27hP983doMzThhCymiAAeAmLWgBAQKJyBRchVosm9E2RJFnOaDv9+ew1e10SK4nVBAHAa+q3lSxW6che6cg+s6MBAHjIL5KrV155RY0aNVJkZKQ6dOigNWtK/k3dvHnz1KxZM0VGRqply5ZasGCBsy0/P19jx45Vy5YtddZZZykxMVFDhw7V/v37fX0ZQSO1RT29OqSNEuJchwgmxEVq2sDWqhbhvuDJaoIA4CURMVJ8C/vj36leAUCgMD25ev/99zVmzBhNmDBB69at04UXXqhevXrpwIEDbvuvXLlSgwYN0ogRI7R+/Xr1799f/fv31+bN9hWVjh07pnXr1umRRx7RunXr9NFHH+mXX35Rv379KvOyAl5qi3paMbab3rv1Yr04sJXeu/VirRjbTbWqRejoiYJiX8dqggDgJc55Vz+YGwcAwGOmJ1fPP/+8br31Vg0fPlwpKSl67bXXFB0drbfffttt/xdffFGpqam6//771bx5cz3xxBNq06aNpk2bJkmKi4vTokWLNGDAAJ133nm6+OKLNW3aNK1du1Z79uypzEsLeCFWizo2qaV/tKqvjk1qKcRqYTVBAKgsLGoBAAHH1AUt8vLytHbtWo0fP955zGq1qnv37lq1apXb16xatUpjxoxxOdarVy998sknxb7PkSNHZLFYVL16dbftJ06c0IkTJ5zPs7OzJdmHGObn+3YDR8f5ff0+3lIr2rNbplZ0aMBcU1UVaPceggP3XRnUa6MwSUbGRhUcy5bCosyOKGBx38Es3HvBoSw/P1OTq4MHD6qwsFDx8fEux+Pj47Vt2za3r8nMzHTbPzMz023/3NxcjR07VoMGDVJsbKzbPpMmTdLEiROLHF+4cKGio6M9uZQKW7RoUaW8T0XZDKl6eIgO50lFl7yQJENRIdIfW1fr863SzmyLsvOl2DCpSawhq7uXwFSBcu8huHDfecAw1Cu0uiILDmv1x6/pUMx5ZkcU8LjvYBbuvcB27Ngxj/sG9VLs+fn5GjBggAzD0Kuvvlpsv/Hjx7tUw7Kzs5WUlKSePXsWm5B5M8ZFixapR48eCgsL8+l7eUtYoyzdPWejJLlZrt2i44XSt8eStGbXIWVmn6oIJsRG6OErm6nX+fFFXoXKF4j3HgIf913ZhOR+IG37TJ2SQmTreKXZ4QQs7juYhXsvODhGtXnC1OSqdu3aCgkJUVZWlsvxrKwsJSQkuH1NQkKCR/0didXu3bv1zTfflJgkRUREKCIiosjxsLCwSvuLUJnvVVFXtWqg0NAQTfxsqzLO2OfqggZx+mpLlub/VHQ59qzsE7p7zka9OqQNe2H5kUC69xA8uO881PBiadtnCtm3ViF8vyqM+w5m4d4LbGX52Zm6oEV4eLjatm2rxYsXO4/ZbDYtXrxYHTt2dPuajh07uvSX7KXW0/s7EqsdO3bo66+/Vq1atXxzAVVYcasJ/ndwW1WLZKl2APCK0xe1MPh3EwD8nenDAseMGaNhw4apXbt2at++vaZOnaq///5bw4cPlyQNHTpU9evX16RJkyRJ99xzj7p27aopU6aoT58+mjNnjn788Ue98cYbkuyJ1XXXXad169bp888/V2FhoXM+Vs2aNRUeHm7OhQYhx2qCp1u1808dzfVsqfb2jWtqTfohHTiaq7rVItW+cU2FMCkLAE6pd4EUEiEdOygd+k2q1cTsiAAAJTA9ubrhhhv0xx9/6NFHH1VmZqZatWqltLQ056IVe/bskdV6qsDWqVMnzZ49Ww8//LAefPBBNW3aVJ988olatLBvtrhv3z7Nnz9fktSqVSuX9/r222912WWXVcp1VVWeLsG+aGumxszdUGRY4YS+KQwZBACH0AgpsbW0d7W0dw3JFQD4OdOTK0kaNWqURo0a5bZtyZIlRY5df/31uv766932b9SokQyGTpimbrVIj/q9/d2uIscyj+Rq5Mx1zMkCgNMlXXQyufpeajXI7GgAACUwfRNhBJf2jWuqXlyk20XaS8OcLABwwznvao25cQAASkVyBa8KsVo0oW+KpKK7YHmScJ0+JwsAIKlBe/vXA1ul3CPmxgIAKBHJFbwutUU9vTqkjRLiXIcIJsRFasQljTw6x4GjuSq0GVq18099umGfVu38k2oWgKqpWrxUPVmSIS1/QUpfLtkKzY4KAOCGX8y5QvBJbVFPPVISiqwGuCb9kN5yM9/qTLsOHtOlT39T4oIXhTaD1QYBBL+t86W/D9gff/eC/U9sopT6tJTSz9zYAAAuSK7gM+6WanfMyco8kqvi6lAhVumFr7cXOX76gheS3G5izGqDAILK1vnS3KHSmf9iZmfYjw+YQYIFAH6EYYGoVCXNyXIotLk/7vhoMe6jTRo5c51LYiWdSr7SNmd4J1gAMJOtUEobqyKJlXTqWNo4hggCgB8huUKlK25OVr24SP3z0kYlvtaQdPhYfkkfNVhtEEBw2L1Syt5fQgdDyt5n7wcA8AsMC4QpipuT9flPJX2QKJ1jtcHVO/+U1WphPhaAwJWT5d1+AACfI7mCadzNyfJ0E+LS3DV7nQ4fz3c+Zz4WgIATE+/dfgAAn2NYIPxKRTYhPt3piZXEfCwAASi5k31VwGL/RbRIsfXt/QAAfoHkCn7Fk02Iq0eHlTn5Yj4WgIBjDbEvty7JfYJlSKmT7f0AAH6B5Ap+p6RNiF8b0kaTr2kpqfjf5RbHMR9rTfoh7wQKAL6W0s++3HqsmyHN8eezDDsA+BnmXMEvFbfghWNRileHtCmyz1X1qLAiwwHdOXA0t9QNiCu6QTEbHAPwmpR+UrM+9lUBc7IkWaSPbpWytki7V0nJHc2OEABwEskV/Ja7BS8c3CVfNsPQ4P/7vtTzbss8qslfflPsBsRpmzNK3aC4pOTJk9cDQJlYQ6TGnU8937VMWjtdWjpZGvqpaWEBAFyRXCFgnZl8FdoM1YuLVOaRXLf7YDm8umRnkWOOBS9u69JYbyxLL/J6R/urQ9pIUrHJkySNnLmuxNeTYAGosM7/ktbPlH5bQvUKAPwIc64QNDxZDKO4kXnGyT9vLi+aWDnaJWncR5s0cuY6l8RKsidPd8xcp3EfbfJog+NCm6FVO//Upxv2adXOP1lkA0DZVG8otR5if7xkkrmxAACcqFwhqDgWwzizspQQF6mBFyXpha93lPj6knIcQ9LhY+7ndDleVly7o0/GkVxN++ZXzflhT7mHHQKApFPVq/Sl9vlYLMkOAKYjuULQKW4xjM9/2m92aJKkF77eXuSYp8MOSb4AODmqV2unS99Okro+YF/wIibenmixRDsAVDqSKwQld4th1K0WWUxv8xmyD10c99EmHTmWX+45X8znAqqYzv+S1s2wL3Cxa9mp47GJ9j2yWKodACoVc65QZbRvXFP14iJL3B/Lain7/lne4hh2WN45XyNnrlPa5gyP3qvQZuj79ENae9Ci79MPeXXOF/PJgEq0f4Nk2Ioez86Q5g6Vts6v9JAAoCqjcoUqw7HgxciZ62SRXJIYR0J1a2f7aoHu2g1J1aPD3FaWHH3iTrarmNdXRGlzviyyV7R6pCRIkodLxYdoxo4fvTbskGXogUpkK5TSxhbTePJfhbRx9j2yGCIIAJWC5ApVSkkLXjgSgNYNaxTbLqnE5GzyNS0lFR225+mCGhXhyYIZjvgrMuywuMQrbXMGy9ADlWn3Sim7pLmkhpS9z97v9D2yAAA+Q3KFKqe4BS8clZnS2ktLziS5fb0kzflhb6n7cFVUcQtm3DFznapHhxU77NCTOV+3dWms+RsziiRej/Rprie++LnEc3tSVZMqvliHv7++Iu3BvpBJRb93VU5Olnf7AQAqjOQKVZK7BS88bS8t+Srp9SUNSyxt2GFFeLpUfGlLzb++LL1IW+aRXN05e32p7+/JMvSeDCss6QO2v7++Iu1SxVeRNDs5OX2uX630Q+p4Tl2vfe88uT6zr9/rYuK92w9AwKhy/94FEIthGMw2P0N2drbi4uJ05MgRxcbG+vS98vPztWDBAl155ZUKCwvz6XvBP5T2AXrkzHWSKjf5MpPjn/rbutjnu515bY720oYsSu6HPPrL60u7vpLai/t5exqbp8lJaSryn7kn9315v3eVdf1+x1YoTW1hX7yiuJmgsYnSvZtUKGvAftDyRjV41a8HtHD59+rZuYNLUu+N9/eH6/Nluy+vzdfM/t5V5N7z5S/yvHFtVU1ZcgOSKzdIruBrFf0QKnl/wQyzWS3Fb+J8+mIhxSUf1aPDiq28Vcbr46LCdOR48ZXBkq7Pk/bilBab5FlyUtJ8Oqli/5lLxSdPpX3vpYrdG966fr8dLrp1voy5Q2XIcFn+1zh5hZYBM5Rmu8jnH7R81e7LanBlVLMren6z2315bb6+r8z+3lXk+gPhl1Fm/5tR2UiuKojkCmYrz38Yvl4wA4GrtOQkIS5Sj/RJ0RNfeP8/c0+SJ1/zxvX783DRT2a/pkfDZijRcsjl2uYXdNSWTs/7/IOWr9r7XViv3NVeT9p9Xc2uyC8dvHF93mh3N8fWG9fm6/uutPc3+3tb0vWXdN/7yy+jiou9MhPTykZyVUEkV/B37j7ISdKlT39T7IIZp/+jKvlu2GFx88kQeLzxn3kg88aQTMm3w0UdPxurbGpv3aa6Oqwmln26J+xj/W1EqFve88oyahR7DRX9oOWrdk9UpBrs62q2VPFfOlS02u2Lark3rs3X911lfG8tFqmkT8+e3nuW0/7eHlB1rbE1k83kLWjLG/sPJ2N3/GxKa6/oSIPKRnJVQSRXCFSO5dAl90vFe/pB8MzXl6a03/5TVUMwqugHdMlXw0UNfRw+Qa2tv2p2weV6sODWsp7AydMPkdZiPiRWtF0quc2f2z25Nn+Ov6Q2x31h1vfO8ffLn793pbX3sq7RhDMqzvuNmpqYP1Rf2doHbOwLbe3Vs5T2kn52Jf1sDVmVEBepFWO7VfoQQZKrCiK5QiDzdKx0cZNsSxuiIxWfuBU3Rl8quaom2T+kGUbwVrlKu75gv/6K4HtTdm0tv+jDiIkqNCzqnTdZ242kCp2vIh8SK9IuyWfnpp3vvZnf21fDpkqy//vm4PhFyhsFV6lf6MqAjf220M+LbR+Zf6+kit1b7916cYkrPvsCyVUFkVwh0HkyEbSke6+8k8tLUlpVzTEUwF17aUMWKzrk0devL+36Sms33Dz2NDbJv5OTin7vAv36fem/YVN1ZcgaLSm8UDfnjy2xb3mTJ6liH7RKa3f+FtsH56ad771Z7YcVo+rK0Rn/LTv7VOR7/2bhVbo1xLzYDVllka3Ydsfry/v+I/Pv1ZUDbtM/WtUv+gY+RHJVQSRXqArKe+9VZAUfb0xQlrw75LGyXu/Lfa5Ki82TxK0iSkpePEmevLUwQHHn9/X1+6tkS6YWhd+vcEuhbsobp+W2C9z2q0jyVPoHLYusMmQpR7vj00l5Xku7vd0io8SfTXHtpX/vS/4AXZH39ta1lXRfOV7vi+99aec3DPffV08YhmRYLLIYJbx3Ce22k+3Wkl7vo9g9OY8n3/tM1dLuIavVsWndigdSBiRXFURyharArHvPzKV9zX59Rdt9sSLdI32a64kvfi73QihS6VU5T5Knin5vfHX9vlaR4aKe/GzeT/5U7bPe18+2JD1ecJPqKNs5ubxQVl0TtU7P2Z5zvpeDI3k6ohjFFZM8eeuDFgCUReHQzxRydpdKfU+SqwoiuUJVEKj3nq/3G/L1632pInsJ+boq53j/imzmasb1V2RIpq+Hi0oe/GzOjlDBlBSFFh53iS1LtbT/4kfUbONkRRzLLLYC4Ce3NgCccu1bUsvrKvUtSa4qiOQKVQH3Hk5XGVU5yX/vO18NyayMPWGkkjcZ1tybilyvY+gPAHMYOvXvRKApLnZDksVilWEYbv998do1D/tcatzZG2fyWFlyg9BKigkA4MdSW9RTj5SEEpOjEKulxBWaSmv3Z6Vdf0ltrw5pUyT5STgj+Snp9aW9d7l/NrZCKc39QhaVllhZrCcn8BTzfqW1+/LctPO9N6XdIkXVkOX4XyeTjVN9PEo+/DZ2iz32jqNkWflykV/gOJ9H1ZSO/1X+949NlJI7uX+tnzB3pzIAgN9wfED/R6v66tiklt8Md6wsJV1/SW2pLeppxdhueu/Wi/XiwFZ679aLtWJsN5cVNEv73la03a3dK6Xs/eX8bnjCYv+gJIuKfiQ8eazjqNOel7Xd3WNvnZt2vvdmtUvq+6I0YIYssa6r7Fpi60udRp86T0DFnigNmCH1fKKE9v/ZX1+R90+dLFlD5M+oXAEAUEF+WbXLyargCey/pbb/lllyO/DR8UEpbaxrIhebaP8QlNJPanBR+dt9eW7a+d6b2S5JzfrYfwmSkyXFxNsrMtYQ82OrSOySvU9J7QNmVPz9/RhzrtxgzhWqAu49mIH7rhKlL5fevcrDzha5TZ4GzLB/LfJBp77rBx1bYfEfpCra7oVzF/y2TBuWf6VWnXsp9Owu3ost2Nv9ObYAaS/x3iuJH8ReYntFmf3+ZcSCFhVEcoWqgHsPZuC+q0S2QmlqCyk7QyXOX+j1lPTV+IolT36O+w5m4d4LDixoAQBAVWcNkVKfluYOVbGVKUcC1bxvycmTNaTSV+cCgEBEcgUAQLBK6Vf6/AaJ5AkAvITkCgCAYFba5HIAgNeQXAEAEOyoTAFApWCfKwAAAADwApIrAAAAAPACkisAAAAA8AKSKwAAAADwApIrAAAAAPACkisAAAAA8AKSKwAAAADwApIrAAAAAPACkisAAAAA8AKSKwAAAADwApIrAAAAAPACkisAAAAA8AKSKwAAAADwglCzA/BHhmFIkrKzs33+Xvn5+Tp27Jiys7MVFhbm8/cDHLj3YAbuO5iB+w5m4d4LDo6cwJEjlITkyo2jR49KkpKSkkyOBAAAAIA/OHr0qOLi4krsYzE8ScGqGJvNpv3796tatWqyWCw+fa/s7GwlJSVp7969io2N9el7Aafj3oMZuO9gBu47mIV7LzgYhqGjR48qMTFRVmvJs6qoXLlhtVrVoEGDSn3P2NhY/tLBFNx7MAP3HczAfQezcO8FvtIqVg4saAEAAAAAXkByBQAAAABeQHJlsoiICE2YMEERERFmh4IqhnsPZuC+gxm472AW7r2qhwUtAAAAAMALqFwBAAAAgBeQXAEAAACAF5BcAQAAAIAXkFwBAAAAgBeQXJnslVdeUaNGjRQZGakOHTpozZo1ZoeEADZp0iRddNFFqlatmurWrav+/fvrl19+cemTm5uru+66S7Vq1VJMTIyuvfZaZWVlufTZs2eP+vTpo+joaNWtW1f333+/CgoKKvNSEMAmT54si8Wie++913mM+w6+sG/fPg0ZMkS1atVSVFSUWrZsqR9//NHZbhiGHn30UdWrV09RUVHq3r27duzY4XKOQ4cOafDgwYqNjVX16tU1YsQI5eTkVPalIEAUFhbqkUceUePGjRUVFaUmTZroiSee0Onrw3HfVW0kVyZ6//33NWbMGE2YMEHr1q3ThRdeqF69eunAgQNmh4YAtXTpUt11111avXq1Fi1apPz8fPXs2VN///23s899992nzz77TPPmzdPSpUu1f/9+XXPNNc72wsJC9enTR3l5eVq5cqXeffddTZ8+XY8++qgZl4QA88MPP+j111/XBRdc4HKc+w7e9tdff+mSSy5RWFiYvvzyS23dulVTpkxRjRo1nH2eeeYZvfTSS3rttdf0/fff66yzzlKvXr2Um5vr7DN48GBt2bJFixYt0ueff65ly5bptttuM+OSEACefvppvfrqq5o2bZp+/vlnPf3003rmmWf08ssvO/tw31VxBkzTvn1746677nI+LywsNBITE41JkyaZGBWCyYEDBwxJxtKlSw3DMIzDhw8bYWFhxrx585x9fv75Z0OSsWrVKsMwDGPBggWG1Wo1MjMznX1effVVIzY21jhx4kTlXgACytGjR42mTZsaixYtMrp27Wrcc889hmFw38E3xo4da1x66aXFtttsNiMhIcF49tlnnccOHz5sREREGO+9955hGIaxdetWQ5Lxww8/OPt8+eWXhsViMfbt2+e74BGw+vTpY9xyyy0ux6655hpj8ODBhmFw38EwqFyZJC8vT2vXrlX37t2dx6xWq7p3765Vq1aZGBmCyZEjRyRJNWvWlCStXbtW+fn5Lvdds2bN1LBhQ+d9t2rVKrVs2VLx8fHOPr169VJ2dra2bNlSidEj0Nx1113q06ePy/0lcd/BN+bPn6927drp+uuvV926ddW6dWu9+eabzvb09HRlZma63HdxcXHq0KGDy31XvXp1tWvXztmne/fuslqt+v777yvvYhAwOnXqpMWLF2v79u2SpI0bN2rFihXq3bu3JO47SKFmB1BVHTx4UIWFhS4fJCQpPj5e27ZtMykqBBObzaZ7771Xl1xyiVq0aCFJyszMVHh4uKpXr+7SNz4+XpmZmc4+7u5LRxvgzpw5c7Ru3Tr98MMPRdq47+ALv/32m1599VWNGTNGDz74oH744QeNHj1a4eHhGjZsmPO+cXdfnX7f1a1b16U9NDRUNWvW5L6DW+PGjVN2draaNWumkJAQFRYW6j//+Y8GDx4sSdx3ILkCgtVdd92lzZs3a8WKFWaHgiC3d+9e3XPPPVq0aJEiIyPNDgdVhM1mU7t27fTUU09Jklq3bq3Nmzfrtdde07Bhw0yODsFq7ty5mjVrlmbPnq3zzz9fGzZs0L333qvExETuO0hiQQvT1K5dWyEhIUVWy8rKylJCQoJJUSFYjBo1Sp9//rm+/fZbNWjQwHk8ISFBeXl5Onz4sEv/0++7hIQEt/elow0409q1a3XgwAG1adNGoaGhCg0N1dKlS/XSSy8pNDRU8fHx3Hfwunr16iklJcXlWPPmzbVnzx5Jp+6bkv6fTUhIKLKIVEFBgQ4dOsR9B7fuv/9+jRs3TgMHDlTLli1100036b777tOkSZMkcd+B5Mo04eHhatu2rRYvXuw8ZrPZtHjxYnXs2NHEyBDIDMPQqFGj9PHHH+ubb75R48aNXdrbtm2rsLAwl/vul19+0Z49e5z3XceOHbVp0yaXf/gXLVqk2NjYIh9kAEm64oortGnTJm3YsMH5p127dho8eLDzMfcdvO2SSy4pstXE9u3blZycLElq3LixEhISXO677Oxsff/99y733eHDh7V27Vpnn2+++UY2m00dOnSohKtAoDl27JisVtePzyEhIbLZbJK47yBWCzTTnDlzjIiICGP69OnG1q1bjdtuu82oXr26y2pZQFmMHDnSiIuLM5YsWWJkZGQ4/xw7dszZ54477jAaNmxofPPNN8aPP/5odOzY0ejYsaOzvaCgwGjRooXRs2dPY8OGDUZaWppRp04dY/z48WZcEgLU6asFGgb3HbxvzZo1RmhoqPGf//zH2LFjhzFr1iwjOjramDlzprPP5MmTjerVqxuffvqp8dNPPxn/+Mc/jMaNGxvHjx939klNTTVat25tfP/998aKFSuMpk2bGoMGDTLjkhAAhg0bZtSvX9/4/PPPjfT0dOOjjz4yateubTzwwAPOPtx3VRvJlclefvllo2HDhkZ4eLjRvn17Y/Xq1WaHhAAmye2fd955x9nn+PHjxp133mnUqFHDiI6ONq6++mojIyPD5Ty7du0yevfubURFRRm1a9c2/vWvfxn5+fmVfDUIZGcmV9x38IXPPvvMaNGihREREWE0a9bMeOONN1zabTab8cgjjxjx8fFGRESEccUVVxi//PKLS58///zTGDRokBETE2PExsYaw4cPN44ePVqZl4EAkp2dbdxzzz1Gw4YNjcjISOPss882HnroIZctI7jvqjaLYZy2pTQAAAAAoFyYcwUAAAAAXkByBQAAAABeQHIFAAAAAF5AcgUAAAAAXkByBQAAAABeQHIFAAAAAF5AcgUAAAAAXkByBQAAAABeQHIFAIAki8WiTz75RJK0a9cuWSwWbdiwwdSYAACBJdTsAAAAqKibb75Zhw8fdiZHFZWUlKSMjAzVrl3bK+cDAFQNJFcAAJwhJCRECQkJZocBAAgwDAsEAASVyy67TKNHj9YDDzygmjVrKiEhQY899phLnx07dqhLly6KjIxUSkqKFi1a5NLubljgli1bdNVVVyk2NlbVqlVT586dtXPnTmf7//3f/6l58+aKjIxUs2bN9N///tfZlpeXp1GjRqlevXqKjIxUcnKyJk2a5JPrBwCYh8oVACDovPvuuxozZoy+//57rVq1SjfffLMuueQS9ejRQzabTddcc43i4+P1/fff68iRI7r33ntLPN++ffvUpUsXXXbZZfrmm28UGxur7777TgUFBZKkWbNm6dFHH9W0adPUunVrrV+/XrfeeqvOOussDRs2TC+99JLmz5+vuXPnqmHDhtq7d6/27t1bCd8JAEBlIrkCAASdCy64QBMmTJAkNW3aVNOmTdPixYvVo0cPff3119q2bZu++uorJSYmSpKeeuop9e7du9jzvfLKK4qLi9OcOXMUFhYmSTr33HOd7RMmTNCUKVN0zTXXSJIaN26srVu36vXXX9ewYcO0Z88eNW3aVJdeeqksFouSk5N9dekAABORXAEAgs4FF1zg8rxevXo6cOCAJOnnn39WUlKSM7GSpI4dO5Z4vg0bNqhz587OxOp0f//9t3bu3KkRI0bo1ltvdR4vKChQXFycJPuCGz169NB5552n1NRUXXXVVerZs2e5rw8A4J9IrgAAQefMJMhischms5X7fFFRUcW25eTkSJLefPNNdejQwaUtJCREktSmTRulp6fryy+/1Ndff60BAwaoe/fu+uCDD8odEwDA/5BcAQCqlObNm2vv3r3KyMhQvXr1JEmrV68u8TUXXHCB3n33XeXn5xdJ3OLj45WYmKjffvtNgwcPLvYcsbGxuuGGG3TDDTfouuuuU2pqqg4dOqSaNWtW/KIAAH6B5AoAUKV0795d5557roYNG6Znn31W2dnZeuihh0p8zahRo/Tyyy9r4MCBGj9+vOLi4rR69Wq1b99e5513niZOnKjRo0crLi5OqampOnHihH788Uf99ddfGjNmjJ5//nnVq1dPrVu3ltVq1bx585SQkKDq1atXzkUDACoFS7EDAKoUq9Wqjz/+WMePH1f79u31z3/+U//5z39KfE2tWrX0zTffKCcnR127dlXbtm315ptvOqtY//znP/V///d/euedd9SyZUt17dpV06dPV+PGjSVJ1apV0zPPPKN27drpoosu0q5du7RgwQJZrfw3DADBxGIYhmF2EAAAAAAQ6PiVGQAAAAB4AckVAAAAAHgByRUAAAAAeAHJFQAAAAB4AckVAAAAAHgByRUAAAAAeAHJFQAAAAB4AckVAAAAAHgByRUAAAAAeAHJFQAAAAB4AckVAAAAAHjB/wNJpKo9Mf1mYQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices = range(1, len(passive_mean) + 1)\n",
    "real_indices = [x * 10 for x in x_values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(real_indices, passive_mean, label='Passive Mean', marker='o')\n",
    "plt.plot(real_indices, active_mean, label='Active Mean', marker='o')\n",
    "\n",
    "plt.title('Mean Comparison')\n",
    "plt.xlabel('Indices')\n",
    "plt.ylabel('Mean Value')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, I found that the two results between two active learning are close. And the passive learning's learning rate is more smooth. After around 500 selected training data, the performance of two learning method is similiar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
